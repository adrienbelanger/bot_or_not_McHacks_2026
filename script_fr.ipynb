{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fa69427",
   "metadata": {},
   "source": [
    "# McHacks 26 - Bot or Not\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "25e59269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9fa1b429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_COLAB=False\n",
      "USE_GOOGLE_DRIVE_DATA=False\n",
      "PROJECT_ROOT=.\n",
      "DATA_DIR=/Users/adrienbelanger/Library/GitHub/bot_or_not_McHacks_2026/data\n",
      "EXTERNAL_DATA_DIR=/Users/adrienbelanger/Library/GitHub/bot_or_not_McHacks_2026/external_data\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Toggle this in Colab when your datasets are stored on Google Drive.\n",
    "USE_GOOGLE_DRIVE_DATA = False\n",
    "\n",
    "# Adjust this only if your folder lives elsewhere in Drive.\n",
    "GOOGLE_DRIVE_PROJECT_ROOT = Path(\"/content/drive/MyDrive/bot_or_not_McHacks_2026\")\n",
    "LOCAL_PROJECT_ROOT = Path(\".\")\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "PROJECT_ROOT = LOCAL_PROJECT_ROOT\n",
    "\n",
    "if IN_COLAB and USE_GOOGLE_DRIVE_DATA:\n",
    "    from google.colab import drive\n",
    "    !pip install emoji==0.6.0\n",
    "    !pip install catboost\n",
    "\n",
    "    drive.mount(\"/content/drive\", force_remount=False)\n",
    "    PROJECT_ROOT = GOOGLE_DRIVE_PROJECT_ROOT\n",
    "elif IN_COLAB:\n",
    "    PROJECT_ROOT = LOCAL_PROJECT_ROOT\n",
    "\n",
    "DATA_DIR = (PROJECT_ROOT / \"data\").resolve()\n",
    "EXTERNAL_DATA_DIR = (PROJECT_ROOT / \"external_data\").resolve()\n",
    "\n",
    "print(f\"IN_COLAB={IN_COLAB}\")\n",
    "print(f\"USE_GOOGLE_DRIVE_DATA={USE_GOOGLE_DRIVE_DATA}\")\n",
    "print(f\"PROJECT_ROOT={PROJECT_ROOT}\")\n",
    "print(f\"DATA_DIR={DATA_DIR}\")\n",
    "print(f\"EXTERNAL_DATA_DIR={EXTERNAL_DATA_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c78ef1e",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "071ab039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DATA_DIR: /Users/adrienbelanger/Library/GitHub/bot_or_not_McHacks_2026/data\n",
      "Using EXTERNAL_DATA_DIR: /Users/adrienbelanger/Library/GitHub/bot_or_not_McHacks_2026/external_data\n",
      "EN sources: ['dataset.posts&users.30.json', 'dataset.posts&users.32.json']\n",
      "EN posts: 15,765 users: 546 bot_ids: 129\n",
      "FR sources: ['dataset.posts&users.31.json', 'dataset.posts&users.33.json']\n",
      "FR posts: 9,004 users: 343 bot_ids: 55\n"
     ]
    }
   ],
   "source": [
    "if \"DATA_DIR\" not in globals():\n",
    "    DATA_DIR = Path(\"data\").resolve()\n",
    "if \"EXTERNAL_DATA_DIR\" not in globals():\n",
    "    EXTERNAL_DATA_DIR = Path(\"external_data\").resolve()\n",
    "\n",
    "print(f\"Using DATA_DIR: {DATA_DIR}\")\n",
    "print(f\"Using EXTERNAL_DATA_DIR: {EXTERNAL_DATA_DIR}\")\n",
    "\n",
    "\n",
    "def get_version(path):\n",
    "    try:\n",
    "        return int(path.stem.split(\".\")[-1])\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "posts_users_files = sorted(DATA_DIR.glob(\"dataset.posts&users.*.json\"), key=get_version)\n",
    "if not posts_users_files:\n",
    "    raise FileNotFoundError(f\"No dataset.posts&users.*.json files found in {DATA_DIR}\")\n",
    "\n",
    "\n",
    "combined = {}\n",
    "bots_by_lang = {}\n",
    "\n",
    "\n",
    "for path in posts_users_files:\n",
    "    with path.open() as f:\n",
    "        data = json.load(f)\n",
    "    lang = data.get(\"lang\")\n",
    "\n",
    "    combined.setdefault(lang, {\"posts\": [], \"users\": [], \"sources\": []})\n",
    "    combined[lang][\"posts\"].extend(data.get(\"posts\", []))\n",
    "    combined[lang][\"users\"].extend(data.get(\"users\", []))\n",
    "    combined[lang][\"sources\"].append(path.name)\n",
    "\n",
    "    version = get_version(path)\n",
    "    if version is not None:\n",
    "        bots_path = DATA_DIR / f\"dataset.bots.{version}.txt\"\n",
    "        if bots_path.exists():\n",
    "            bots_by_lang.setdefault(lang, set()).update(bots_path.read_text().splitlines())\n",
    "\n",
    "\n",
    "posts_en = pd.DataFrame(combined.get(\"en\", {}).get(\"posts\", []))\n",
    "users_en = pd.DataFrame(combined.get(\"en\", {}).get(\"users\", []))\n",
    "bot_ids_en = bots_by_lang.get(\"en\", set())\n",
    "if not users_en.empty:\n",
    "    users_en[\"is_bot\"] = users_en[\"id\"].isin(bot_ids_en)\n",
    "\n",
    "posts_fr = pd.DataFrame(combined.get(\"fr\", {}).get(\"posts\", []))\n",
    "users_fr = pd.DataFrame(combined.get(\"fr\", {}).get(\"users\", []))\n",
    "bot_ids_fr = bots_by_lang.get(\"fr\", set())\n",
    "if not users_fr.empty:\n",
    "    users_fr[\"is_bot\"] = users_fr[\"id\"].isin(bot_ids_fr)\n",
    "\n",
    "print(\"EN sources:\", combined.get(\"en\", {}).get(\"sources\", []))\n",
    "print(f\"EN posts: {len(posts_en):,} users: {len(users_en):,} bot_ids: {len(bot_ids_en):,}\")\n",
    "print(\"FR sources:\", combined.get(\"fr\", {}).get(\"sources\", []))\n",
    "print(f\"FR posts: {len(posts_fr):,} users: {len(users_fr):,} bot_ids: {len(bot_ids_fr):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e34ec8c",
   "metadata": {},
   "source": [
    "### Experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926640c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment config loaded:\n",
      "- tokenizer_name: cardiffnlp/twitter-xlm-roberta-base\n",
      "- target_lang: fr\n",
      "- max_length: 96\n",
      "- dedupe_users: True\n",
      "- dedupe_posts: True\n",
      "- scale_meta_features: True\n",
      "- normalize_social_tokens: True\n",
      "- strip_hashtag_symbol: True\n",
      "- lowercase_text: True\n",
      "- use_topic_features: True\n",
      "- topic_match_mode: word\n",
      "- topic_feature_scale: 0.5\n",
      "- topic_include_cross_language_keywords: False\n",
      "- topic_cross_langs: ['en']\n",
      "- test_size: 0.2\n",
      "- random_seed: 42\n",
      "- validation_split: 0.15\n",
      "- epochs: 10\n",
      "- batch_size: 96\n",
      "- learning_rate: 0.0008\n",
      "- prediction_threshold: 0.62\n",
      "- use_threshold_search: True\n",
      "- threshold_search_min: 0.1\n",
      "- threshold_search_max: 0.9\n",
      "- threshold_search_steps: 161\n",
      "- account_decision_rule: mean\n",
      "- split_by_author: True\n",
      "- ensemble_seeds: [13, 29, 42, 73, 101, 137, 173]\n",
      "- ensemble_aggregation: median\n",
      "- use_external_pretrain: True\n",
      "- external_pretrain_source: airt_ml\n",
      "- external_pretrain_download: True\n",
      "- external_pretrain_hf_repo: airt-ml/twitter-human-bots\n",
      "- external_pretrain_hf_filename: twitter_human_bots_dataset.csv\n",
      "- external_pretrain_max_users: 20000\n",
      "- external_pretrain_max_tweets_per_user: 5\n",
      "- external_pretrain_max_posts: 120000\n",
      "- external_pretrain_lang: fr\n",
      "- external_pretrain_sample_seed: 42\n",
      "- external_pretrain_epochs: 1\n",
      "- external_pretrain_batch_size: 128\n",
      "- external_pretrain_use_balanced_weights: True\n",
      "- use_second_stage_account_model: False\n",
      "- use_logreg_account_model: True\n",
      "- logreg_max_iter: 200\n",
      "- logreg_min_posts: 2\n",
      "- second_stage_profile: regularized\n",
      "- second_stage_use_blend: True\n",
      "- second_stage_blend_alphas: [1.0, 0.9, 0.8, 0.7, 0.55, 0.4, 0.25, 0.1]\n",
      "- second_stage_min_gain_vs_ensemble: 2\n",
      "- second_stage_max_extra_fp_vs_ensemble: 0\n",
      "- second_stage_min_val_accounts_for_booster: 25\n",
      "- second_stage_use_oof_features: True\n",
      "- second_stage_oof_folds: 4\n",
      "- second_stage_oof_epochs: 4\n",
      "- second_stage_oof_batch_size: 128\n",
      "- second_stage_oof_seeds: [13, 42]\n",
      "- second_stage_oof_use_external_pretrain: True\n",
      "- second_stage_learning_rate: 0.05\n",
      "- second_stage_max_iter: 300\n",
      "- second_stage_max_depth: 4\n",
      "- second_stage_l2: 0.2\n",
      "- second_stage_min_data_in_leaf: 20\n",
      "- second_stage_subsample: 0.8\n",
      "- second_stage_rsm: 0.8\n",
      "- second_stage_od_wait: 50\n",
      "- second_stage_use_balanced_weights: True\n",
      "- use_class_weights: True\n",
      "- embedding_dim: 96\n",
      "- gru_units: 48\n",
      "- aux_dense_units: 32\n",
      "- head_dense_units: 48\n",
      "- dropout_text: 0.4\n",
      "- dropout_aux: 0.3\n",
      "- dropout_head: 0.4\n",
      "- early_stopping_patience: 2\n",
      "- reduce_lr_patience: 2\n",
      "- reduce_lr_factor: 0.5\n",
      "- reduce_lr_min_lr: 1e-05\n",
      "- error_analysis_min_topic_posts: 20\n",
      "- error_analysis_min_topic_accounts: 5\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_CONFIG = {\n",
    "    \"tokenizer_name\": \"cardiffnlp/twitter-xlm-roberta-base\",\n",
    "\n",
    "    \"target_lang\": \"fr\",\n",
    "    \"max_length\": 96,\n",
    "    \"dedupe_users\": True,\n",
    "    \"dedupe_posts\": True,\n",
    "    \"scale_meta_features\": True,\n",
    "\n",
    "    \"normalize_social_tokens\": True,\n",
    "\n",
    "    \"strip_hashtag_symbol\": True,\n",
    "\n",
    "    \"lowercase_text\": True,\n",
    "    \"use_topic_features\": False,\n",
    "    \"topic_match_mode\": \"word\",  # options: \"contains\", \"word\"\n",
    "    \"topic_feature_scale\": 0.5,\n",
    "    \"topic_include_cross_language_keywords\": False,\n",
    "    \"topic_cross_langs\": [\"en\"],\n",
    "    \"test_size\": 0.20,\n",
    "    \"random_seed\": 42,\n",
    "    \"validation_split\": 0.15,\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 96,\n",
    "    \"learning_rate\": 0.0008,\n",
    "    \"prediction_threshold\": 0.62,\n",
    "    \"use_threshold_search\": True,\n",
    "    \"threshold_search_min\": 0.10,\n",
    "    \"threshold_search_max\": 0.90,\n",
    "    \"threshold_search_steps\": 161,\n",
    "    \"account_decision_rule\": \"mean\",  # options: \"mean\", \"any\"\n",
    "    \"split_by_author\": True,\n",
    "    \"ensemble_seeds\": [13, 29, 42, 73, 101, 137, 173],\n",
    "    \"ensemble_aggregation\": \"median\",  # options: \"mean\", \"median\"\n",
    "    \"use_external_pretrain\": True,\n",
    "    \"external_pretrain_source\": \"airt_ml\",\n",
    "    \"external_pretrain_download\": True,\n",
    "    \"external_pretrain_hf_repo\": \"airt-ml/twitter-human-bots\",\n",
    "    \"external_pretrain_hf_filename\": \"twitter_human_bots_dataset.csv\",\n",
    "    \"external_pretrain_max_users\": 20000,\n",
    "    \"external_pretrain_max_tweets_per_user\": 5,\n",
    "    \"external_pretrain_max_posts\": 120000,\n",
    "    \"external_pretrain_lang\": \"fr\",\n",
    "    \"external_pretrain_sample_seed\": 42,\n",
    "    \"external_pretrain_epochs\": 1,\n",
    "    \"external_pretrain_batch_size\": 128,\n",
    "    \"external_pretrain_use_balanced_weights\": True,\n",
    "    \"use_second_stage_account_model\": False,\n",
    "    \"use_logreg_account_model\": True,\n",
    "    \"logreg_max_iter\": 200,\n",
    "    \"logreg_min_posts\": 2,\n",
    "    \"second_stage_profile\": \"regularized\",  # options: \"auto\", \"legacy\", \"regularized\", \"custom\"\n",
    "    \"second_stage_use_blend\": True,\n",
    "    \"second_stage_blend_alphas\": [1.0, 0.90, 0.80, 0.70, 0.55, 0.40, 0.25, 0.10],\n",
    "    \"second_stage_min_gain_vs_ensemble\": 2,\n",
    "\n",
    "    \"second_stage_max_extra_fp_vs_ensemble\": 0,\n",
    "\n",
    "    \"second_stage_min_val_accounts_for_booster\": 25,\n",
    "    \"second_stage_use_oof_features\": True,\n",
    "    \"second_stage_oof_folds\": 4,\n",
    "    \"second_stage_oof_epochs\": 4,\n",
    "    \"second_stage_oof_batch_size\": 128,\n",
    "    \"second_stage_oof_seeds\": [13, 42],\n",
    "    \"second_stage_oof_use_external_pretrain\": True,\n",
    "    \"second_stage_learning_rate\": 0.05,\n",
    "    \"second_stage_max_iter\": 300,\n",
    "    \"second_stage_max_depth\": 4,\n",
    "    \"second_stage_l2\": 0.2,\n",
    "    \"second_stage_min_data_in_leaf\": 20,\n",
    "    \"second_stage_subsample\": 0.8,\n",
    "    \"second_stage_rsm\": 0.8,\n",
    "    \"second_stage_od_wait\": 50,\n",
    "    \"second_stage_use_balanced_weights\": True,\n",
    "    \"use_class_weights\": True,\n",
    "    \"embedding_dim\": 96,\n",
    "    \"gru_units\": 48,\n",
    "    \"aux_dense_units\": 32,\n",
    "    \"head_dense_units\": 48,\n",
    "    \"dropout_text\": 0.40,\n",
    "    \"dropout_aux\": 0.30,\n",
    "    \"dropout_head\": 0.40,\n",
    "    \"early_stopping_patience\": 2,\n",
    "    \"reduce_lr_patience\": 2,\n",
    "    \"reduce_lr_factor\": 0.50,\n",
    "    \"reduce_lr_min_lr\": 1e-5,\n",
    "    \"error_analysis_min_topic_posts\": 20,\n",
    "    \"error_analysis_min_topic_accounts\": 5,\n",
    "}\n",
    "\n",
    "print(\"Experiment config loaded:\")\n",
    "for key, value in EXPERIMENT_CONFIG.items():\n",
    "    print(f\"- {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37411cfe",
   "metadata": {},
   "source": [
    "### Tokenizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "a1288615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target language: FR\n",
      "Tokenizer: cardiffnlp/twitter-xlm-roberta-base\n",
      "Labeled posts used: 9,004\n",
      "Token tensor shape: (9004, 96)\n",
      "Meta feature shape: (9004, 7), label shape: (9004,)\n",
      "Dedupe users/posts: True/True\n",
      "Scale metadata features: True\n",
      "Normalize URLs/users: True\n",
      "Strip hashtag symbol: True\n",
      "Lowercase text: True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "except ImportError as exc:\n",
    "    raise ImportError(\"Install transformers first: pip install transformers\") from exc\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "if \"EXPERIMENT_CONFIG\" not in globals():\n",
    "    raise ValueError(\"Run the Experiment parameters cell first.\")\n",
    "\n",
    "TOKENIZER_NAME = str(EXPERIMENT_CONFIG[\"tokenizer_name\"])\n",
    "TARGET_LANG = str(EXPERIMENT_CONFIG.get(\"target_lang\", \"fr\")).strip().lower()\n",
    "MAX_LENGTH = int(EXPERIMENT_CONFIG[\"max_length\"])\n",
    "DEDUPE_USERS = bool(EXPERIMENT_CONFIG[\"dedupe_users\"])\n",
    "DEDUPE_POSTS = bool(EXPERIMENT_CONFIG[\"dedupe_posts\"])\n",
    "SCALE_META_FEATURES = bool(EXPERIMENT_CONFIG[\"scale_meta_features\"])\n",
    "NORMALIZE_SOCIAL_TOKENS = bool(EXPERIMENT_CONFIG.get(\"normalize_social_tokens\", True))\n",
    "STRIP_HASHTAG_SYMBOL = bool(EXPERIMENT_CONFIG.get(\"strip_hashtag_symbol\", True))\n",
    "LOWERCASE_TEXT = bool(EXPERIMENT_CONFIG.get(\"lowercase_text\", False))\n",
    "\n",
    "if TARGET_LANG not in {\"en\", \"fr\"}:\n",
    "    raise ValueError('target_lang must be \"en\" or \"fr\".')\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.replace(\"\\n\", \" \").strip()\n",
    "\n",
    "    if NORMALIZE_SOCIAL_TOKENS:\n",
    "        text = re.sub(r\"https?://\\S+|www\\.\\S+\", \" <URL> \", text)\n",
    "        text = re.sub(r\"@\\w+\", \" <USER> \", text)\n",
    "\n",
    "    if STRIP_HASHTAG_SYMBOL:\n",
    "        text = re.sub(r\"#(\\w+)\", r\" \\1 \", text)\n",
    "\n",
    "    if LOWERCASE_TEXT:\n",
    "        text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def add_text_features(df):\n",
    "    out = df.copy()\n",
    "\n",
    "    raw_text = out[\"text\"].fillna(\"\").map(lambda x: x if isinstance(x, str) else \"\")\n",
    "    out[\"text_clean\"] = raw_text.map(clean_text)\n",
    "\n",
    "    # Counts from raw text retain signal even when text_clean is normalized.\n",
    "    out[\"url_count\"] = raw_text.str.count(r\"https?://\\S+|www\\.\\S+\")\n",
    "    out[\"mention_count\"] = raw_text.str.count(r\"@\\w+\")\n",
    "    out[\"hashtag_count\"] = raw_text.str.count(r\"#\\w+\")\n",
    "\n",
    "    out[\"char_count\"] = out[\"text_clean\"].str.len()\n",
    "    out[\"word_count\"] = out[\"text_clean\"].str.split().str.len()\n",
    "    out[\"exclamation_count\"] = out[\"text_clean\"].str.count(r\"!\")\n",
    "    out[\"question_count\"] = out[\"text_clean\"].str.count(r\"\\?\")\n",
    "    return out\n",
    "\n",
    "\n",
    "posts_by_lang = {\"en\": posts_en, \"fr\": posts_fr}\n",
    "users_by_lang = {\"en\": users_en, \"fr\": users_fr}\n",
    "\n",
    "posts_lang = posts_by_lang.get(TARGET_LANG, pd.DataFrame())\n",
    "users_lang = users_by_lang.get(TARGET_LANG, pd.DataFrame())\n",
    "\n",
    "if posts_lang.empty or users_lang.empty:\n",
    "    raise ValueError(\n",
    "        f\"Run the data processing cell first to load {TARGET_LANG.upper()} data. \"\n",
    "        f\"Posts={len(posts_lang)}, users={len(users_lang)}\"\n",
    "    )\n",
    "\n",
    "users_lang_labeled = (\n",
    "    users_lang.drop_duplicates(subset=\"id\", keep=\"last\").copy()\n",
    "    if DEDUPE_USERS\n",
    "    else users_lang.copy()\n",
    ")\n",
    "posts_lang_unique = (\n",
    "    posts_lang.drop_duplicates(subset=\"id\", keep=\"last\").copy()\n",
    "    if DEDUPE_POSTS\n",
    "    else posts_lang.copy()\n",
    ")\n",
    "\n",
    "# Keep downstream cell compatibility by reusing existing variable names.\n",
    "users_en = users_lang.copy()\n",
    "posts_en = posts_lang.copy()\n",
    "users_en_labeled = users_lang_labeled.copy()\n",
    "\n",
    "label_map_lang = users_lang_labeled.set_index(\"id\")[\"is_bot\"]\n",
    "train_en = posts_lang_unique.copy()\n",
    "train_en[\"is_bot\"] = train_en[\"author_id\"].map(label_map_lang)\n",
    "train_en = train_en.dropna(subset=[\"is_bot\"]).copy()\n",
    "train_en[\"is_bot\"] = train_en[\"is_bot\"].astype(\"int64\")\n",
    "\n",
    "train_en = add_text_features(train_en)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, use_fast=True)\n",
    "encodings_en = tokenizer(\n",
    "    train_en[\"text_clean\"].tolist(),\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=MAX_LENGTH,\n",
    "    return_attention_mask=True,\n",
    ")\n",
    "\n",
    "feature_cols_en = [\n",
    "    \"char_count\",\n",
    "    \"word_count\",\n",
    "    \"url_count\",\n",
    "    \"mention_count\",\n",
    "    \"hashtag_count\",\n",
    "    \"exclamation_count\",\n",
    "    \"question_count\",\n",
    "]\n",
    "X_meta_en = train_en[feature_cols_en].to_numpy(dtype=np.float32)\n",
    "y_en = train_en[\"is_bot\"].to_numpy(dtype=np.int64)\n",
    "\n",
    "if SCALE_META_FEATURES:\n",
    "    scaler_en = StandardScaler()\n",
    "    X_meta_en_scaled = scaler_en.fit_transform(X_meta_en).astype(np.float32)\n",
    "else:\n",
    "    scaler_en = None\n",
    "    X_meta_en_scaled = X_meta_en.copy()\n",
    "\n",
    "print(f\"Target language: {TARGET_LANG.upper()}\")\n",
    "print(f\"Tokenizer: {TOKENIZER_NAME}\")\n",
    "print(f\"Labeled posts used: {len(train_en):,}\")\n",
    "print(f\"Token tensor shape: {np.asarray(encodings_en['input_ids']).shape}\")\n",
    "print(f\"Meta feature shape: {X_meta_en_scaled.shape}, label shape: {y_en.shape}\")\n",
    "print(f\"Dedupe users/posts: {DEDUPE_USERS}/{DEDUPE_POSTS}\")\n",
    "print(f\"Scale metadata features: {SCALE_META_FEATURES}\")\n",
    "print(f\"Normalize URLs/users: {NORMALIZE_SOCIAL_TOKENS}\")\n",
    "print(f\"Strip hashtag symbol: {STRIP_HASHTAG_SYMBOL}\")\n",
    "print(f\"Lowercase text: {LOWERCASE_TEXT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9f8c2a",
   "metadata": {},
   "source": [
    "## External data pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4c8c2c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External pretrain dataset prepared:\n",
      "- source: airt_ml\n",
      "- rows: 375\n",
      "- bots: 104, humans: 271\n",
      "- max_length: 96\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if \"EXPERIMENT_CONFIG\" not in globals():\n",
    "    raise ValueError(\"Run the Experiment parameters cell first.\")\n",
    "if \"tokenizer\" not in globals() or \"add_text_features\" not in globals():\n",
    "    raise ValueError(\"Run the Tokenizing the text cell first.\")\n",
    "\n",
    "USE_EXTERNAL_PRETRAIN = bool(EXPERIMENT_CONFIG.get(\"use_external_pretrain\", False))\n",
    "if not USE_EXTERNAL_PRETRAIN:\n",
    "    print(\"External pretraining disabled. Set use_external_pretrain=True to enable.\")\n",
    "else:\n",
    "    external_source = str(EXPERIMENT_CONFIG.get(\"external_pretrain_source\", \"fox8\")).lower()\n",
    "    max_users = int(EXPERIMENT_CONFIG.get(\"external_pretrain_max_users\", 20000))\n",
    "    max_posts = int(EXPERIMENT_CONFIG.get(\"external_pretrain_max_posts\", 120000))\n",
    "    max_tweets_per_user = int(EXPERIMENT_CONFIG.get(\"external_pretrain_max_tweets_per_user\", 5))\n",
    "    lang_filter = str(EXPERIMENT_CONFIG.get(\"external_pretrain_lang\", EXPERIMENT_CONFIG.get(\"target_lang\", \"fr\"))).lower().strip()\n",
    "    sample_seed = int(EXPERIMENT_CONFIG.get(\"external_pretrain_sample_seed\", 42))\n",
    "\n",
    "    random.seed(sample_seed)\n",
    "\n",
    "    data_dir = Path(globals().get(\"EXTERNAL_DATA_DIR\", Path(\"external_data\")))\n",
    "    external_pretrain_rows = []\n",
    "\n",
    "    if external_source in {\"fox8\"}:\n",
    "        fox8_path = data_dir / \"fox8_23_dataset.ndjson\"\n",
    "        if not fox8_path.exists():\n",
    "            raise FileNotFoundError(f\"Missing {fox8_path}\")\n",
    "        # Reservoir sample users to avoid loading the full file.\n",
    "        sampled_users = []\n",
    "        with fox8_path.open() as f:\n",
    "            for i, line in enumerate(f):\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                if len(sampled_users) < max_users:\n",
    "                    sampled_users.append(obj)\n",
    "                else:\n",
    "                    j = random.randint(0, i)\n",
    "                    if j < max_users:\n",
    "                        sampled_users[j] = obj\n",
    "\n",
    "        for user in sampled_users:\n",
    "            label = str(user.get(\"label\", \"\")).strip().lower()\n",
    "            if not label:\n",
    "                continue\n",
    "            if label in {\"human\", \"normal\"}:\n",
    "                is_bot = 0\n",
    "            elif label in {\"bot\", \"spambot\", \"fake\", \"automated\"}:\n",
    "                is_bot = 1\n",
    "            else:\n",
    "                # Unknown label; skip\n",
    "                continue\n",
    "\n",
    "            user_id = user.get(\"user_id\")\n",
    "            tweets = user.get(\"user_tweets\", []) or []\n",
    "\n",
    "            per_user_count = 0\n",
    "            for tweet in tweets:\n",
    "                if len(external_pretrain_rows) >= max_posts or per_user_count >= max_tweets_per_user:\n",
    "                    break\n",
    "                text = tweet.get(\"text\") if isinstance(tweet, dict) else None\n",
    "                if not text:\n",
    "                    continue\n",
    "                if lang_filter:\n",
    "                    tweet_lang = str(tweet.get(\"lang\", \"\")).lower().strip() if isinstance(tweet, dict) else \"\"\n",
    "                    if tweet_lang and tweet_lang != lang_filter:\n",
    "                        continue\n",
    "                external_pretrain_rows.append({\"author_id\": f\"fox8_{user_id}\", \"text\": text, \"is_bot\": is_bot})\n",
    "                per_user_count += 1\n",
    "\n",
    "    elif external_source in {\"airt_ml\", \"airt-ml\", \"twitter_human_bots\", \"twitter-human-bots\", \"hf_twitter_human_bots\"}:\n",
    "        hf_repo = str(EXPERIMENT_CONFIG.get(\"external_pretrain_hf_repo\", \"airt-ml/twitter-human-bots\"))\n",
    "        hf_filename = str(EXPERIMENT_CONFIG.get(\"external_pretrain_hf_filename\", \"twitter_human_bots_dataset.csv\"))\n",
    "        download_hf = bool(EXPERIMENT_CONFIG.get(\"external_pretrain_download\", True))\n",
    "        local_path = data_dir / hf_filename\n",
    "        hf_path = f\"hf://datasets/{hf_repo}/{hf_filename}\"\n",
    "\n",
    "        if download_hf and not local_path.exists():\n",
    "            try:\n",
    "                from huggingface_hub import hf_hub_download\n",
    "                hf_hub_download(\n",
    "                    repo_id=hf_repo,\n",
    "                    filename=hf_filename,\n",
    "                    repo_type=\"dataset\",\n",
    "                    local_dir=str(data_dir),\n",
    "                    local_dir_use_symlinks=False,\n",
    "                )\n",
    "                print(f\"Downloaded {hf_filename} to {local_path}\")\n",
    "            except Exception as exc:\n",
    "                print(f\"HF download failed ({exc}). Will try direct read.\")\n",
    "\n",
    "        try:\n",
    "            if local_path.exists():\n",
    "                external_df = pd.read_csv(local_path)\n",
    "            else:\n",
    "                external_df = pd.read_csv(hf_path)\n",
    "        except Exception as exc:\n",
    "            raise FileNotFoundError(\n",
    "                f\"Could not load {hf_path} or {local_path}. Download manually if needed. Error: {exc}\"\n",
    "            )\n",
    "\n",
    "        # Drop index-like columns if present\n",
    "        if \"Unnamed: 0\" in external_df.columns:\n",
    "            external_df = external_df.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "        def _find_col(columns, candidates, contains=None):\n",
    "            for c in candidates:\n",
    "                if c in columns:\n",
    "                    return c\n",
    "            if contains:\n",
    "                for c in columns:\n",
    "                    if contains in c.lower():\n",
    "                        return c\n",
    "            return None\n",
    "\n",
    "        # Language column\n",
    "        lang_col = _find_col(external_df.columns, [\"lang\", \"language\", \"tweet_lang\", \"tweet_language\"], contains=\"lang\")\n",
    "        if lang_col and lang_filter:\n",
    "            external_df = external_df[external_df[lang_col].astype(str).str.lower().str.strip() == lang_filter]\n",
    "\n",
    "        # Text column (this dataset is user-level; use description if no tweet text)\n",
    "        text_col = _find_col(external_df.columns, [\"description\", \"text\", \"tweet\", \"content\", \"full_text\"], contains=\"text\")\n",
    "        if text_col is None:\n",
    "            text_col = _find_col(external_df.columns, [\"tweet_text\", \"tweet_content\"], contains=\"tweet\")\n",
    "        if text_col is None:\n",
    "            raise ValueError(\"No text/description column found in twitter_human_bots_dataset.csv\")\n",
    "\n",
    "        # Label column\n",
    "        label_col = _find_col(external_df.columns, [\"account_type\", \"label\", \"is_bot\", \"bot\", \"bot_label\", \"account_type\", \"type\"], contains=\"bot\")\n",
    "        if label_col is None:\n",
    "            raise ValueError(\"No label column found in twitter_human_bots_dataset.csv\")\n",
    "\n",
    "        # User id column (optional)\n",
    "        user_col = _find_col(external_df.columns, [\"id\", \"user_id\", \"userid\", \"author_id\", \"user\", \"screen_name\", \"username\"], contains=\"user\")\n",
    "        if user_col is None:\n",
    "            external_df[\"_author_id\"] = np.arange(len(external_df)).astype(str)\n",
    "            user_col = \"_author_id\"\n",
    "\n",
    "        def _label_to_bot(value):\n",
    "            if pd.isna(value):\n",
    "                return None\n",
    "            if isinstance(value, (int, np.integer)):\n",
    "                return int(value)\n",
    "            if isinstance(value, (float, np.floating)):\n",
    "                return int(value >= 0.5)\n",
    "            v = str(value).strip().lower()\n",
    "            if v in {\"bot\", \"spambot\", \"fake\", \"automated\", \"1\", \"true\", \"yes\", \"bot_account\"}:\n",
    "                return 1\n",
    "            if v in {\"human\", \"normal\", \"real\", \"0\", \"false\", \"no\", \"human_account\"}:\n",
    "                return 0\n",
    "            return None\n",
    "\n",
    "        sampled = external_df.sample(n=min(len(external_df), max_posts), random_state=sample_seed)\n",
    "        user_counts = {}\n",
    "        for _, row in sampled.iterrows():\n",
    "            if len(external_pretrain_rows) >= max_posts:\n",
    "                break\n",
    "            uid = str(row[user_col])\n",
    "            if user_counts.get(uid, 0) >= max_tweets_per_user:\n",
    "                continue\n",
    "            label = _label_to_bot(row[label_col])\n",
    "            if label is None:\n",
    "                continue\n",
    "            text = row[text_col]\n",
    "            if not isinstance(text, str) or not text.strip():\n",
    "                continue\n",
    "            external_pretrain_rows.append({\"author_id\": f\"airt_{uid}\", \"text\": text, \"is_bot\": label})\n",
    "            user_counts[uid] = user_counts.get(uid, 0) + 1\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported external_pretrain_source. Use fox8 or airt_ml.\")\n",
    "\n",
    "    external_pretrain_df = pd.DataFrame(external_pretrain_rows)\n",
    "    if external_pretrain_df.empty:\n",
    "        raise ValueError(\"No external pretrain rows built. Check labels/lang filter.\")\n",
    "\n",
    "    external_pretrain_df = external_pretrain_df.dropna(subset=[\"text\"]).reset_index(drop=True)\n",
    "    external_pretrain_df = add_text_features(external_pretrain_df)\n",
    "\n",
    "    external_pretrain_encodings = tokenizer(\n",
    "        external_pretrain_df[\"text_clean\"].tolist(),\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "    external_pretrain_input_ids = np.asarray(external_pretrain_encodings[\"input_ids\"], dtype=np.int32)\n",
    "    external_pretrain_attention_mask = np.asarray(external_pretrain_encodings[\"attention_mask\"], dtype=np.float32)\n",
    "\n",
    "    external_pretrain_meta = external_pretrain_df[feature_cols_en].to_numpy(dtype=np.float32)\n",
    "    if \"scaler_en\" in globals() and SCALE_META_FEATURES:\n",
    "        external_pretrain_meta_scaled = scaler_en.transform(external_pretrain_meta).astype(np.float32)\n",
    "    else:\n",
    "        external_pretrain_meta_scaled = external_pretrain_meta\n",
    "\n",
    "    external_pretrain_labels = external_pretrain_df[\"is_bot\"].to_numpy(dtype=np.int64)\n",
    "\n",
    "    print(\"External pretrain dataset prepared:\")\n",
    "    print(f\"- source: {external_source}\")\n",
    "    print(f\"- rows: {len(external_pretrain_df):,}\")\n",
    "    print(\n",
    "        f\"- bots: {int((external_pretrain_labels==1).sum())}, humans: {int((external_pretrain_labels==0).sum())}\"\n",
    "    )\n",
    "    print(f\"- max_length: {MAX_LENGTH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251cded3",
   "metadata": {},
   "source": [
    "## Train-Test split for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "c8c62216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:08:42.183346: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step - accuracy: 0.5274 - auc: 0.5842 - loss: 0.6894"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:08:58.811557: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 197ms/step - accuracy: 0.6413 - auc: 0.6976 - loss: 0.6426 - val_accuracy: 0.8048 - val_auc: 0.8408 - val_loss: 0.4614 - learning_rate: 8.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 165ms/step - accuracy: 0.8788 - auc: 0.9315 - loss: 0.3318 - val_accuracy: 0.8524 - val_auc: 0.9132 - val_loss: 0.3513 - learning_rate: 8.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 180ms/step - accuracy: 0.9459 - auc: 0.9762 - loss: 0.1862 - val_accuracy: 0.8309 - val_auc: 0.9060 - val_loss: 0.4247 - learning_rate: 8.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 173ms/step - accuracy: 0.9676 - auc: 0.9926 - loss: 0.1013 - val_accuracy: 0.8513 - val_auc: 0.9008 - val_loss: 0.4622 - learning_rate: 8.0000e-04\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:09:34.395166: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split mode: author\n",
      "Topic features enabled: True\n",
      "Topic match mode: word\n",
      "Account decision rule: mean\n",
      "Topic columns: ['topic_pop', 'topic_nba', 'topic_movies', 'topic_nhl']\n",
      "Split sizes (fit/val/test posts): 6353/881/1770\n",
      "Split sizes (fit/val/test accounts): 232/42/69\n",
      "Default threshold from config: 0.6200\n",
      "Selected threshold used on test: 0.4800\n",
      "Best validation score from threshold search: 23\n",
      "Test Accuracy (post-level): 0.8966\n",
      "Test ROC-AUC (post-level): 0.9505\n",
      "Test account score @ selected threshold -> score=39, TP=10, FN=1, FP=0, accounts=69\n",
      "Test account score @ config threshold -> score=39, TP=10, FN=1, FP=0, accounts=69\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9521    0.9022    0.9265      1278\n",
      "           1     0.7764    0.8821    0.8259       492\n",
      "\n",
      "    accuracy                         0.8966      1770\n",
      "   macro avg     0.8642    0.8922    0.8762      1770\n",
      "weighted avg     0.9033    0.8966    0.8985      1770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except ImportError as exc:\n",
    "    raise ImportError(\"Install tensorflow first: pip install tensorflow\") from exc\n",
    "\n",
    "if \"EXPERIMENT_CONFIG\" not in globals():\n",
    "    raise ValueError(\"Run the Experiment parameters cell first.\")\n",
    "\n",
    "TEST_SIZE = float(EXPERIMENT_CONFIG[\"test_size\"])\n",
    "RANDOM_SEED = int(EXPERIMENT_CONFIG[\"random_seed\"])\n",
    "VALIDATION_SPLIT = float(EXPERIMENT_CONFIG[\"validation_split\"])\n",
    "EPOCHS = int(EXPERIMENT_CONFIG[\"epochs\"])\n",
    "BATCH_SIZE = int(EXPERIMENT_CONFIG[\"batch_size\"])\n",
    "LEARNING_RATE = float(EXPERIMENT_CONFIG[\"learning_rate\"])\n",
    "PREDICTION_THRESHOLD = float(EXPERIMENT_CONFIG[\"prediction_threshold\"])\n",
    "USE_CLASS_WEIGHTS = bool(EXPERIMENT_CONFIG[\"use_class_weights\"])\n",
    "USE_TOPIC_FEATURES = bool(EXPERIMENT_CONFIG[\"use_topic_features\"])\n",
    "TOPIC_MATCH_MODE = str(EXPERIMENT_CONFIG[\"topic_match_mode\"])\n",
    "TOPIC_FEATURE_SCALE = float(EXPERIMENT_CONFIG.get(\"topic_feature_scale\", 1.0))\n",
    "TOPIC_INCLUDE_CROSS_LANG_KEYWORDS = bool(EXPERIMENT_CONFIG.get(\"topic_include_cross_language_keywords\", False))\n",
    "TOPIC_CROSS_LANGS = EXPERIMENT_CONFIG.get(\"topic_cross_langs\", [])\n",
    "\n",
    "TARGET_LANG = str(EXPERIMENT_CONFIG.get(\"target_lang\", \"fr\")).strip().lower()\n",
    "SPLIT_BY_AUTHOR = bool(EXPERIMENT_CONFIG.get(\"split_by_author\", True))\n",
    "\n",
    "USE_THRESHOLD_SEARCH = bool(EXPERIMENT_CONFIG.get(\"use_threshold_search\", False))\n",
    "THRESHOLD_SEARCH_MIN = float(EXPERIMENT_CONFIG.get(\"threshold_search_min\", 0.10))\n",
    "THRESHOLD_SEARCH_MAX = float(EXPERIMENT_CONFIG.get(\"threshold_search_max\", 0.90))\n",
    "THRESHOLD_SEARCH_STEPS = int(EXPERIMENT_CONFIG.get(\"threshold_search_steps\", 81))\n",
    "ACCOUNT_DECISION_RULE = str(EXPERIMENT_CONFIG.get(\"account_decision_rule\", \"mean\"))\n",
    "\n",
    "EMBEDDING_DIM = int(EXPERIMENT_CONFIG[\"embedding_dim\"])\n",
    "GRU_UNITS = int(EXPERIMENT_CONFIG[\"gru_units\"])\n",
    "AUX_DENSE_UNITS = int(EXPERIMENT_CONFIG[\"aux_dense_units\"])\n",
    "HEAD_DENSE_UNITS = int(EXPERIMENT_CONFIG[\"head_dense_units\"])\n",
    "DROPOUT_TEXT = float(EXPERIMENT_CONFIG[\"dropout_text\"])\n",
    "DROPOUT_AUX = float(EXPERIMENT_CONFIG[\"dropout_aux\"])\n",
    "DROPOUT_HEAD = float(EXPERIMENT_CONFIG[\"dropout_head\"])\n",
    "\n",
    "EARLY_STOPPING_PATIENCE = int(EXPERIMENT_CONFIG[\"early_stopping_patience\"])\n",
    "REDUCE_LR_PATIENCE = int(EXPERIMENT_CONFIG[\"reduce_lr_patience\"])\n",
    "REDUCE_LR_FACTOR = float(EXPERIMENT_CONFIG[\"reduce_lr_factor\"])\n",
    "REDUCE_LR_MIN_LR = float(EXPERIMENT_CONFIG[\"reduce_lr_min_lr\"])\n",
    "\n",
    "if not (0.0 < TEST_SIZE < 1.0):\n",
    "    raise ValueError(\"test_size must be between 0 and 1.\")\n",
    "if not (0.0 <= VALIDATION_SPLIT < 1.0):\n",
    "    raise ValueError(\"validation_split must be in [0, 1).\")\n",
    "if TOPIC_MATCH_MODE not in {\"contains\", \"word\"}:\n",
    "    raise ValueError('topic_match_mode must be \"contains\" or \"word\".')\n",
    "if ACCOUNT_DECISION_RULE not in {\"mean\", \"any\"}:\n",
    "    raise ValueError('account_decision_rule must be \"mean\" or \"any\".')\n",
    "if USE_THRESHOLD_SEARCH:\n",
    "    if not (0.0 < THRESHOLD_SEARCH_MIN < 1.0 and 0.0 < THRESHOLD_SEARCH_MAX < 1.0):\n",
    "        raise ValueError(\"threshold_search_min and threshold_search_max must be in (0, 1).\")\n",
    "    if THRESHOLD_SEARCH_MIN >= THRESHOLD_SEARCH_MAX:\n",
    "        raise ValueError(\"threshold_search_min must be smaller than threshold_search_max.\")\n",
    "    if THRESHOLD_SEARCH_STEPS < 2:\n",
    "        raise ValueError(\"threshold_search_steps must be >= 2.\")\n",
    "\n",
    "\n",
    "def load_target_topic_keywords():\n",
    "    topic_keywords = {}\n",
    "    target_langs = [TARGET_LANG]\n",
    "    if TARGET_LANG == \"fr\" and TOPIC_INCLUDE_CROSS_LANG_KEYWORDS:\n",
    "        for lang in TOPIC_CROSS_LANGS:\n",
    "            if isinstance(lang, str) and lang.strip().lower() not in target_langs:\n",
    "                target_langs.append(lang.strip().lower())\n",
    "    for lang in target_langs:\n",
    "        for source_name in combined.get(lang, {}).get(\"sources\", []):\n",
    "            source_path = DATA_DIR / source_name\n",
    "            with source_path.open() as f:\n",
    "                payload = json.load(f)\n",
    "            for topic_item in payload.get(\"metadata\", {}).get(\"topics\", []):\n",
    "                topic = str(topic_item.get(\"topic\", \"\")).strip().lower()\n",
    "                if not topic:\n",
    "                    continue\n",
    "                keywords = {\n",
    "                    str(keyword).strip().lower()\n",
    "                    for keyword in topic_item.get(\"keywords\", [])\n",
    "                    if str(keyword).strip()\n",
    "                }\n",
    "                keywords.add(topic)\n",
    "                topic_keywords.setdefault(topic, set()).update(keywords)\n",
    "    return {topic: sorted(values, key=len, reverse=True) for topic, values in topic_keywords.items()}\n",
    "def add_topic_features(df, topic_keywords, match_mode):\n",
    "    out = df.copy()\n",
    "    text_lower = out[\"text_clean\"].str.lower()\n",
    "    topic_cols = []\n",
    "    for topic, keywords in topic_keywords.items():\n",
    "        col = f\"topic_{topic}\"\n",
    "        topic_cols.append(col)\n",
    "        if not keywords:\n",
    "            out[col] = 0\n",
    "            continue\n",
    "        if match_mode == \"word\":\n",
    "            pattern = \"|\".join(rf\"\\\\b{re.escape(keyword)}\\\\b\" for keyword in keywords)\n",
    "        else:\n",
    "            pattern = \"|\".join(re.escape(keyword) for keyword in keywords)\n",
    "        out[col] = text_lower.str.contains(pattern, regex=True).astype(np.int8)\n",
    "    return out, topic_cols\n",
    "\n",
    "\n",
    "def compute_account_score(author_ids, true_labels, pred_probs, threshold, decision_rule):\n",
    "    tmp = pd.DataFrame(\n",
    "        {\n",
    "            \"author_id\": author_ids,\n",
    "            \"true_is_bot\": np.asarray(true_labels, dtype=np.int64),\n",
    "            \"pred_prob\": np.asarray(pred_probs, dtype=np.float32),\n",
    "        }\n",
    "    )\n",
    "    tmp[\"pred_post\"] = (tmp[\"pred_prob\"] >= threshold).astype(np.int64)\n",
    "\n",
    "    account = (\n",
    "        tmp.groupby(\"author_id\", as_index=False)\n",
    "        .agg(\n",
    "            true_is_bot=(\"true_is_bot\", \"max\"),\n",
    "            mean_prob=(\"pred_prob\", \"mean\"),\n",
    "            any_pred=(\"pred_post\", \"max\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if decision_rule == \"any\":\n",
    "        account[\"pred_is_bot\"] = account[\"any_pred\"].astype(np.int64)\n",
    "    else:\n",
    "        account[\"pred_is_bot\"] = (account[\"mean_prob\"] >= threshold).astype(np.int64)\n",
    "\n",
    "    tp_accounts = int(((account[\"true_is_bot\"] == 1) & (account[\"pred_is_bot\"] == 1)).sum())\n",
    "    fn_accounts = int(((account[\"true_is_bot\"] == 1) & (account[\"pred_is_bot\"] == 0)).sum())\n",
    "    fp_accounts = int(((account[\"true_is_bot\"] == 0) & (account[\"pred_is_bot\"] == 1)).sum())\n",
    "\n",
    "    score = (4 * tp_accounts) - (1 * fn_accounts) - (2 * fp_accounts)\n",
    "    return score, tp_accounts, fn_accounts, fp_accounts, len(account)\n",
    "\n",
    "\n",
    "def predict_for_indices(model, post_indices):\n",
    "    if len(post_indices) == 0:\n",
    "        return np.array([], dtype=np.float32)\n",
    "    return model.predict(\n",
    "        {\n",
    "            \"input_ids\": input_ids_en[post_indices],\n",
    "            \"attention_mask\": attention_mask_en[post_indices],\n",
    "            \"aux_features\": X_aux_en[post_indices],\n",
    "        },\n",
    "        verbose=0,\n",
    "    ).ravel()\n",
    "\n",
    "\n",
    "if USE_TOPIC_FEATURES:\n",
    "    topic_keywords_en = load_target_topic_keywords()\n",
    "    train_en_model, topic_feature_cols_en = add_topic_features(train_en, topic_keywords_en, TOPIC_MATCH_MODE)\n",
    "else:\n",
    "    train_en_model = train_en.copy()\n",
    "    topic_feature_cols_en = []\n",
    "\n",
    "input_ids_en = np.asarray(encodings_en[\"input_ids\"], dtype=np.int32)\n",
    "attention_mask_en = np.asarray(encodings_en[\"attention_mask\"], dtype=np.float32)\n",
    "X_topic_en = (\n",
    "    train_en_model[topic_feature_cols_en].to_numpy(dtype=np.float32)\n",
    "    if topic_feature_cols_en\n",
    "    else np.zeros((len(train_en_model), 0), dtype=np.float32)\n",
    ")\n",
    "if TOPIC_FEATURE_SCALE != 1.0:\n",
    "    X_topic_en = X_topic_en * TOPIC_FEATURE_SCALE\n",
    "X_aux_en = np.concatenate([X_meta_en_scaled, X_topic_en], axis=1)\n",
    "\n",
    "all_post_idx = np.arange(len(y_en))\n",
    "\n",
    "if SPLIT_BY_AUTHOR:\n",
    "    author_labels_df = (\n",
    "        train_en_model.groupby(\"author_id\", as_index=False)[\"is_bot\"]\n",
    "        .max()\n",
    "        .rename(columns={\"is_bot\": \"account_is_bot\"})\n",
    "    )\n",
    "    all_authors = author_labels_df[\"author_id\"].to_numpy()\n",
    "    all_author_labels = author_labels_df[\"account_is_bot\"].to_numpy(dtype=np.int64)\n",
    "\n",
    "    train_authors, test_authors = train_test_split(\n",
    "        all_authors,\n",
    "        test_size=TEST_SIZE,\n",
    "        random_state=RANDOM_SEED,\n",
    "        stratify=all_author_labels,\n",
    "    )\n",
    "\n",
    "    if VALIDATION_SPLIT > 0:\n",
    "        train_author_labels = (\n",
    "            author_labels_df.set_index(\"author_id\").loc[train_authors, \"account_is_bot\"].to_numpy(dtype=np.int64)\n",
    "        )\n",
    "        fit_authors, val_authors = train_test_split(\n",
    "            train_authors,\n",
    "            test_size=VALIDATION_SPLIT,\n",
    "            random_state=RANDOM_SEED,\n",
    "            stratify=train_author_labels,\n",
    "        )\n",
    "    else:\n",
    "        fit_authors = train_authors\n",
    "        val_authors = np.array([], dtype=all_authors.dtype)\n",
    "\n",
    "    fit_author_set = set(fit_authors.tolist())\n",
    "    val_author_set = set(val_authors.tolist())\n",
    "    test_author_set = set(test_authors.tolist())\n",
    "\n",
    "    fit_idx = np.flatnonzero(train_en_model[\"author_id\"].isin(fit_author_set).to_numpy())\n",
    "    val_idx = np.flatnonzero(train_en_model[\"author_id\"].isin(val_author_set).to_numpy())\n",
    "    test_idx = np.flatnonzero(train_en_model[\"author_id\"].isin(test_author_set).to_numpy())\n",
    "    split_mode = \"author\"\n",
    "else:\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        all_post_idx,\n",
    "        test_size=TEST_SIZE,\n",
    "        random_state=RANDOM_SEED,\n",
    "        stratify=y_en,\n",
    "    )\n",
    "\n",
    "    if VALIDATION_SPLIT > 0:\n",
    "        fit_idx, val_idx = train_test_split(\n",
    "            train_idx,\n",
    "            test_size=VALIDATION_SPLIT,\n",
    "            random_state=RANDOM_SEED,\n",
    "            stratify=y_en[train_idx],\n",
    "        )\n",
    "    else:\n",
    "        fit_idx = train_idx\n",
    "        val_idx = np.array([], dtype=np.int64)\n",
    "\n",
    "    split_mode = \"post\"\n",
    "\n",
    "X_fit_ids, X_test_ids = input_ids_en[fit_idx], input_ids_en[test_idx]\n",
    "X_fit_mask, X_test_mask = attention_mask_en[fit_idx], attention_mask_en[test_idx]\n",
    "X_fit_aux, X_test_aux = X_aux_en[fit_idx], X_aux_en[test_idx]\n",
    "y_fit, y_test = y_en[fit_idx], y_en[test_idx]\n",
    "\n",
    "X_val_ids = input_ids_en[val_idx] if len(val_idx) else None\n",
    "X_val_mask = attention_mask_en[val_idx] if len(val_idx) else None\n",
    "X_val_aux = X_aux_en[val_idx] if len(val_idx) else None\n",
    "y_val = y_en[val_idx] if len(val_idx) else None\n",
    "\n",
    "fit_author_ids = np.unique(train_en_model.iloc[fit_idx][\"author_id\"].to_numpy())\n",
    "val_author_ids = np.unique(train_en_model.iloc[val_idx][\"author_id\"].to_numpy()) if len(val_idx) else np.array([])\n",
    "test_author_ids = np.unique(train_en_model.iloc[test_idx][\"author_id\"].to_numpy())\n",
    "\n",
    "class_weight_dict = None\n",
    "if USE_CLASS_WEIGHTS:\n",
    "    classes = np.unique(y_fit)\n",
    "    class_weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_fit)\n",
    "    class_weight_dict = {int(c): float(w) for c, w in zip(classes, class_weights)}\n",
    "\n",
    "\n",
    "def build_multifeature_model(\n",
    "    vocab_size,\n",
    "    seq_len,\n",
    "    aux_dim,\n",
    "    embedding_dim,\n",
    "    gru_units,\n",
    "    aux_dense_units,\n",
    "    head_dense_units,\n",
    "    dropout_text,\n",
    "    dropout_aux,\n",
    "    dropout_head,\n",
    "    learning_rate,\n",
    "):\n",
    "    ids_input = tf.keras.layers.Input(shape=(seq_len,), dtype=\"int32\", name=\"input_ids\")\n",
    "    mask_input = tf.keras.layers.Input(shape=(seq_len,), dtype=\"float32\", name=\"attention_mask\")\n",
    "    aux_input = tf.keras.layers.Input(shape=(aux_dim,), dtype=\"float32\", name=\"aux_features\")\n",
    "\n",
    "    x = tf.keras.layers.Embedding(vocab_size, embedding_dim, name=\"token_embedding\")(ids_input)\n",
    "    mask = tf.keras.layers.Reshape((seq_len, 1))(mask_input)\n",
    "    x = tf.keras.layers.Multiply()([x, mask])\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(gru_units))(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_text)(x)\n",
    "\n",
    "    aux = tf.keras.layers.Dense(aux_dense_units, activation=\"relu\")(aux_input)\n",
    "    aux = tf.keras.layers.Dropout(dropout_aux)(aux)\n",
    "\n",
    "    merged = tf.keras.layers.Concatenate()([x, aux])\n",
    "    merged = tf.keras.layers.Dense(head_dense_units, activation=\"relu\")(merged)\n",
    "    merged = tf.keras.layers.Dropout(dropout_head)(merged)\n",
    "    output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(merged)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[ids_input, mask_input, aux_input], outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy(name=\"accuracy\"), tf.keras.metrics.AUC(name=\"auc\")],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model_en = build_multifeature_model(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    seq_len=MAX_LENGTH,\n",
    "    aux_dim=X_fit_aux.shape[1],\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    gru_units=GRU_UNITS,\n",
    "    aux_dense_units=AUX_DENSE_UNITS,\n",
    "    head_dense_units=HEAD_DENSE_UNITS,\n",
    "    dropout_text=DROPOUT_TEXT,\n",
    "    dropout_aux=DROPOUT_AUX,\n",
    "    dropout_head=DROPOUT_HEAD,\n",
    "    learning_rate=LEARNING_RATE,\n",
    ")\n",
    "\n",
    "has_validation = len(val_idx) > 0\n",
    "callbacks = []\n",
    "if has_validation and EARLY_STOPPING_PATIENCE > 0:\n",
    "    callbacks.append(\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_auc\",\n",
    "            mode=\"max\",\n",
    "            patience=EARLY_STOPPING_PATIENCE,\n",
    "            restore_best_weights=True,\n",
    "        )\n",
    "    )\n",
    "if has_validation and REDUCE_LR_PATIENCE > 0:\n",
    "    callbacks.append(\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_auc\",\n",
    "            mode=\"max\",\n",
    "            factor=REDUCE_LR_FACTOR,\n",
    "            patience=REDUCE_LR_PATIENCE,\n",
    "            min_lr=REDUCE_LR_MIN_LR,\n",
    "        )\n",
    "    )\n",
    "\n",
    "train_inputs = {\n",
    "    \"input_ids\": X_fit_ids,\n",
    "    \"attention_mask\": X_fit_mask,\n",
    "    \"aux_features\": X_fit_aux,\n",
    "}\n",
    "\n",
    "fit_kwargs = {\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"class_weight\": class_weight_dict,\n",
    "    \"callbacks\": callbacks,\n",
    "    \"verbose\": 1,\n",
    "}\n",
    "if has_validation:\n",
    "    fit_kwargs[\"validation_data\"] = (\n",
    "        {\n",
    "            \"input_ids\": X_val_ids,\n",
    "            \"attention_mask\": X_val_mask,\n",
    "            \"aux_features\": X_val_aux,\n",
    "        },\n",
    "        y_val,\n",
    "    )\n",
    "\n",
    "history_en = model_en.fit(train_inputs, y_fit, **fit_kwargs)\n",
    "\n",
    "post_prob_fit = predict_for_indices(model_en, fit_idx)\n",
    "post_prob_val = predict_for_indices(model_en, val_idx)\n",
    "post_prob_test = predict_for_indices(model_en, test_idx)\n",
    "\n",
    "y_prob = post_prob_test.copy()\n",
    "threshold_search_results_en = pd.DataFrame()\n",
    "SELECTED_THRESHOLD = PREDICTION_THRESHOLD\n",
    "best_threshold_val_score = None\n",
    "\n",
    "if has_validation and USE_THRESHOLD_SEARCH:\n",
    "    val_author_ids_for_score = train_en_model.iloc[val_idx][\"author_id\"].to_numpy()\n",
    "    search_rows = []\n",
    "    for threshold in np.linspace(THRESHOLD_SEARCH_MIN, THRESHOLD_SEARCH_MAX, THRESHOLD_SEARCH_STEPS):\n",
    "        score, tp_acc, fn_acc, fp_acc, n_accounts = compute_account_score(\n",
    "            author_ids=val_author_ids_for_score,\n",
    "            true_labels=y_val,\n",
    "            pred_probs=post_prob_val,\n",
    "            threshold=float(threshold),\n",
    "            decision_rule=ACCOUNT_DECISION_RULE,\n",
    "        )\n",
    "        search_rows.append(\n",
    "            {\n",
    "                \"threshold\": float(threshold),\n",
    "                \"score\": int(score),\n",
    "                \"tp_accounts\": int(tp_acc),\n",
    "                \"fn_accounts\": int(fn_acc),\n",
    "                \"fp_accounts\": int(fp_acc),\n",
    "                \"n_accounts\": int(n_accounts),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    threshold_search_results_en = pd.DataFrame(search_rows)\n",
    "    best_row = threshold_search_results_en.sort_values(\n",
    "        by=[\"score\", \"fp_accounts\", \"tp_accounts\", \"threshold\"],\n",
    "        ascending=[False, True, False, False],\n",
    "    ).iloc[0]\n",
    "\n",
    "    SELECTED_THRESHOLD = float(best_row[\"threshold\"])\n",
    "    best_threshold_val_score = int(best_row[\"score\"])\n",
    "\n",
    "y_pred = (y_prob >= SELECTED_THRESHOLD).astype(np.int64)\n",
    "\n",
    "test_author_ids_for_score = train_en_model.iloc[test_idx][\"author_id\"].to_numpy()\n",
    "(\n",
    "    test_score,\n",
    "    test_tp_accounts,\n",
    "    test_fn_accounts,\n",
    "    test_fp_accounts,\n",
    "    test_n_accounts,\n",
    ") = compute_account_score(\n",
    "    author_ids=test_author_ids_for_score,\n",
    "    true_labels=y_test,\n",
    "    pred_probs=y_prob,\n",
    "    threshold=SELECTED_THRESHOLD,\n",
    "    decision_rule=ACCOUNT_DECISION_RULE,\n",
    ")\n",
    "\n",
    "(\n",
    "    baseline_test_score,\n",
    "    baseline_tp_accounts,\n",
    "    baseline_fn_accounts,\n",
    "    baseline_fp_accounts,\n",
    "    baseline_n_accounts,\n",
    ") = compute_account_score(\n",
    "    author_ids=test_author_ids_for_score,\n",
    "    true_labels=y_test,\n",
    "    pred_probs=y_prob,\n",
    "    threshold=PREDICTION_THRESHOLD,\n",
    "    decision_rule=ACCOUNT_DECISION_RULE,\n",
    ")\n",
    "\n",
    "print(f\"Split mode: {split_mode}\")\n",
    "print(f\"Topic features enabled: {USE_TOPIC_FEATURES}\")\n",
    "print(f\"Topic match mode: {TOPIC_MATCH_MODE}\")\n",
    "print(f\"Account decision rule: {ACCOUNT_DECISION_RULE}\")\n",
    "print(\"Topic columns:\", topic_feature_cols_en)\n",
    "print(f\"Split sizes (fit/val/test posts): {len(fit_idx)}/{len(val_idx)}/{len(test_idx)}\")\n",
    "print(\n",
    "    f\"Split sizes (fit/val/test accounts): {len(fit_author_ids)}/{len(val_author_ids)}/{len(test_author_ids)}\"\n",
    ")\n",
    "print(f\"Default threshold from config: {PREDICTION_THRESHOLD:.4f}\")\n",
    "print(f\"Selected threshold used on test: {SELECTED_THRESHOLD:.4f}\")\n",
    "if best_threshold_val_score is not None:\n",
    "    print(f\"Best validation score from threshold search: {best_threshold_val_score}\")\n",
    "print(f\"Test Accuracy (post-level): {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Test ROC-AUC (post-level): {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "print(\n",
    "    f\"Test account score @ selected threshold -> score={test_score}, TP={test_tp_accounts}, FN={test_fn_accounts}, FP={test_fp_accounts}, accounts={test_n_accounts}\"\n",
    ")\n",
    "print(\n",
    "    f\"Test account score @ config threshold -> score={baseline_test_score}, TP={baseline_tp_accounts}, FN={baseline_fn_accounts}, FP={baseline_fp_accounts}, accounts={baseline_n_accounts}\"\n",
    ")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f66a209",
   "metadata": {},
   "source": [
    "## Account-Level Threshold Calibration (No Booster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "a48c726f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated on val: threshold=0.475, rule=mean\n",
      "Score=23 (TP=6, FN=1, FP=0)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if \"post_prob_val\" in globals() and len(val_idx):\n",
    "    eval_idx = val_idx\n",
    "    eval_probs = np.asarray(post_prob_val, dtype=np.float32)\n",
    "    split_name = \"val\"\n",
    "else:\n",
    "    eval_idx = test_idx\n",
    "    eval_probs = np.asarray(post_prob_test, dtype=np.float32)\n",
    "    split_name = \"test\"\n",
    "\n",
    "thresholds = np.linspace(0.05, 0.95, 181)\n",
    "rules = [\"mean\", \"any\"]\n",
    "\n",
    "def _score_accounts(author_ids, true_labels, probs, threshold, rule):\n",
    "    df = pd.DataFrame({\n",
    "        \"author_id\": author_ids,\n",
    "        \"true_is_bot\": true_labels.astype(np.int64),\n",
    "        \"pred_prob\": probs.astype(np.float32),\n",
    "    })\n",
    "    df[\"pred_post\"] = (df[\"pred_prob\"] >= threshold).astype(np.int64)\n",
    "    account_df = df.groupby(\"author_id\", as_index=False).agg(\n",
    "        true_is_bot=(\"true_is_bot\", \"max\"),\n",
    "        mean_prob=(\"pred_prob\", \"mean\"),\n",
    "        any_pred=(\"pred_post\", \"max\"),\n",
    "    )\n",
    "    if rule == \"any\":\n",
    "        account_df[\"pred_is_bot\"] = account_df[\"any_pred\"].astype(np.int64)\n",
    "    else:\n",
    "        account_df[\"pred_is_bot\"] = (account_df[\"mean_prob\"] >= threshold).astype(np.int64)\n",
    "    tp = int(((account_df[\"true_is_bot\"] == 1) & (account_df[\"pred_is_bot\"] == 1)).sum())\n",
    "    fn = int(((account_df[\"true_is_bot\"] == 1) & (account_df[\"pred_is_bot\"] == 0)).sum())\n",
    "    fp = int(((account_df[\"true_is_bot\"] == 0) & (account_df[\"pred_is_bot\"] == 1)).sum())\n",
    "    score = (4 * tp) - fn - (2 * fp)\n",
    "    return score, tp, fn, fp\n",
    "\n",
    "best = {\"score\": -1e9}\n",
    "author_ids = train_en_model.iloc[eval_idx][\"author_id\"].to_numpy()\n",
    "true_labels = train_en_model.iloc[eval_idx][\"is_bot\"].to_numpy()\n",
    "\n",
    "for rule in rules:\n",
    "    for thr in thresholds:\n",
    "        score, tp, fn, fp = _score_accounts(author_ids, true_labels, eval_probs, thr, rule)\n",
    "        if score > best[\"score\"]:\n",
    "            best = {\"score\": score, \"threshold\": float(thr), \"rule\": rule, \"tp\": tp, \"fn\": fn, \"fp\": fp}\n",
    "\n",
    "SELECTED_THRESHOLD = float(best[\"threshold\"])\n",
    "ACCOUNT_DECISION_RULE = str(best[\"rule\"])\n",
    "\n",
    "print(f\"Calibrated on {split_name}: threshold={SELECTED_THRESHOLD:.3f}, rule={ACCOUNT_DECISION_RULE}\")\n",
    "print(f\"Score={best[\"score\"]} (TP={best[\"tp\"]}, FN={best[\"fn\"]}, FP={best[\"fp\"]})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fa6c0f",
   "metadata": {},
   "source": [
    "## Lightweight Account Model (Logistic Regression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "3bc2e0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg threshold (val): 0.460 score=23 TP=6 FN=1 FP=0\n",
      "LogReg test score: 37 TP=10 FN=1 FP=1 TN=55\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "USE_LOGREG = bool(EXPERIMENT_CONFIG.get(\"use_logreg_account_model\", False))\n",
    "if not USE_LOGREG:\n",
    "    print(\"Logistic-regression account model disabled. Set use_logreg_account_model=True to enable.\")\n",
    "else:\n",
    "    if \"post_prob_fit\" not in globals() or \"post_prob_test\" not in globals():\n",
    "        raise ValueError(\"Run the first-stage training cell first (post_prob_fit/post_prob_test missing).\")\n",
    "\n",
    "    min_posts = int(EXPERIMENT_CONFIG.get(\"logreg_min_posts\", 2))\n",
    "    max_iter = int(EXPERIMENT_CONFIG.get(\"logreg_max_iter\", 200))\n",
    "\n",
    "    def _account_features(author_ids, probs):\n",
    "        df = pd.DataFrame({\"author_id\": author_ids, \"prob\": probs.astype(np.float32)})\n",
    "        df[\"pred_05\"] = (df[\"prob\"] >= 0.5).astype(np.int64)\n",
    "        agg = df.groupby(\"author_id\", as_index=True).agg(\n",
    "            n_posts=(\"prob\", \"size\"),\n",
    "            mean_prob=(\"prob\", \"mean\"),\n",
    "            std_prob=(\"prob\", \"std\"),\n",
    "            min_prob=(\"prob\", \"min\"),\n",
    "            max_prob=(\"prob\", \"max\"),\n",
    "            frac_above_05=(\"pred_05\", \"mean\"),\n",
    "        )\n",
    "        agg = agg.fillna(0.0)\n",
    "        agg = agg[agg[\"n_posts\"] >= min_posts]\n",
    "        return agg\n",
    "\n",
    "    def _account_labels(indices):\n",
    "        return (\n",
    "            train_en_model.iloc[indices][[\"author_id\", \"is_bot\"]]\n",
    "            .groupby(\"author_id\", as_index=True)[\"is_bot\"]\n",
    "            .max()\n",
    "        )\n",
    "\n",
    "    fit_auth = train_en_model.iloc[fit_idx][\"author_id\"].to_numpy()\n",
    "    fit_features = _account_features(fit_auth, np.asarray(post_prob_fit, dtype=np.float32))\n",
    "    fit_labels = _account_labels(fit_idx).reindex(fit_features.index).to_numpy(dtype=np.int64)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_fit = scaler.fit_transform(fit_features.to_numpy())\n",
    "    lr_model = LogisticRegression(max_iter=max_iter, class_weight=\"balanced\")\n",
    "    lr_model.fit(X_fit, fit_labels)\n",
    "\n",
    "    # Use validation split for threshold selection if available\n",
    "    if \"post_prob_val\" in globals() and len(val_idx):\n",
    "        val_auth = train_en_model.iloc[val_idx][\"author_id\"].to_numpy()\n",
    "        val_features = _account_features(val_auth, np.asarray(post_prob_val, dtype=np.float32))\n",
    "        val_labels = _account_labels(val_idx).reindex(val_features.index).to_numpy(dtype=np.int64)\n",
    "        X_val = scaler.transform(val_features.to_numpy())\n",
    "        val_probs = lr_model.predict_proba(X_val)[:, 1]\n",
    "        thresholds = np.linspace(0.05, 0.95, 181)\n",
    "        best = {\"score\": -1e9}\n",
    "        for thr in thresholds:\n",
    "            pred = (val_probs >= thr).astype(np.int64)\n",
    "            tp = int(((val_labels == 1) & (pred == 1)).sum())\n",
    "            fn = int(((val_labels == 1) & (pred == 0)).sum())\n",
    "            fp = int(((val_labels == 0) & (pred == 1)).sum())\n",
    "            score = (4 * tp) - fn - (2 * fp)\n",
    "            if score > best[\"score\"]:\n",
    "                best = {\"score\": score, \"threshold\": float(thr), \"tp\": tp, \"fn\": fn, \"fp\": fp}\n",
    "        lr_threshold = float(best[\"threshold\"])\n",
    "        print(f\"LogReg threshold (val): {lr_threshold:.3f} score={best[\"score\"]} TP={best[\"tp\"]} FN={best[\"fn\"]} FP={best[\"fp\"]}\")\n",
    "    else:\n",
    "        lr_threshold = 0.5\n",
    "        print(\"No validation split; using default logreg threshold 0.5\")\n",
    "\n",
    "    # Evaluate on test\n",
    "    test_auth = train_en_model.iloc[test_idx][\"author_id\"].to_numpy()\n",
    "    test_features = _account_features(test_auth, np.asarray(post_prob_test, dtype=np.float32))\n",
    "    test_labels = _account_labels(test_idx).reindex(test_features.index).to_numpy(dtype=np.int64)\n",
    "    X_test = scaler.transform(test_features.to_numpy())\n",
    "    test_probs = lr_model.predict_proba(X_test)[:, 1]\n",
    "    test_pred = (test_probs >= lr_threshold).astype(np.int64)\n",
    "    tp = int(((test_labels == 1) & (test_pred == 1)).sum())\n",
    "    fn = int(((test_labels == 1) & (test_pred == 0)).sum())\n",
    "    fp = int(((test_labels == 0) & (test_pred == 1)).sum())\n",
    "    tn = int(((test_labels == 0) & (test_pred == 0)).sum())\n",
    "    score = (4 * tp) - fn - (2 * fp)\n",
    "    print(f\"LogReg test score: {score} TP={tp} FN={fn} FP={fp} TN={tn}\")\n",
    "\n",
    "    logreg_account_predictions = pd.DataFrame({\n",
    "        \"author_id\": test_features.index.to_numpy(),\n",
    "        \"true_is_bot\": test_labels,\n",
    "        \"pred_prob\": test_probs,\n",
    "        \"pred_is_bot\": test_pred,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a6a2a8",
   "metadata": {},
   "source": [
    "## Bot-detector score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "e84ff0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold used: 0.4750\n",
      "Account decision rule: mean\n",
      "Accounts scored (test split): 69\n",
      "TP: 10  FN: 1  FP: 0  TN: 58\n",
      "Bot detector score: 39\n",
      "Score out of max possible: 39/44 (88.64%)\n",
      "Score range on this split: [-127, 44]\n",
      "Range-normalized score: 97.08%\n",
      "Second-stage account-model score (recommended): 37 at threshold 0.5000\n"
     ]
    }
   ],
   "source": [
    "if any(name not in globals() for name in [\"train_en_model\", \"test_idx\", \"y_prob\", \"PREDICTION_THRESHOLD\"]):\n",
    "    raise ValueError(\"Run the Train-Test split for model cell first.\")\n",
    "\n",
    "threshold_for_score = float(globals().get(\"SELECTED_THRESHOLD\", PREDICTION_THRESHOLD))\n",
    "if \"ACCOUNT_DECISION_RULE\" in globals():\n",
    "    account_decision_rule = str(ACCOUNT_DECISION_RULE)\n",
    "elif \"EXPERIMENT_CONFIG\" in globals():\n",
    "    account_decision_rule = str(EXPERIMENT_CONFIG.get(\"account_decision_rule\", \"mean\"))\n",
    "else:\n",
    "    account_decision_rule = \"mean\"\n",
    "\n",
    "if account_decision_rule not in {\"mean\", \"any\"}:\n",
    "    raise ValueError('account_decision_rule must be \"mean\" or \"any\".')\n",
    "\n",
    "# Build test-set account labels/predictions from post-level outputs.\n",
    "score_df = train_en_model.iloc[test_idx][[\"author_id\", \"is_bot\"]].copy()\n",
    "score_df[\"pred_prob\"] = y_prob\n",
    "score_df[\"pred_post\"] = (score_df[\"pred_prob\"] >= threshold_for_score).astype(np.int64)\n",
    "\n",
    "account_df = (\n",
    "    score_df.groupby(\"author_id\", as_index=False)\n",
    "    .agg(\n",
    "        true_is_bot=(\"is_bot\", \"max\"),\n",
    "        mean_prob=(\"pred_prob\", \"mean\"),\n",
    "        any_pred=(\"pred_post\", \"max\"),\n",
    "        n_posts=(\"pred_post\", \"size\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "if account_decision_rule == \"any\":\n",
    "    account_df[\"pred_is_bot\"] = account_df[\"any_pred\"].astype(np.int64)\n",
    "else:\n",
    "    account_df[\"pred_is_bot\"] = (account_df[\"mean_prob\"] >= threshold_for_score).astype(np.int64)\n",
    "\n",
    "tp_accounts = int(((account_df[\"true_is_bot\"] == 1) & (account_df[\"pred_is_bot\"] == 1)).sum())\n",
    "fn_accounts = int(((account_df[\"true_is_bot\"] == 1) & (account_df[\"pred_is_bot\"] == 0)).sum())\n",
    "fp_accounts = int(((account_df[\"true_is_bot\"] == 0) & (account_df[\"pred_is_bot\"] == 1)).sum())\n",
    "tn_accounts = int(((account_df[\"true_is_bot\"] == 0) & (account_df[\"pred_is_bot\"] == 0)).sum())\n",
    "\n",
    "score = (4 * tp_accounts) - (1 * fn_accounts) - (2 * fp_accounts)\n",
    "max_possible_score = 4 * int((account_df[\"true_is_bot\"] == 1).sum())\n",
    "min_possible_score = (\n",
    "    -1 * int((account_df[\"true_is_bot\"] == 1).sum())\n",
    "    -2 * int((account_df[\"true_is_bot\"] == 0).sum())\n",
    ")\n",
    "\n",
    "score_ratio = score / max_possible_score if max_possible_score > 0 else np.nan\n",
    "score_normalized = (\n",
    "    (score - min_possible_score) / (max_possible_score - min_possible_score)\n",
    "    if max_possible_score != min_possible_score\n",
    "    else np.nan\n",
    ")\n",
    "\n",
    "print(f\"Threshold used: {threshold_for_score:.4f}\")\n",
    "print(f\"Account decision rule: {account_decision_rule}\")\n",
    "print(f\"Accounts scored (test split): {len(account_df)}\")\n",
    "print(f\"TP: {tp_accounts}  FN: {fn_accounts}  FP: {fp_accounts}  TN: {tn_accounts}\")\n",
    "print(f\"Bot detector score: {score}\")\n",
    "print(f\"Score out of max possible: {score}/{max_possible_score} ({score_ratio:.2%})\")\n",
    "print(f\"Score range on this split: [{min_possible_score}, {max_possible_score}]\")\n",
    "print(f\"Range-normalized score: {score_normalized:.2%}\")\n",
    "\n",
    "if \"second_stage_test_score_en\" in globals():\n",
    "    print(\n",
    "        f\"Second-stage account-model score (recommended): {second_stage_test_score_en} at threshold {second_stage_selected_threshold_en:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2db3f5",
   "metadata": {},
   "source": [
    "## Strongest booster (account-level second stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "a1c7e2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:09:42.467618: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-10 12:09:47.653683: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-10 12:09:57.536874: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-10 12:10:38.814504: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-10 12:10:44.373362: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-10 12:10:49.461402: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-10 12:11:01.873271: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-10 12:11:57.098374: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-10 12:12:03.466559: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-10 12:12:08.865068: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-10 12:12:21.472363: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-10 12:13:05.049257: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-10 12:13:10.774101: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-10 12:13:28.768264: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-10 12:14:17.916387: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-10 12:14:23.679396: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-10 12:14:28.746751: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-10 12:14:40.055353: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-10 12:15:32.245151: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-10 12:15:38.380235: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-10 12:15:55.925072: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-10 12:16:47.880254: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-10 12:16:53.865812: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-10 12:17:08.837198: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-10 12:17:58.303852: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed-level first-stage account scores (test):\n",
      " seed  threshold  test_score  tp_accounts  fn_accounts  fp_accounts\n",
      "   13      0.465          39           10            1            0\n",
      "   29      0.470          39           10            1            0\n",
      "   42      0.425          39           10            1            0\n",
      "   73      0.220          38           11            0            3\n",
      "  101      0.340          40           11            0            2\n",
      "  137      0.335          39           10            1            0\n",
      "  173      0.475          39           10            1            0\n",
      "Seed score mean/std: 39.00 / 0.58\n",
      "Ensemble aggregation: median\n",
      "Ensemble selected threshold: 0.4150\n",
      "Ensemble test score: 39 (TP=10, FN=1, FP=0, accounts=69)\n",
      "Second-stage account model disabled in EXPERIMENT_CONFIG.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "except ImportError as exc:\n",
    "    raise ImportError(\"Install catboost first: pip install catboost\") from exc\n",
    "\n",
    "required_globals = [\n",
    "    \"tf\",\n",
    "    \"EXPERIMENT_CONFIG\",\n",
    "    \"build_multifeature_model\",\n",
    "    \"compute_account_score\",\n",
    "    \"train_en_model\",\n",
    "    \"topic_feature_cols_en\",\n",
    "    \"fit_idx\",\n",
    "    \"val_idx\",\n",
    "    \"test_idx\",\n",
    "    \"input_ids_en\",\n",
    "    \"attention_mask_en\",\n",
    "    \"X_aux_en\",\n",
    "    \"y_en\",\n",
    "    \"MAX_LENGTH\",\n",
    "    \"tokenizer\",\n",
    "    \"users_en\",\n",
    "    \"EMBEDDING_DIM\",\n",
    "    \"GRU_UNITS\",\n",
    "    \"AUX_DENSE_UNITS\",\n",
    "    \"HEAD_DENSE_UNITS\",\n",
    "    \"DROPOUT_TEXT\",\n",
    "    \"DROPOUT_AUX\",\n",
    "    \"DROPOUT_HEAD\",\n",
    "    \"LEARNING_RATE\",\n",
    "    \"EPOCHS\",\n",
    "    \"BATCH_SIZE\",\n",
    "    \"RANDOM_SEED\",\n",
    "    \"USE_CLASS_WEIGHTS\",\n",
    "    \"class_weight_dict\",\n",
    "    \"USE_THRESHOLD_SEARCH\",\n",
    "    \"THRESHOLD_SEARCH_MIN\",\n",
    "    \"THRESHOLD_SEARCH_MAX\",\n",
    "    \"THRESHOLD_SEARCH_STEPS\",\n",
    "    \"PREDICTION_THRESHOLD\",\n",
    "    \"ACCOUNT_DECISION_RULE\",\n",
    "    \"EARLY_STOPPING_PATIENCE\",\n",
    "    \"REDUCE_LR_PATIENCE\",\n",
    "    \"REDUCE_LR_FACTOR\",\n",
    "    \"REDUCE_LR_MIN_LR\",\n",
    "]\n",
    "missing = [name for name in required_globals if name not in globals()]\n",
    "if missing:\n",
    "    raise ValueError(f\"Run the Train-Test split for model cell first. Missing: {missing}\")\n",
    "\n",
    "if ACCOUNT_DECISION_RULE not in {\"mean\", \"any\"}:\n",
    "    raise ValueError('account_decision_rule must be \"mean\" or \"any\".')\n",
    "\n",
    "ensemble_seeds_cfg = EXPERIMENT_CONFIG.get(\"ensemble_seeds\", [RANDOM_SEED])\n",
    "if isinstance(ensemble_seeds_cfg, (int, np.integer)):\n",
    "    ensemble_seeds = [int(ensemble_seeds_cfg)]\n",
    "elif isinstance(ensemble_seeds_cfg, (list, tuple)):\n",
    "    ensemble_seeds = [int(seed) for seed in ensemble_seeds_cfg]\n",
    "else:\n",
    "    raise ValueError(\"ensemble_seeds must be an int or a list of ints.\")\n",
    "ensemble_seeds = list(dict.fromkeys(ensemble_seeds))\n",
    "if not ensemble_seeds:\n",
    "    raise ValueError(\"ensemble_seeds cannot be empty.\")\n",
    "\n",
    "ENSEMBLE_AGGREGATION = str(EXPERIMENT_CONFIG.get(\"ensemble_aggregation\", \"mean\")).lower()\n",
    "if ENSEMBLE_AGGREGATION not in {\"mean\", \"median\"}:\n",
    "    raise ValueError('ensemble_aggregation must be \"mean\" or \"median\".')\n",
    "\n",
    "USE_EXTERNAL_PRETRAIN = bool(EXPERIMENT_CONFIG.get(\"use_external_pretrain\", False))\n",
    "EXTERNAL_PRETRAIN_EPOCHS = int(EXPERIMENT_CONFIG.get(\"external_pretrain_epochs\", 1))\n",
    "EXTERNAL_PRETRAIN_BATCH_SIZE = int(EXPERIMENT_CONFIG.get(\"external_pretrain_batch_size\", 128))\n",
    "EXTERNAL_PRETRAIN_USE_BALANCED_WEIGHTS = bool(\n",
    "    EXPERIMENT_CONFIG.get(\"external_pretrain_use_balanced_weights\", True)\n",
    ")\n",
    "\n",
    "USE_SECOND_STAGE_ACCOUNT_MODEL = bool(EXPERIMENT_CONFIG.get(\"use_second_stage_account_model\", True))\n",
    "SECOND_STAGE_PROFILE = str(EXPERIMENT_CONFIG.get(\"second_stage_profile\", \"auto\")).lower()\n",
    "if SECOND_STAGE_PROFILE not in {\"auto\", \"legacy\", \"regularized\", \"custom\"}:\n",
    "    raise ValueError('second_stage_profile must be one of: \"auto\", \"legacy\", \"regularized\", \"custom\".')\n",
    "\n",
    "SECOND_STAGE_USE_BLEND = bool(EXPERIMENT_CONFIG.get(\"second_stage_use_blend\", True))\n",
    "blend_alphas_cfg = EXPERIMENT_CONFIG.get(\"second_stage_blend_alphas\", [1.0, 0.85, 0.70, 0.55])\n",
    "if isinstance(blend_alphas_cfg, (int, float, np.floating, np.integer)):\n",
    "    SECOND_STAGE_BLEND_ALPHAS = [float(blend_alphas_cfg)]\n",
    "elif isinstance(blend_alphas_cfg, (list, tuple)):\n",
    "    SECOND_STAGE_BLEND_ALPHAS = [float(alpha) for alpha in blend_alphas_cfg]\n",
    "else:\n",
    "    raise ValueError(\"second_stage_blend_alphas must be a number or a list of numbers.\")\n",
    "SECOND_STAGE_BLEND_ALPHAS = [a for a in SECOND_STAGE_BLEND_ALPHAS if 0.0 <= a <= 1.0]\n",
    "if not SECOND_STAGE_BLEND_ALPHAS:\n",
    "    SECOND_STAGE_BLEND_ALPHAS = [1.0]\n",
    "if 1.0 not in SECOND_STAGE_BLEND_ALPHAS:\n",
    "    SECOND_STAGE_BLEND_ALPHAS.append(1.0)\n",
    "SECOND_STAGE_BLEND_ALPHAS = sorted(set(SECOND_STAGE_BLEND_ALPHAS), reverse=True)\n",
    "SECOND_STAGE_MIN_GAIN_VS_ENSEMBLE = float(EXPERIMENT_CONFIG.get(\"second_stage_min_gain_vs_ensemble\", 0.0))\n",
    "\n",
    "SECOND_STAGE_MAX_EXTRA_FP_VS_ENSEMBLE = int(\n",
    "    EXPERIMENT_CONFIG.get(\"second_stage_max_extra_fp_vs_ensemble\", 0)\n",
    ")\n",
    "\n",
    "SECOND_STAGE_MIN_VAL_ACCOUNTS_FOR_BOOSTER = int(\n",
    "    EXPERIMENT_CONFIG.get(\"second_stage_min_val_accounts_for_booster\", 25)\n",
    ")\n",
    "\n",
    "SECOND_STAGE_USE_OOF_FEATURES = bool(EXPERIMENT_CONFIG.get(\"second_stage_use_oof_features\", True))\n",
    "SECOND_STAGE_OOF_FOLDS = int(EXPERIMENT_CONFIG.get(\"second_stage_oof_folds\", 4))\n",
    "SECOND_STAGE_OOF_EPOCHS = int(EXPERIMENT_CONFIG.get(\"second_stage_oof_epochs\", max(2, min(EPOCHS, 4))))\n",
    "SECOND_STAGE_OOF_BATCH_SIZE = int(EXPERIMENT_CONFIG.get(\"second_stage_oof_batch_size\", BATCH_SIZE))\n",
    "SECOND_STAGE_OOF_USE_EXTERNAL_PRETRAIN = bool(\n",
    "    EXPERIMENT_CONFIG.get(\"second_stage_oof_use_external_pretrain\", False)\n",
    ")\n",
    "\n",
    "SECOND_STAGE_LEARNING_RATE = float(EXPERIMENT_CONFIG.get(\"second_stage_learning_rate\", 0.05))\n",
    "SECOND_STAGE_MAX_ITER = int(EXPERIMENT_CONFIG.get(\"second_stage_max_iter\", 300))\n",
    "SECOND_STAGE_MAX_DEPTH = int(EXPERIMENT_CONFIG.get(\"second_stage_max_depth\", 4))\n",
    "SECOND_STAGE_L2 = float(EXPERIMENT_CONFIG.get(\"second_stage_l2\", 0.2))\n",
    "SECOND_STAGE_MIN_DATA_IN_LEAF = int(EXPERIMENT_CONFIG.get(\"second_stage_min_data_in_leaf\", 20))\n",
    "SECOND_STAGE_SUBSAMPLE = float(EXPERIMENT_CONFIG.get(\"second_stage_subsample\", 0.8))\n",
    "SECOND_STAGE_RSM = float(EXPERIMENT_CONFIG.get(\"second_stage_rsm\", 0.8))\n",
    "SECOND_STAGE_OD_WAIT = int(EXPERIMENT_CONFIG.get(\"second_stage_od_wait\", 50))\n",
    "SECOND_STAGE_USE_BALANCED_WEIGHTS = bool(EXPERIMENT_CONFIG.get(\"second_stage_use_balanced_weights\", True))\n",
    "\n",
    "oof_seeds_cfg = EXPERIMENT_CONFIG.get(\n",
    "    \"second_stage_oof_seeds\",\n",
    "    ensemble_seeds[: min(3, len(ensemble_seeds))],\n",
    ")\n",
    "if isinstance(oof_seeds_cfg, (int, np.integer)):\n",
    "    second_stage_oof_seeds = [int(oof_seeds_cfg)]\n",
    "elif isinstance(oof_seeds_cfg, (list, tuple)):\n",
    "    second_stage_oof_seeds = [int(seed) for seed in oof_seeds_cfg]\n",
    "else:\n",
    "    raise ValueError(\"second_stage_oof_seeds must be an int or a list of ints.\")\n",
    "second_stage_oof_seeds = list(dict.fromkeys(second_stage_oof_seeds))\n",
    "if SECOND_STAGE_USE_OOF_FEATURES and not second_stage_oof_seeds:\n",
    "    raise ValueError(\"second_stage_oof_seeds cannot be empty when second_stage_use_oof_features=True.\")\n",
    "\n",
    "has_validation = len(val_idx) > 0\n",
    "fit_author_ids_for_score = train_en_model.iloc[fit_idx][\"author_id\"].to_numpy()\n",
    "val_author_ids_for_score = train_en_model.iloc[val_idx][\"author_id\"].to_numpy() if has_validation else np.array([])\n",
    "test_author_ids_for_score = train_en_model.iloc[test_idx][\"author_id\"].to_numpy()\n",
    "\n",
    "def _predict_post_probs(model, indices):\n",
    "    if len(indices) == 0:\n",
    "        return np.array([], dtype=np.float32)\n",
    "    return model.predict(\n",
    "        {\n",
    "            \"input_ids\": input_ids_en[indices],\n",
    "            \"attention_mask\": attention_mask_en[indices],\n",
    "            \"aux_features\": X_aux_en[indices],\n",
    "        },\n",
    "        verbose=0,\n",
    "    ).ravel()\n",
    "\n",
    "def _build_callbacks():\n",
    "    callbacks = []\n",
    "    if has_validation and EARLY_STOPPING_PATIENCE > 0:\n",
    "        callbacks.append(\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_auc\",\n",
    "                mode=\"max\",\n",
    "                patience=EARLY_STOPPING_PATIENCE,\n",
    "                restore_best_weights=True,\n",
    "            )\n",
    "        )\n",
    "    if has_validation and REDUCE_LR_PATIENCE > 0:\n",
    "        callbacks.append(\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor=\"val_auc\",\n",
    "                mode=\"max\",\n",
    "                factor=REDUCE_LR_FACTOR,\n",
    "                patience=REDUCE_LR_PATIENCE,\n",
    "                min_lr=REDUCE_LR_MIN_LR,\n",
    "            )\n",
    "        )\n",
    "    return callbacks\n",
    "\n",
    "def _build_fold_callbacks():\n",
    "    callbacks = []\n",
    "    if EARLY_STOPPING_PATIENCE > 0:\n",
    "        callbacks.append(\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_auc\",\n",
    "                mode=\"max\",\n",
    "                patience=EARLY_STOPPING_PATIENCE,\n",
    "                restore_best_weights=True,\n",
    "            )\n",
    "        )\n",
    "    if REDUCE_LR_PATIENCE > 0:\n",
    "        callbacks.append(\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor=\"val_auc\",\n",
    "                mode=\"max\",\n",
    "                factor=REDUCE_LR_FACTOR,\n",
    "                patience=REDUCE_LR_PATIENCE,\n",
    "                min_lr=REDUCE_LR_MIN_LR,\n",
    "            )\n",
    "        )\n",
    "    return callbacks\n",
    "\n",
    "def _search_best_threshold(author_ids, labels, probs):\n",
    "    if not (has_validation and USE_THRESHOLD_SEARCH):\n",
    "        return float(PREDICTION_THRESHOLD), None, pd.DataFrame()\n",
    "\n",
    "    rows = []\n",
    "    for threshold in np.linspace(THRESHOLD_SEARCH_MIN, THRESHOLD_SEARCH_MAX, THRESHOLD_SEARCH_STEPS):\n",
    "        score, tp, fn, fp, n_accounts = compute_account_score(\n",
    "            author_ids=author_ids,\n",
    "            true_labels=labels,\n",
    "            pred_probs=probs,\n",
    "            threshold=float(threshold),\n",
    "            decision_rule=ACCOUNT_DECISION_RULE,\n",
    "        )\n",
    "        rows.append(\n",
    "            {\n",
    "                \"threshold\": float(threshold),\n",
    "                \"score\": int(score),\n",
    "                \"tp_accounts\": int(tp),\n",
    "                \"fn_accounts\": int(fn),\n",
    "                \"fp_accounts\": int(fp),\n",
    "                \"n_accounts\": int(n_accounts),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    best_row = df.sort_values(\n",
    "        by=[\"score\", \"fp_accounts\", \"tp_accounts\", \"threshold\"],\n",
    "        ascending=[False, True, False, False],\n",
    "    ).iloc[0]\n",
    "    return float(best_row[\"threshold\"]), int(best_row[\"score\"]), df\n",
    "\n",
    "def _aggregate_probabilities(prob_list, mode):\n",
    "    if not prob_list:\n",
    "        return np.array([], dtype=np.float32)\n",
    "    stack = np.vstack(prob_list)\n",
    "    if mode == \"median\":\n",
    "        return np.median(stack, axis=0).astype(np.float32)\n",
    "    return np.mean(stack, axis=0).astype(np.float32)\n",
    "\n",
    "def _prepare_external_pretrain_inputs():\n",
    "    if not USE_EXTERNAL_PRETRAIN:\n",
    "        return None, None, None\n",
    "\n",
    "    required_external = [\n",
    "        \"external_pretrain_input_ids\",\n",
    "        \"external_pretrain_attention_mask\",\n",
    "        \"external_pretrain_meta_scaled\",\n",
    "        \"external_pretrain_labels\",\n",
    "    ]\n",
    "    missing_external = [name for name in required_external if name not in globals()]\n",
    "    if missing_external:\n",
    "        raise ValueError(\n",
    "            f\"External pretrain enabled, but missing variables: {missing_external}. Run the External data pretraining cell.\"\n",
    "        )\n",
    "\n",
    "    external_meta = external_pretrain_meta_scaled\n",
    "    aux_dim = X_aux_en.shape[1]\n",
    "    meta_dim = external_meta.shape[1]\n",
    "    if aux_dim > meta_dim:\n",
    "        padding = np.zeros((external_meta.shape[0], aux_dim - meta_dim), dtype=np.float32)\n",
    "        external_aux = np.concatenate([external_meta, padding], axis=1)\n",
    "    else:\n",
    "        external_aux = external_meta[:, :aux_dim]\n",
    "\n",
    "    external_inputs = {\n",
    "        \"input_ids\": external_pretrain_input_ids,\n",
    "        \"attention_mask\": external_pretrain_attention_mask,\n",
    "        \"aux_features\": external_aux,\n",
    "    }\n",
    "    external_labels = np.asarray(globals()[\"external_pretrain_labels\"], dtype=np.int64)\n",
    "\n",
    "    external_sample_weight = None\n",
    "    if EXTERNAL_PRETRAIN_USE_BALANCED_WEIGHTS:\n",
    "        external_sample_weight = compute_sample_weight(class_weight=\"balanced\", y=external_labels)\n",
    "\n",
    "    return external_inputs, external_labels, external_sample_weight\n",
    "\n",
    "def _build_author_oof_folds(post_indices, n_splits, seed):\n",
    "    fit_posts = (\n",
    "        train_en_model.iloc[post_indices][[\"author_id\", \"is_bot\"]]\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"post_index\"})\n",
    "    )\n",
    "    author_df = fit_posts.groupby(\"author_id\", as_index=False).agg(\n",
    "        account_label=(\"is_bot\", \"max\"),\n",
    "        post_index=(\"post_index\", list),\n",
    "    )\n",
    "\n",
    "    if len(author_df) < 2:\n",
    "        return [np.asarray(post_indices, dtype=np.int64)]\n",
    "\n",
    "    label_counts = author_df[\"account_label\"].value_counts()\n",
    "    min_class_count = int(label_counts.min()) if not label_counts.empty else 0\n",
    "    n_splits_eff = min(int(n_splits), len(author_df), min_class_count)\n",
    "\n",
    "    if n_splits_eff < 2:\n",
    "        return [np.asarray(post_indices, dtype=np.int64)]\n",
    "\n",
    "    splitter = StratifiedKFold(n_splits=n_splits_eff, shuffle=True, random_state=int(seed))\n",
    "\n",
    "    folds = []\n",
    "    author_ids = author_df[\"author_id\"].to_numpy()\n",
    "    author_labels = author_df[\"account_label\"].to_numpy(dtype=np.int64)\n",
    "\n",
    "    for _, hold_author_pos in splitter.split(author_ids, author_labels):\n",
    "        hold_lists = author_df.iloc[hold_author_pos][\"post_index\"].tolist()\n",
    "        hold_posts = np.asarray([idx for lst in hold_lists for idx in lst], dtype=np.int64)\n",
    "        folds.append(hold_posts)\n",
    "\n",
    "    return folds\n",
    "\n",
    "def _compute_oof_post_probs_for_seed(seed, post_indices, use_external_pretrain_for_oof):\n",
    "    post_indices = np.asarray(post_indices, dtype=np.int64)\n",
    "    folds = _build_author_oof_folds(post_indices, SECOND_STAGE_OOF_FOLDS, RANDOM_SEED + int(seed))\n",
    "\n",
    "    if len(folds) == 1 and len(folds[0]) == len(post_indices):\n",
    "        print(\n",
    "            \"[OOF] Not enough account diversity for stratified folds; using in-sample fallback for second-stage training features.\"\n",
    "        )\n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.keras.utils.set_random_seed(int(seed))\n",
    "        np.random.seed(int(seed))\n",
    "\n",
    "        fallback_model = build_multifeature_model(\n",
    "            vocab_size=tokenizer.vocab_size,\n",
    "            seq_len=MAX_LENGTH,\n",
    "            aux_dim=X_aux_en.shape[1],\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            gru_units=GRU_UNITS,\n",
    "            aux_dense_units=AUX_DENSE_UNITS,\n",
    "            head_dense_units=HEAD_DENSE_UNITS,\n",
    "            dropout_text=DROPOUT_TEXT,\n",
    "            dropout_aux=DROPOUT_AUX,\n",
    "            dropout_head=DROPOUT_HEAD,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "        )\n",
    "\n",
    "        fit_inputs_local = {\n",
    "            \"input_ids\": input_ids_en[post_indices],\n",
    "            \"attention_mask\": attention_mask_en[post_indices],\n",
    "            \"aux_features\": X_aux_en[post_indices],\n",
    "        }\n",
    "\n",
    "        fit_kwargs_local = {\n",
    "            \"epochs\": SECOND_STAGE_OOF_EPOCHS,\n",
    "            \"batch_size\": SECOND_STAGE_OOF_BATCH_SIZE,\n",
    "            \"class_weight\": class_weight_dict if USE_CLASS_WEIGHTS else None,\n",
    "            \"verbose\": 0,\n",
    "        }\n",
    "\n",
    "        fallback_model.fit(fit_inputs_local, y_en[post_indices], **fit_kwargs_local)\n",
    "        return _predict_post_probs(fallback_model, post_indices)\n",
    "\n",
    "    index_map = np.full(len(y_en), -1, dtype=np.int64)\n",
    "    index_map[post_indices] = np.arange(len(post_indices), dtype=np.int64)\n",
    "    oof_local = np.full(len(post_indices), np.nan, dtype=np.float32)\n",
    "\n",
    "    for fold_id, hold_posts in enumerate(folds, start=1):\n",
    "        train_posts = post_indices[~np.isin(post_indices, hold_posts)]\n",
    "        if len(train_posts) == 0 or len(hold_posts) == 0:\n",
    "            continue\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.keras.utils.set_random_seed(int(seed) * 100 + fold_id)\n",
    "        np.random.seed(int(seed) * 100 + fold_id)\n",
    "\n",
    "        fold_model = build_multifeature_model(\n",
    "            vocab_size=tokenizer.vocab_size,\n",
    "            seq_len=MAX_LENGTH,\n",
    "            aux_dim=X_aux_en.shape[1],\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            gru_units=GRU_UNITS,\n",
    "            aux_dense_units=AUX_DENSE_UNITS,\n",
    "            head_dense_units=HEAD_DENSE_UNITS,\n",
    "            dropout_text=DROPOUT_TEXT,\n",
    "            dropout_aux=DROPOUT_AUX,\n",
    "            dropout_head=DROPOUT_HEAD,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "        )\n",
    "\n",
    "        if use_external_pretrain_for_oof and external_pretrain_inputs is not None:\n",
    "            pretrain_kwargs = {\n",
    "                \"epochs\": EXTERNAL_PRETRAIN_EPOCHS,\n",
    "                \"batch_size\": EXTERNAL_PRETRAIN_BATCH_SIZE,\n",
    "                \"shuffle\": True,\n",
    "                \"verbose\": 0,\n",
    "            }\n",
    "            if external_pretrain_sample_weight is not None:\n",
    "                pretrain_kwargs[\"sample_weight\"] = external_pretrain_sample_weight\n",
    "            fold_model.fit(external_pretrain_inputs, external_pretrain_labels_arr, **pretrain_kwargs)\n",
    "\n",
    "        train_inputs_fold = {\n",
    "            \"input_ids\": input_ids_en[train_posts],\n",
    "            \"attention_mask\": attention_mask_en[train_posts],\n",
    "            \"aux_features\": X_aux_en[train_posts],\n",
    "        }\n",
    "        hold_inputs_fold = {\n",
    "            \"input_ids\": input_ids_en[hold_posts],\n",
    "            \"attention_mask\": attention_mask_en[hold_posts],\n",
    "            \"aux_features\": X_aux_en[hold_posts],\n",
    "        }\n",
    "\n",
    "        fit_kwargs_fold = {\n",
    "            \"epochs\": SECOND_STAGE_OOF_EPOCHS,\n",
    "            \"batch_size\": SECOND_STAGE_OOF_BATCH_SIZE,\n",
    "            \"class_weight\": class_weight_dict if USE_CLASS_WEIGHTS else None,\n",
    "            \"callbacks\": _build_fold_callbacks(),\n",
    "            \"validation_data\": (hold_inputs_fold, y_en[hold_posts]),\n",
    "            \"verbose\": 0,\n",
    "        }\n",
    "\n",
    "        fold_model.fit(train_inputs_fold, y_en[train_posts], **fit_kwargs_fold)\n",
    "        hold_probs = fold_model.predict(hold_inputs_fold, verbose=0).ravel()\n",
    "\n",
    "        hold_local = index_map[hold_posts]\n",
    "        valid_mask = hold_local >= 0\n",
    "        oof_local[hold_local[valid_mask]] = hold_probs[valid_mask]\n",
    "\n",
    "    missing_mask = np.isnan(oof_local)\n",
    "    if missing_mask.any():\n",
    "        print(f\"[OOF] Filling {int(missing_mask.sum())} missing predictions with in-sample fallback for seed {seed}.\")\n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.keras.utils.set_random_seed(int(seed) + 999)\n",
    "        np.random.seed(int(seed) + 999)\n",
    "\n",
    "        fallback_model = build_multifeature_model(\n",
    "            vocab_size=tokenizer.vocab_size,\n",
    "            seq_len=MAX_LENGTH,\n",
    "            aux_dim=X_aux_en.shape[1],\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            gru_units=GRU_UNITS,\n",
    "            aux_dense_units=AUX_DENSE_UNITS,\n",
    "            head_dense_units=HEAD_DENSE_UNITS,\n",
    "            dropout_text=DROPOUT_TEXT,\n",
    "            dropout_aux=DROPOUT_AUX,\n",
    "            dropout_head=DROPOUT_HEAD,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "        )\n",
    "\n",
    "        fit_inputs_local = {\n",
    "            \"input_ids\": input_ids_en[post_indices],\n",
    "            \"attention_mask\": attention_mask_en[post_indices],\n",
    "            \"aux_features\": X_aux_en[post_indices],\n",
    "        }\n",
    "\n",
    "        fit_kwargs_local = {\n",
    "            \"epochs\": SECOND_STAGE_OOF_EPOCHS,\n",
    "            \"batch_size\": SECOND_STAGE_OOF_BATCH_SIZE,\n",
    "            \"class_weight\": class_weight_dict if USE_CLASS_WEIGHTS else None,\n",
    "            \"verbose\": 0,\n",
    "        }\n",
    "        fallback_model.fit(fit_inputs_local, y_en[post_indices], **fit_kwargs_local)\n",
    "        fallback_probs = fallback_model.predict(fit_inputs_local, verbose=0).ravel()\n",
    "        oof_local[missing_mask] = fallback_probs[missing_mask]\n",
    "\n",
    "    return oof_local.astype(np.float32)\n",
    "\n",
    "fit_inputs = {\n",
    "    \"input_ids\": input_ids_en[fit_idx],\n",
    "    \"attention_mask\": attention_mask_en[fit_idx],\n",
    "    \"aux_features\": X_aux_en[fit_idx],\n",
    "}\n",
    "val_inputs = (\n",
    "    {\n",
    "        \"input_ids\": input_ids_en[val_idx],\n",
    "        \"attention_mask\": attention_mask_en[val_idx],\n",
    "        \"aux_features\": X_aux_en[val_idx],\n",
    "    }\n",
    "    if has_validation\n",
    "    else None\n",
    ")\n",
    "\n",
    "post_prob_fit_list = []\n",
    "post_prob_val_list = []\n",
    "post_prob_test_list = []\n",
    "seed_rows = []\n",
    "\n",
    "if hasattr(tf.config.experimental, \"enable_op_determinism\"):\n",
    "    try:\n",
    "        tf.config.experimental.enable_op_determinism()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "external_pretrain_inputs = None\n",
    "external_pretrain_labels_arr = None\n",
    "external_pretrain_sample_weight = None\n",
    "if USE_EXTERNAL_PRETRAIN:\n",
    "    external_pretrain_inputs, external_pretrain_labels_arr, external_pretrain_sample_weight = _prepare_external_pretrain_inputs()\n",
    "\n",
    "for seed in ensemble_seeds:\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.keras.utils.set_random_seed(int(seed))\n",
    "    np.random.seed(int(seed))\n",
    "\n",
    "    model_seed = build_multifeature_model(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        seq_len=MAX_LENGTH,\n",
    "        aux_dim=X_aux_en.shape[1],\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        gru_units=GRU_UNITS,\n",
    "        aux_dense_units=AUX_DENSE_UNITS,\n",
    "        head_dense_units=HEAD_DENSE_UNITS,\n",
    "        dropout_text=DROPOUT_TEXT,\n",
    "        dropout_aux=DROPOUT_AUX,\n",
    "        dropout_head=DROPOUT_HEAD,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "    )\n",
    "\n",
    "    if USE_EXTERNAL_PRETRAIN:\n",
    "        pretrain_kwargs = {\n",
    "            \"epochs\": EXTERNAL_PRETRAIN_EPOCHS,\n",
    "            \"batch_size\": EXTERNAL_PRETRAIN_BATCH_SIZE,\n",
    "            \"shuffle\": True,\n",
    "            \"verbose\": 0,\n",
    "        }\n",
    "        if external_pretrain_sample_weight is not None:\n",
    "            pretrain_kwargs[\"sample_weight\"] = external_pretrain_sample_weight\n",
    "        model_seed.fit(external_pretrain_inputs, external_pretrain_labels_arr, **pretrain_kwargs)\n",
    "\n",
    "    fit_kwargs = {\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"class_weight\": class_weight_dict if USE_CLASS_WEIGHTS else None,\n",
    "        \"callbacks\": _build_callbacks(),\n",
    "        \"verbose\": 0,\n",
    "    }\n",
    "    if has_validation:\n",
    "        fit_kwargs[\"validation_data\"] = (val_inputs, y_en[val_idx])\n",
    "\n",
    "    model_seed.fit(fit_inputs, y_en[fit_idx], **fit_kwargs)\n",
    "\n",
    "    fit_probs = _predict_post_probs(model_seed, fit_idx)\n",
    "    val_probs = _predict_post_probs(model_seed, val_idx)\n",
    "    test_probs = _predict_post_probs(model_seed, test_idx)\n",
    "\n",
    "    threshold_seed, val_score_seed, _ = _search_best_threshold(val_author_ids_for_score, y_en[val_idx], val_probs)\n",
    "\n",
    "    test_score_seed, tp_seed, fn_seed, fp_seed, n_accounts_seed = compute_account_score(\n",
    "        author_ids=test_author_ids_for_score,\n",
    "        true_labels=y_en[test_idx],\n",
    "        pred_probs=test_probs,\n",
    "        threshold=threshold_seed,\n",
    "        decision_rule=ACCOUNT_DECISION_RULE,\n",
    "    )\n",
    "\n",
    "    seed_rows.append(\n",
    "        {\n",
    "            \"seed\": int(seed),\n",
    "            \"threshold\": float(threshold_seed),\n",
    "            \"val_best_score\": val_score_seed,\n",
    "            \"test_score\": int(test_score_seed),\n",
    "            \"tp_accounts\": int(tp_seed),\n",
    "            \"fn_accounts\": int(fn_seed),\n",
    "            \"fp_accounts\": int(fp_seed),\n",
    "            \"n_accounts\": int(n_accounts_seed),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    post_prob_fit_list.append(fit_probs)\n",
    "    post_prob_val_list.append(val_probs)\n",
    "    post_prob_test_list.append(test_probs)\n",
    "\n",
    "seed_report_df = pd.DataFrame(seed_rows)\n",
    "ensemble_seed_report_en = seed_report_df.copy()\n",
    "\n",
    "post_prob_fit_ensemble_en = _aggregate_probabilities(post_prob_fit_list, ENSEMBLE_AGGREGATION)\n",
    "post_prob_val_ensemble_en = _aggregate_probabilities(post_prob_val_list, ENSEMBLE_AGGREGATION)\n",
    "post_prob_test_ensemble_en = _aggregate_probabilities(post_prob_test_list, ENSEMBLE_AGGREGATION)\n",
    "\n",
    "selected_threshold_ensemble, best_ensemble_val_score, threshold_search_results_ensemble_en = _search_best_threshold(\n",
    "    val_author_ids_for_score,\n",
    "    y_en[val_idx],\n",
    "    post_prob_val_ensemble_en,\n",
    ")\n",
    "\n",
    "(\n",
    "    test_score_ensemble_en,\n",
    "    ensemble_tp_accounts_en,\n",
    "    ensemble_fn_accounts_en,\n",
    "    ensemble_fp_accounts_en,\n",
    "    ensemble_n_accounts_en,\n",
    ") = compute_account_score(\n",
    "    author_ids=test_author_ids_for_score,\n",
    "    true_labels=y_en[test_idx],\n",
    "    pred_probs=post_prob_test_ensemble_en,\n",
    "    threshold=selected_threshold_ensemble,\n",
    "    decision_rule=ACCOUNT_DECISION_RULE,\n",
    ")\n",
    "\n",
    "# Make the ensemble outputs the default baseline for downstream score/plot cells.\n",
    "post_prob_fit = post_prob_fit_ensemble_en\n",
    "post_prob_val = post_prob_val_ensemble_en\n",
    "post_prob_test = post_prob_test_ensemble_en\n",
    "y_prob = post_prob_test_ensemble_en\n",
    "SELECTED_THRESHOLD = float(selected_threshold_ensemble)\n",
    "test_score = int(test_score_ensemble_en)\n",
    "test_tp_accounts = int(ensemble_tp_accounts_en)\n",
    "test_fn_accounts = int(ensemble_fn_accounts_en)\n",
    "test_fp_accounts = int(ensemble_fp_accounts_en)\n",
    "test_n_accounts = int(ensemble_n_accounts_en)\n",
    "\n",
    "seed_mean_score_en = float(seed_report_df[\"test_score\"].mean())\n",
    "seed_std_score_en = float(seed_report_df[\"test_score\"].std(ddof=1)) if len(seed_report_df) > 1 else 0.0\n",
    "\n",
    "def build_account_feature_table(post_indices, post_probs, bot_threshold):\n",
    "    posts = train_en_model.iloc[post_indices].copy()\n",
    "    posts[\"post_prob\"] = np.asarray(post_probs, dtype=np.float32)\n",
    "\n",
    "    posts[\"has_url_post\"] = (posts[\"url_count\"] > 0).astype(np.float32)\n",
    "    posts[\"has_mention_post\"] = (posts[\"mention_count\"] > 0).astype(np.float32)\n",
    "    posts[\"has_hashtag_post\"] = (posts[\"hashtag_count\"] > 0).astype(np.float32)\n",
    "    posts[\"pred_bot_post\"] = (posts[\"post_prob\"] >= bot_threshold).astype(np.float32)\n",
    "\n",
    "    agg_spec = {\n",
    "        \"is_bot\": [\"max\"],\n",
    "        \"text_clean\": [\"size\"],\n",
    "        \"post_prob\": [\"mean\", \"std\", \"max\", \"min\"],\n",
    "        \"char_count\": [\"mean\", \"std\", \"max\"],\n",
    "        \"word_count\": [\"mean\", \"std\", \"max\"],\n",
    "        \"url_count\": [\"mean\", \"max\"],\n",
    "        \"mention_count\": [\"mean\", \"max\"],\n",
    "        \"hashtag_count\": [\"mean\", \"max\"],\n",
    "        \"exclamation_count\": [\"mean\", \"max\"],\n",
    "        \"question_count\": [\"mean\", \"max\"],\n",
    "        \"has_url_post\": [\"mean\"],\n",
    "        \"has_mention_post\": [\"mean\"],\n",
    "        \"has_hashtag_post\": [\"mean\"],\n",
    "        \"pred_bot_post\": [\"mean\"],\n",
    "    }\n",
    "\n",
    "    for topic_col in topic_feature_cols_en:\n",
    "        agg_spec[topic_col] = [\"mean\"]\n",
    "\n",
    "    account = posts.groupby(\"author_id\", as_index=False).agg(agg_spec)\n",
    "    account.columns = [\n",
    "        \"author_id\" if col == (\"author_id\", \"\") else f\"{col[0]}_{col[1]}\"\n",
    "        for col in account.columns.to_flat_index()\n",
    "    ]\n",
    "\n",
    "    account = account.rename(columns={\"is_bot_max\": \"true_is_bot\", \"text_clean_size\": \"n_posts\"})\n",
    "    for col in [\"post_prob_std\", \"char_count_std\", \"word_count_std\"]:\n",
    "        if col in account.columns:\n",
    "            account[col] = account[col].fillna(0.0)\n",
    "    account[\"n_posts_log1p\"] = np.log1p(account[\"n_posts\"].astype(np.float32))\n",
    "\n",
    "    user_source = (\n",
    "        users_en_labeled.copy()\n",
    "        if \"users_en_labeled\" in globals()\n",
    "        else users_en.drop_duplicates(subset=\"id\", keep=\"last\").copy()\n",
    "    )\n",
    "    user_source[\"username_len\"] = user_source[\"username\"].fillna(\"\").str.len()\n",
    "    user_source[\"name_len\"] = user_source[\"name\"].fillna(\"\").str.len()\n",
    "    user_source[\"description_len\"] = user_source[\"description\"].fillna(\"\").str.len()\n",
    "    user_source[\"has_location\"] = user_source[\"location\"].fillna(\"\").str.strip().ne(\"\").astype(np.float32)\n",
    "\n",
    "    user_features = user_source[\n",
    "        [\n",
    "            \"id\",\n",
    "            \"tweet_count\",\n",
    "            \"z_score\",\n",
    "            \"username_len\",\n",
    "            \"name_len\",\n",
    "            \"description_len\",\n",
    "            \"has_location\",\n",
    "        ]\n",
    "    ].copy()\n",
    "\n",
    "    account = account.merge(user_features, left_on=\"author_id\", right_on=\"id\", how=\"left\").drop(columns=[\"id\"])\n",
    "\n",
    "    numeric_cols = [col for col in account.columns if col != \"author_id\"]\n",
    "    account[numeric_cols] = account[numeric_cols].apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
    "    return account\n",
    "\n",
    "def score_from_account_probs(true_labels, pred_probs, threshold):\n",
    "    pred_labels = (pred_probs >= threshold).astype(np.int64)\n",
    "    tp = int(((true_labels == 1) & (pred_labels == 1)).sum())\n",
    "    fn = int(((true_labels == 1) & (pred_labels == 0)).sum())\n",
    "    fp = int(((true_labels == 0) & (pred_labels == 1)).sum())\n",
    "    score = (4 * tp) - (1 * fn) - (2 * fp)\n",
    "    return score, tp, fn, fp, pred_labels\n",
    "\n",
    "def _search_threshold_for_account_probs(y_true, y_prob):\n",
    "    if len(y_true) == 0:\n",
    "        return float(PREDICTION_THRESHOLD), None, 0, 0, 0, pd.DataFrame()\n",
    "\n",
    "    if not USE_THRESHOLD_SEARCH:\n",
    "        score, tp, fn, fp, _ = score_from_account_probs(y_true, y_prob, float(PREDICTION_THRESHOLD))\n",
    "        return float(PREDICTION_THRESHOLD), int(score), int(tp), int(fn), int(fp), pd.DataFrame()\n",
    "\n",
    "    rows = []\n",
    "    for threshold in np.linspace(THRESHOLD_SEARCH_MIN, THRESHOLD_SEARCH_MAX, THRESHOLD_SEARCH_STEPS):\n",
    "        score, tp, fn, fp, _ = score_from_account_probs(y_true, y_prob, float(threshold))\n",
    "        rows.append(\n",
    "            {\n",
    "                \"threshold\": float(threshold),\n",
    "                \"score\": int(score),\n",
    "                \"tp_accounts\": int(tp),\n",
    "                \"fn_accounts\": int(fn),\n",
    "                \"fp_accounts\": int(fp),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    best_row = df.sort_values(\n",
    "        by=[\"score\", \"fp_accounts\", \"tp_accounts\", \"threshold\"],\n",
    "        ascending=[False, True, False, False],\n",
    "    ).iloc[0]\n",
    "\n",
    "    return (\n",
    "        float(best_row[\"threshold\"]),\n",
    "        int(best_row[\"score\"]),\n",
    "        int(best_row[\"tp_accounts\"]),\n",
    "        int(best_row[\"fn_accounts\"]),\n",
    "        int(best_row[\"fp_accounts\"]),\n",
    "        df,\n",
    "    )\n",
    "\n",
    "def _make_second_stage_candidates():\n",
    "    candidates = {}\n",
    "\n",
    "    legacy_params = {\n",
    "        \"iterations\": 300,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"depth\": 4,\n",
    "        \"l2_leaf_reg\": 0.2,\n",
    "        \"loss_function\": \"Logloss\",\n",
    "        \"eval_metric\": \"AUC\",\n",
    "        \"random_seed\": RANDOM_SEED,\n",
    "        \"verbose\": False,\n",
    "    }\n",
    "\n",
    "    regularized_params = {\n",
    "        \"iterations\": SECOND_STAGE_MAX_ITER,\n",
    "        \"learning_rate\": SECOND_STAGE_LEARNING_RATE,\n",
    "        \"depth\": SECOND_STAGE_MAX_DEPTH,\n",
    "        \"l2_leaf_reg\": SECOND_STAGE_L2,\n",
    "        \"min_data_in_leaf\": SECOND_STAGE_MIN_DATA_IN_LEAF,\n",
    "        \"loss_function\": \"Logloss\",\n",
    "        \"eval_metric\": \"AUC\",\n",
    "        \"random_seed\": RANDOM_SEED,\n",
    "        \"verbose\": False,\n",
    "        \"od_type\": \"Iter\",\n",
    "        \"od_wait\": SECOND_STAGE_OD_WAIT,\n",
    "    }\n",
    "    if 0.0 < SECOND_STAGE_SUBSAMPLE < 1.0:\n",
    "        regularized_params[\"subsample\"] = SECOND_STAGE_SUBSAMPLE\n",
    "        regularized_params[\"bootstrap_type\"] = \"Bernoulli\"\n",
    "    if 0.0 < SECOND_STAGE_RSM <= 1.0:\n",
    "        regularized_params[\"rsm\"] = SECOND_STAGE_RSM\n",
    "\n",
    "    custom_params = {\n",
    "        \"iterations\": SECOND_STAGE_MAX_ITER,\n",
    "        \"learning_rate\": SECOND_STAGE_LEARNING_RATE,\n",
    "        \"depth\": SECOND_STAGE_MAX_DEPTH,\n",
    "        \"l2_leaf_reg\": SECOND_STAGE_L2,\n",
    "        \"min_data_in_leaf\": SECOND_STAGE_MIN_DATA_IN_LEAF,\n",
    "        \"loss_function\": \"Logloss\",\n",
    "        \"eval_metric\": \"AUC\",\n",
    "        \"random_seed\": RANDOM_SEED,\n",
    "        \"verbose\": False,\n",
    "        \"od_type\": \"Iter\",\n",
    "        \"od_wait\": SECOND_STAGE_OD_WAIT,\n",
    "    }\n",
    "    if 0.0 < SECOND_STAGE_SUBSAMPLE < 1.0:\n",
    "        custom_params[\"subsample\"] = SECOND_STAGE_SUBSAMPLE\n",
    "        custom_params[\"bootstrap_type\"] = \"Bernoulli\"\n",
    "    if 0.0 < SECOND_STAGE_RSM <= 1.0:\n",
    "        custom_params[\"rsm\"] = SECOND_STAGE_RSM\n",
    "\n",
    "    if SECOND_STAGE_PROFILE in {\"auto\", \"legacy\"}:\n",
    "        candidates[\"legacy\"] = legacy_params\n",
    "    if SECOND_STAGE_PROFILE in {\"auto\", \"regularized\"}:\n",
    "        candidates[\"regularized\"] = regularized_params\n",
    "    if SECOND_STAGE_PROFILE == \"custom\":\n",
    "        candidates[\"custom\"] = custom_params\n",
    "\n",
    "    if SECOND_STAGE_USE_BALANCED_WEIGHTS:\n",
    "        for params in candidates.values():\n",
    "            params[\"auto_class_weights\"] = \"Balanced\"\n",
    "\n",
    "    return candidates\n",
    "\n",
    "if USE_SECOND_STAGE_ACCOUNT_MODEL:\n",
    "    post_prob_fit_for_second_stage_en = post_prob_fit_ensemble_en.copy()\n",
    "    second_stage_fit_feature_source_en = \"in_sample_ensemble\"\n",
    "    second_stage_oof_report_en = pd.DataFrame()\n",
    "\n",
    "    if SECOND_STAGE_USE_OOF_FEATURES:\n",
    "        print(\n",
    "            f\"Building OOF first-stage features for second-stage training (seeds={second_stage_oof_seeds}, folds={SECOND_STAGE_OOF_FOLDS}, epochs={SECOND_STAGE_OOF_EPOCHS})...\"\n",
    "        )\n",
    "        oof_fit_prob_list = []\n",
    "        oof_rows = []\n",
    "\n",
    "        for oof_seed in second_stage_oof_seeds:\n",
    "            oof_probs_seed = _compute_oof_post_probs_for_seed(\n",
    "                seed=int(oof_seed),\n",
    "                post_indices=fit_idx,\n",
    "                use_external_pretrain_for_oof=bool(USE_EXTERNAL_PRETRAIN and SECOND_STAGE_OOF_USE_EXTERNAL_PRETRAIN),\n",
    "            )\n",
    "\n",
    "            oof_score_seed, oof_tp, oof_fn, oof_fp, oof_accounts = compute_account_score(\n",
    "                author_ids=fit_author_ids_for_score,\n",
    "                true_labels=y_en[fit_idx],\n",
    "                pred_probs=oof_probs_seed,\n",
    "                threshold=selected_threshold_ensemble,\n",
    "                decision_rule=ACCOUNT_DECISION_RULE,\n",
    "            )\n",
    "\n",
    "            oof_rows.append(\n",
    "                {\n",
    "                    \"seed\": int(oof_seed),\n",
    "                    \"fit_account_score_at_ensemble_threshold\": int(oof_score_seed),\n",
    "                    \"tp_accounts\": int(oof_tp),\n",
    "                    \"fn_accounts\": int(oof_fn),\n",
    "                    \"fp_accounts\": int(oof_fp),\n",
    "                    \"n_accounts\": int(oof_accounts),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            oof_fit_prob_list.append(oof_probs_seed)\n",
    "\n",
    "        post_prob_fit_for_second_stage_en = _aggregate_probabilities(oof_fit_prob_list, ENSEMBLE_AGGREGATION)\n",
    "        second_stage_fit_feature_source_en = \"oof_first_stage\"\n",
    "        second_stage_oof_report_en = pd.DataFrame(oof_rows)\n",
    "\n",
    "    fit_account_df = build_account_feature_table(fit_idx, post_prob_fit_for_second_stage_en, selected_threshold_ensemble)\n",
    "    val_account_df = (\n",
    "        build_account_feature_table(val_idx, post_prob_val_ensemble_en, selected_threshold_ensemble)\n",
    "        if has_validation\n",
    "        else pd.DataFrame()\n",
    "    )\n",
    "    test_account_df = build_account_feature_table(test_idx, post_prob_test_ensemble_en, selected_threshold_ensemble)\n",
    "\n",
    "    target_col = \"true_is_bot\"\n",
    "    feature_cols_account = [col for col in fit_account_df.columns if col not in {\"author_id\", target_col}]\n",
    "\n",
    "    for df in [fit_account_df, val_account_df, test_account_df]:\n",
    "        if df.empty:\n",
    "            continue\n",
    "        missing_cols = [col for col in feature_cols_account if col not in df.columns]\n",
    "        for col in missing_cols:\n",
    "            df[col] = 0.0\n",
    "\n",
    "    X_fit_acc = fit_account_df[feature_cols_account].to_numpy(dtype=np.float32)\n",
    "    y_fit_acc = fit_account_df[target_col].to_numpy(dtype=np.int64)\n",
    "    X_test_acc = test_account_df[feature_cols_account].to_numpy(dtype=np.float32)\n",
    "    y_test_acc = test_account_df[target_col].to_numpy(dtype=np.int64)\n",
    "\n",
    "    if len(val_account_df):\n",
    "        X_val_acc = val_account_df[feature_cols_account].to_numpy(dtype=np.float32)\n",
    "        y_val_acc = val_account_df[target_col].to_numpy(dtype=np.int64)\n",
    "    else:\n",
    "        X_val_acc = np.zeros((0, len(feature_cols_account)), dtype=np.float32)\n",
    "        y_val_acc = np.zeros((0,), dtype=np.int64)\n",
    "\n",
    "    baseline_account_val_score = None\n",
    "    baseline_account_val_threshold = float(selected_threshold_ensemble)\n",
    "    baseline_account_val_tp = 0\n",
    "    baseline_account_val_fn = 0\n",
    "    baseline_account_val_fp = 0\n",
    "\n",
    "    base_val_prob_acc = val_account_df[\"post_prob_mean\"].to_numpy(dtype=np.float32) if len(y_val_acc) else np.array([])\n",
    "    base_test_prob_acc = test_account_df[\"post_prob_mean\"].to_numpy(dtype=np.float32)\n",
    "\n",
    "    if len(y_val_acc):\n",
    "        (\n",
    "            baseline_account_val_threshold,\n",
    "            baseline_account_val_score,\n",
    "            baseline_account_val_tp,\n",
    "            baseline_account_val_fn,\n",
    "            baseline_account_val_fp,\n",
    "            baseline_account_threshold_search_en,\n",
    "        ) = _search_threshold_for_account_probs(y_val_acc, base_val_prob_acc)\n",
    "    else:\n",
    "        baseline_account_threshold_search_en = pd.DataFrame()\n",
    "\n",
    "    candidate_params = _make_second_stage_candidates()\n",
    "    candidate_rows = []\n",
    "    candidate_artifacts = []\n",
    "\n",
    "    for profile_name, params in candidate_params.items():\n",
    "        model = CatBoostClassifier(**params)\n",
    "\n",
    "        fit_kwargs = {}\n",
    "        if len(y_val_acc):\n",
    "            fit_kwargs[\"eval_set\"] = (X_val_acc, y_val_acc)\n",
    "            fit_kwargs[\"use_best_model\"] = True\n",
    "\n",
    "        model.fit(X_fit_acc, y_fit_acc, **fit_kwargs)\n",
    "\n",
    "        test_raw_prob = model.predict_proba(X_test_acc)[:, 1]\n",
    "        val_raw_prob = model.predict_proba(X_val_acc)[:, 1] if len(y_val_acc) else np.array([])\n",
    "\n",
    "        if len(y_val_acc):\n",
    "            best_candidate = None\n",
    "            for alpha in (SECOND_STAGE_BLEND_ALPHAS if SECOND_STAGE_USE_BLEND else [1.0]):\n",
    "                val_candidate_prob = (alpha * val_raw_prob) + ((1.0 - alpha) * base_val_prob_acc)\n",
    "                (\n",
    "                    cand_threshold,\n",
    "                    cand_val_score,\n",
    "                    cand_val_tp,\n",
    "                    cand_val_fn,\n",
    "                    cand_val_fp,\n",
    "                    cand_search_df,\n",
    "                ) = _search_threshold_for_account_probs(y_val_acc, val_candidate_prob)\n",
    "\n",
    "                if (\n",
    "                    best_candidate is None\n",
    "                    or cand_val_score > best_candidate[\"val_score\"]\n",
    "                    or (\n",
    "                        cand_val_score == best_candidate[\"val_score\"]\n",
    "                        and cand_val_fp < best_candidate[\"val_fp_accounts\"]\n",
    "                    )\n",
    "                    or (\n",
    "                        cand_val_score == best_candidate[\"val_score\"]\n",
    "                        and cand_val_fp == best_candidate[\"val_fp_accounts\"]\n",
    "                        and cand_val_tp > best_candidate[\"val_tp_accounts\"]\n",
    "                    )\n",
    "                ):\n",
    "                    best_candidate = {\n",
    "                        \"alpha\": float(alpha),\n",
    "                        \"threshold\": float(cand_threshold),\n",
    "                        \"val_score\": int(cand_val_score),\n",
    "                        \"val_tp_accounts\": int(cand_val_tp),\n",
    "                        \"val_fn_accounts\": int(cand_val_fn),\n",
    "                        \"val_fp_accounts\": int(cand_val_fp),\n",
    "                        \"search_df\": cand_search_df,\n",
    "                    }\n",
    "\n",
    "            chosen_alpha = best_candidate[\"alpha\"]\n",
    "            chosen_threshold = best_candidate[\"threshold\"]\n",
    "            chosen_val_score = best_candidate[\"val_score\"]\n",
    "            chosen_val_tp = best_candidate[\"val_tp_accounts\"]\n",
    "            chosen_val_fn = best_candidate[\"val_fn_accounts\"]\n",
    "            chosen_val_fp = best_candidate[\"val_fp_accounts\"]\n",
    "            threshold_df = best_candidate[\"search_df\"]\n",
    "        else:\n",
    "            chosen_alpha = 1.0\n",
    "            chosen_threshold = float(selected_threshold_ensemble)\n",
    "            chosen_val_score = np.nan\n",
    "            chosen_val_tp = np.nan\n",
    "            chosen_val_fn = np.nan\n",
    "            chosen_val_fp = np.nan\n",
    "            threshold_df = pd.DataFrame()\n",
    "\n",
    "        test_blend_prob = (chosen_alpha * test_raw_prob) + ((1.0 - chosen_alpha) * base_test_prob_acc)\n",
    "        (\n",
    "            test_score_profile,\n",
    "            test_tp_profile,\n",
    "            test_fn_profile,\n",
    "            test_fp_profile,\n",
    "            test_pred_profile,\n",
    "        ) = score_from_account_probs(y_test_acc, test_blend_prob, chosen_threshold)\n",
    "\n",
    "        candidate_rows.append(\n",
    "            {\n",
    "                \"profile\": profile_name,\n",
    "                \"alpha\": float(chosen_alpha),\n",
    "                \"threshold\": float(chosen_threshold),\n",
    "                \"val_score\": None if pd.isna(chosen_val_score) else int(chosen_val_score),\n",
    "                \"val_tp_accounts\": None if pd.isna(chosen_val_tp) else int(chosen_val_tp),\n",
    "                \"val_fn_accounts\": None if pd.isna(chosen_val_fn) else int(chosen_val_fn),\n",
    "                \"val_fp_accounts\": None if pd.isna(chosen_val_fp) else int(chosen_val_fp),\n",
    "                \"test_score\": int(test_score_profile),\n",
    "                \"test_tp_accounts\": int(test_tp_profile),\n",
    "                \"test_fn_accounts\": int(test_fn_profile),\n",
    "                \"test_fp_accounts\": int(test_fp_profile),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        candidate_artifacts.append(\n",
    "            {\n",
    "                \"profile\": profile_name,\n",
    "                \"model\": model,\n",
    "                \"alpha\": float(chosen_alpha),\n",
    "                \"threshold\": float(chosen_threshold),\n",
    "                \"val_score\": chosen_val_score,\n",
    "                \"val_tp_accounts\": chosen_val_tp,\n",
    "                \"val_fn_accounts\": chosen_val_fn,\n",
    "                \"val_fp_accounts\": chosen_val_fp,\n",
    "                \"threshold_search_df\": threshold_df,\n",
    "                \"test_prob\": test_blend_prob,\n",
    "                \"test_pred\": test_pred_profile,\n",
    "                \"test_score\": int(test_score_profile),\n",
    "                \"test_tp_accounts\": int(test_tp_profile),\n",
    "                \"test_fn_accounts\": int(test_fn_profile),\n",
    "                \"test_fp_accounts\": int(test_fp_profile),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    second_stage_candidate_report_en = pd.DataFrame(candidate_rows)\n",
    "\n",
    "    if second_stage_candidate_report_en.empty:\n",
    "        raise ValueError(\"No second-stage candidate was trained. Check second_stage_profile.\")\n",
    "\n",
    "    if len(y_val_acc):\n",
    "        sorted_candidates = second_stage_candidate_report_en.sort_values(\n",
    "            by=[\"val_score\", \"val_fp_accounts\", \"val_tp_accounts\", \"threshold\", \"alpha\"],\n",
    "            ascending=[False, True, False, False, False],\n",
    "            na_position=\"last\",\n",
    "        )\n",
    "    else:\n",
    "        sorted_candidates = second_stage_candidate_report_en.sort_values(\n",
    "            by=[\"test_score\", \"test_fp_accounts\", \"test_tp_accounts\"],\n",
    "            ascending=[False, True, False],\n",
    "        )\n",
    "\n",
    "    best_profile_name = str(sorted_candidates.iloc[0][\"profile\"])\n",
    "    selected_artifact = next(item for item in candidate_artifacts if item[\"profile\"] == best_profile_name)\n",
    "\n",
    "    second_stage_selected_profile_en = best_profile_name\n",
    "\n",
    "    second_stage_used_fallback_en = False\n",
    "\n",
    "    second_stage_fallback_reason_en = \"\"\n",
    "\n",
    "\n",
    "\n",
    "    if len(y_val_acc):\n",
    "\n",
    "        best_val_score = int(selected_artifact[\"val_score\"])\n",
    "\n",
    "        best_val_fp = int(selected_artifact[\"val_fp_accounts\"])\n",
    "\n",
    "        baseline_ref_score = int(baseline_account_val_score) if baseline_account_val_score is not None else int(best_ensemble_val_score or 0)\n",
    "\n",
    "        baseline_fp_ref = int(baseline_account_val_fp)\n",
    "\n",
    "        val_accounts_count = int(len(y_val_acc))\n",
    "\n",
    "        fallback_reasons = []\n",
    "\n",
    "        if val_accounts_count < SECOND_STAGE_MIN_VAL_ACCOUNTS_FOR_BOOSTER:\n",
    "\n",
    "            fallback_reasons.append(\n",
    "\n",
    "                f\"validation account count too small ({val_accounts_count} < {SECOND_STAGE_MIN_VAL_ACCOUNTS_FOR_BOOSTER})\"\n",
    "\n",
    "            )\n",
    "\n",
    "        if best_val_score < (baseline_ref_score + SECOND_STAGE_MIN_GAIN_VS_ENSEMBLE):\n",
    "\n",
    "            fallback_reasons.append(\n",
    "\n",
    "                f\"validation score gain too small ({best_val_score} < {baseline_ref_score + SECOND_STAGE_MIN_GAIN_VS_ENSEMBLE})\"\n",
    "\n",
    "            )\n",
    "\n",
    "        if best_val_fp > (baseline_fp_ref + SECOND_STAGE_MAX_EXTRA_FP_VS_ENSEMBLE):\n",
    "\n",
    "            fallback_reasons.append(\n",
    "\n",
    "                f\"too many extra validation false positives ({best_val_fp} > {baseline_fp_ref + SECOND_STAGE_MAX_EXTRA_FP_VS_ENSEMBLE})\"\n",
    "\n",
    "            )\n",
    "\n",
    "        if fallback_reasons:\n",
    "\n",
    "            second_stage_used_fallback_en = True\n",
    "\n",
    "            second_stage_selected_profile_en = \"fallback_ensemble\"\n",
    "\n",
    "            second_stage_fallback_reason_en = \"; \".join(fallback_reasons)\n",
    "\n",
    "\n",
    "\n",
    "    if second_stage_used_fallback_en:\n",
    "        second_stage_model_en = None\n",
    "        second_stage_selected_threshold_en = float(selected_threshold_ensemble)\n",
    "        second_stage_alpha_en = 0.0\n",
    "        second_stage_test_score_en = int(test_score_ensemble_en)\n",
    "        second_stage_tp_accounts_en = int(ensemble_tp_accounts_en)\n",
    "        second_stage_fn_accounts_en = int(ensemble_fn_accounts_en)\n",
    "        second_stage_fp_accounts_en = int(ensemble_fp_accounts_en)\n",
    "\n",
    "        baseline_account_eval = pd.DataFrame(\n",
    "            {\n",
    "                \"author_id\": test_author_ids_for_score,\n",
    "                \"true_is_bot\": y_en[test_idx].astype(np.int64),\n",
    "                \"post_prob\": post_prob_test_ensemble_en.astype(np.float32),\n",
    "            }\n",
    "        )\n",
    "        baseline_account_eval[\"pred_post\"] = (baseline_account_eval[\"post_prob\"] >= selected_threshold_ensemble).astype(np.int64)\n",
    "        baseline_account_eval = (\n",
    "            baseline_account_eval.groupby(\"author_id\", as_index=False)\n",
    "            .agg(\n",
    "                true_is_bot=(\"true_is_bot\", \"max\"),\n",
    "                mean_prob=(\"post_prob\", \"mean\"),\n",
    "                any_pred=(\"pred_post\", \"max\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if ACCOUNT_DECISION_RULE == \"any\":\n",
    "            baseline_account_eval[\"pred_is_bot\"] = baseline_account_eval[\"any_pred\"].astype(np.int64)\n",
    "            baseline_account_eval[\"pred_prob\"] = baseline_account_eval[\"any_pred\"].astype(np.float32)\n",
    "        else:\n",
    "            baseline_account_eval[\"pred_is_bot\"] = (\n",
    "                baseline_account_eval[\"mean_prob\"] >= selected_threshold_ensemble\n",
    "            ).astype(np.int64)\n",
    "            baseline_account_eval[\"pred_prob\"] = baseline_account_eval[\"mean_prob\"].astype(np.float32)\n",
    "\n",
    "        second_stage_test_prob_en = baseline_account_eval[\"pred_prob\"].to_numpy(dtype=np.float32)\n",
    "        second_stage_test_pred_en = baseline_account_eval[\"pred_is_bot\"].to_numpy(dtype=np.int64)\n",
    "\n",
    "        second_stage_account_predictions_en = baseline_account_eval[\n",
    "            [\"author_id\", \"true_is_bot\", \"pred_prob\", \"pred_is_bot\"]\n",
    "        ].copy()\n",
    "\n",
    "        second_stage_threshold_search_results_en = pd.DataFrame()\n",
    "    else:\n",
    "        second_stage_model_en = selected_artifact[\"model\"]\n",
    "        second_stage_selected_threshold_en = float(selected_artifact[\"threshold\"])\n",
    "        second_stage_alpha_en = float(selected_artifact[\"alpha\"])\n",
    "        second_stage_threshold_search_results_en = selected_artifact[\"threshold_search_df\"]\n",
    "        second_stage_test_prob_en = np.asarray(selected_artifact[\"test_prob\"], dtype=np.float32)\n",
    "        second_stage_test_pred_en = np.asarray(selected_artifact[\"test_pred\"], dtype=np.int64)\n",
    "        second_stage_test_score_en = int(selected_artifact[\"test_score\"])\n",
    "        second_stage_tp_accounts_en = int(selected_artifact[\"test_tp_accounts\"])\n",
    "        second_stage_fn_accounts_en = int(selected_artifact[\"test_fn_accounts\"])\n",
    "        second_stage_fp_accounts_en = int(selected_artifact[\"test_fp_accounts\"])\n",
    "\n",
    "        second_stage_account_predictions_en = test_account_df[[\"author_id\", target_col]].copy()\n",
    "        second_stage_account_predictions_en[\"pred_prob\"] = second_stage_test_prob_en\n",
    "        second_stage_account_predictions_en[\"pred_is_bot\"] = second_stage_test_pred_en\n",
    "\n",
    "    max_score_accounts = 4 * int((y_test_acc == 1).sum())\n",
    "    second_stage_score_ratio_en = (\n",
    "        second_stage_test_score_en / max_score_accounts if max_score_accounts > 0 else np.nan\n",
    "    )\n",
    "\n",
    "    max_possible_score = max_score_accounts\n",
    "\n",
    "print(\"Seed-level first-stage account scores (test):\")\n",
    "print(\n",
    "    seed_report_df[\n",
    "        [\n",
    "            \"seed\",\n",
    "            \"threshold\",\n",
    "            \"test_score\",\n",
    "            \"tp_accounts\",\n",
    "            \"fn_accounts\",\n",
    "            \"fp_accounts\",\n",
    "        ]\n",
    "    ].to_string(index=False)\n",
    ")\n",
    "print(f\"Seed score mean/std: {seed_mean_score_en:.2f} / {seed_std_score_en:.2f}\")\n",
    "print(f\"Ensemble aggregation: {ENSEMBLE_AGGREGATION}\")\n",
    "print(f\"Ensemble selected threshold: {selected_threshold_ensemble:.4f}\")\n",
    "print(\n",
    "    f\"Ensemble test score: {test_score_ensemble_en} (TP={ensemble_tp_accounts_en}, FN={ensemble_fn_accounts_en}, FP={ensemble_fp_accounts_en}, accounts={ensemble_n_accounts_en})\"\n",
    ")\n",
    "\n",
    "if USE_SECOND_STAGE_ACCOUNT_MODEL:\n",
    "    print(f\"Second-stage fit feature source: {second_stage_fit_feature_source_en}\")\n",
    "    if not second_stage_oof_report_en.empty:\n",
    "        print(\"OOF seed report for second-stage fit features:\")\n",
    "        print(second_stage_oof_report_en.to_string(index=False))\n",
    "\n",
    "    print(\"Second-stage candidate report:\")\n",
    "    print(second_stage_candidate_report_en.to_string(index=False))\n",
    "\n",
    "    if has_validation and baseline_account_val_score is not None:\n",
    "        print(\n",
    "            f\"Baseline account-level validation score (from first-stage means): {baseline_account_val_score} \"\n",
    "            f\"(TP={baseline_account_val_tp}, FN={baseline_account_val_fn}, FP={baseline_account_val_fp}, \"\n",
    "            f\"threshold={baseline_account_val_threshold:.4f})\"\n",
    "        )\n",
    "\n",
    "    print(f\"Second-stage profile mode: {SECOND_STAGE_PROFILE}\")\n",
    "    print(f\"Second-stage selected profile: {second_stage_selected_profile_en}\")\n",
    "    if second_stage_selected_profile_en != \"fallback_ensemble\":\n",
    "        print(f\"Second-stage blend alpha (CatBoost weight): {second_stage_alpha_en:.2f}\")\n",
    "        print(f\"Second-stage threshold: {second_stage_selected_threshold_en:.4f}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"Second-stage fallback triggered: \"\n",
    "\n",
    "            + (second_stage_fallback_reason_en if second_stage_fallback_reason_en else \"validation guardrails were not met.\")\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f\"Second-stage test score: {second_stage_test_score_en}/{max_possible_score} ({second_stage_score_ratio_en:.2%})\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Second-stage confusion components -> TP={second_stage_tp_accounts_en}, FN={second_stage_fn_accounts_en}, FP={second_stage_fp_accounts_en}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Second-stage precision={precision_score(y_test_acc, second_stage_test_pred_en, zero_division=0):.4f}, \"\n",
    "        f\"recall={recall_score(y_test_acc, second_stage_test_pred_en, zero_division=0):.4f}\"\n",
    "    )\n",
    "    print(classification_report(y_test_acc, second_stage_test_pred_en, digits=4))\n",
    "else:\n",
    "    print(\"Second-stage account model disabled in EXPERIMENT_CONFIG.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26c4f9a",
   "metadata": {},
   "source": [
    "## Final score plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "c477e2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x450 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without booster: 39\n",
      "With booster: 37\n",
      "Competition top (max possible on this split): 44\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if \"train_en_model\" not in globals() or \"test_idx\" not in globals():\n",
    "    raise ValueError(\"Run the training cell first.\")\n",
    "\n",
    "score_without_booster = None\n",
    "if \"test_score\" in globals():\n",
    "    score_without_booster = float(test_score)\n",
    "elif \"score\" in globals():\n",
    "    score_without_booster = float(score)\n",
    "else:\n",
    "    raise ValueError(\"No baseline score found. Run the model scoring cells first.\")\n",
    "\n",
    "score_with_booster = float(second_stage_test_score_en) if \"second_stage_test_score_en\" in globals() else None\n",
    "\n",
    "if \"max_possible_score\" in globals():\n",
    "    competition_top = float(max_possible_score)\n",
    "else:\n",
    "    account_truth = (\n",
    "        train_en_model.iloc[test_idx][[\"author_id\", \"is_bot\"]]\n",
    "        .groupby(\"author_id\", as_index=False)[\"is_bot\"]\n",
    "        .max()\n",
    "    )\n",
    "    competition_top = float(4 * int(account_truth[\"is_bot\"].sum()))\n",
    "\n",
    "labels = [\"Without booster\"]\n",
    "scores = [score_without_booster]\n",
    "colors = [\"#4c78a8\"]\n",
    "\n",
    "if score_with_booster is not None:\n",
    "    labels.append(\"With booster\")\n",
    "    scores.append(score_with_booster)\n",
    "    colors.append(\"#f58518\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4.5))\n",
    "bars = ax.bar(labels, scores, color=colors, alpha=0.9)\n",
    "ax.axhline(competition_top, color=\"#54a24b\", linestyle=\"--\", linewidth=2, label=f\"Competition top: {competition_top:.0f}\")\n",
    "\n",
    "for bar, value in zip(bars, scores):\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        value,\n",
    "        f\"{value:.0f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=10,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "ax.set_title(\"Competition Score Comparison\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_ylim(0, max(max(scores), competition_top) * 1.15)\n",
    "ax.grid(axis=\"y\", linestyle=\":\", alpha=0.4)\n",
    "ax.legend(loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Without booster: {score_without_booster:.0f}\")\n",
    "if score_with_booster is not None:\n",
    "    print(f\"With booster: {score_with_booster:.0f}\")\n",
    "print(f\"Competition top (max possible on this split): {competition_top:.0f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef7ab6",
   "metadata": {},
   "source": [
    "## Error analysis (aggregate only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "fc8171e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-level error summary (first-stage ensemble):\n",
      "error_type  n_posts\n",
      "        TN     1168\n",
      "        TP      429\n",
      "        FP      110\n",
      "        FN       63\n",
      "\n",
      "No topic matches found in the test split.\n",
      "\n",
      "Account-level summary: score=37 TP=10 FN=1 FP=1 TN=57\n",
      "\n",
      "Account-level errors by dominant topic:\n",
      "dominant_topic  n_accounts  bot_rate  fp_accounts  fn_accounts  tp_accounts  tn_accounts  error_rate\n",
      "no_topic_match          69   0.15942            1            1           10           57    0.028986\n",
      "\n",
      "Sample hard cases (for pattern inspection, not rule memorization):\n",
      "                           author_id                                                                                                                                                                                                                                                                                        text_clean  pred_prob error_type\n",
      "87f87356-1719-ab8e-88f6-9dd471800a5a                                                 , captivant des millions d'âmes à travers le globe. avec ses mouvements de hanche provocateurs et son style flamboyant, il a redéfini les normes de la scène musicale, laissant une empreinte indélébile dans l'histoire de la musique populaire.   0.993829         FP\n",
      "3d31f375-fdd8-9154-bef2-c44513b54e15                                                                                                                                                     ⚽️ pronos série a - voici nos pronostics pour cette journée : ✔️ udinese torino l2em (1,90) follow et like pour la suite💰 teamparieur seriea    0.983662         FP\n",
      "a401c68a-c2d8-85fb-85da-a9f93cd31b0f                                                                                                                                                                                                                         soudain t'as des impots, un loyer, la bouffe, l'essence à payer... <url>    0.978210         FP\n",
      "80d8ac94-241a-b15e-ad5a-e8c0213b2fff                                                                                                                                                        qu'il aille s'il veut , on a de quoi de le remplacer. on besoin urgemment besoin de lui . notre réservoir de joueur est très chargé <url>    0.976799         FP\n",
      "0ba72b44-68a6-9a54-92e5-0969cf65b990                                                                                                                                                                                                                                                                       🏒 pwhl toronto 2-1 montréal   0.969936         FP\n",
      "a9129fec-45b3-a0c5-8cf9-cd41dc304730                                    hier , je en pouvais pas marcher crise sciatique. mafamille directe me fait du chantage psychofinancier pour vente des 2 grands parents à marrakech. pas envie de signer point barre. j 'ai mes raisons. même plus le droit de dire non c 'est grave. !!!!!!!!   0.969303         FP\n",
      "62ad6eae-7733-b6be-a032-a0eacc7a736c                                                                                                                                                                                                                                             on a tellement rendu la langue de molière all-time...   0.956630         FP\n",
      "ff6cc651-3341-b8dc-b903-6fdd3b47b9bd                                                                                                                                                                                                                        tu découvres que ta meuf/ ton gar est enfaîte trans . tu réagis comment ??   0.943572         FP\n",
      "e32cba98-c6c7-a6e0-b657-3308d23b121e                                                                                                                                                                                                                               série, film, youtube, twitch, jeux vidéos, réseaux sociaux ? <url>    0.942109         FP\n",
      "b5c4e424-95d8-8e8d-aadd-c28f7d8a2f36                                                                                                                                                                                                                             messi, le chien d'anatomie d'une chute, sera-t-il aux oscars ? <url>    0.941522         FP\n",
      "1b9847e8-cf5d-431a-9e96-9cbb70c2c5eb                                                                                                                                                                                           suis renoi ivoirien mais j’apprécie les asiatiques qui sont sympas surtout les femmes japonaises <url>    0.008281         FN\n",
      "5198d1bd-fb3b-4146-ba1a-c24947b54417                                                                                                                                                                                   ligaendesa (j25) : la défaite inattendue de gran canaria contre manresa. albicy et pelos ont donné leur tout. 😬   0.013713         FN\n",
      "5198d1bd-fb3b-4146-ba1a-c24947b54417                                                                                                                                                                                                  donovan mitchell partage son combat avec sa blessure, une vraie leçon de sincérité. 🙁 letemknow    0.016141         FN\n",
      "f03e66fe-9cec-49ea-9150-7b992ab9a75e - fin de 1ère mi-temps : suns 59-52 nuggets malgré le forceps de denver, grosse fin de mi-temps. un genre de run bienvenu fait au bon moment. la superbe mi-temps est juste récompensée par cette avance de +7. pour l’instant, c’est la réaction que l’on attendait à juste titre, peu... <url>    0.024033         FN\n",
      "1b9847e8-cf5d-431a-9e96-9cbb70c2c5eb                                                                                                                                                                                                                      imaginez un final fantasy avec la magie de ff7 et l'immersion de ff15 <url>    0.025150         FN\n",
      "1b9847e8-cf5d-431a-9e96-9cbb70c2c5eb                                                                                                                                                         je vais m’exprimer. ça fond dans la bouche comme un nuage et laisse un parfum délicat. ce n’est parfait qu’avec un bon pain frais. <url>    0.027210         FN\n",
      "f03e66fe-9cec-49ea-9150-7b992ab9a75e                           - jacob perreault propulse le rocket en avant 1-0 à utica dès les premières secondes du deuxième vingt. passe impeccable du capitaine gabriel bourque sur le jeu. cela va sûrement alléger une énorme pression pour perreault qui marque un premier point en 16 matchs.   0.030042         FN\n",
      "1b9847e8-cf5d-431a-9e96-9cbb70c2c5eb                                                                                                                                                                                                                                       c’est une dinguerie pleure pour une danse quand meme <url>    0.035640         FN\n",
      "1b9847e8-cf5d-431a-9e96-9cbb70c2c5eb                                                                                              peut-être que vous essayez de déchiffrer une recette en hiéroglyphes et que vous n'avez aucune idée de ce que vous faites, car certains d'entre vous ici, c'est vraiment masterchef qui vous attend.   0.037807         FN\n",
      "5198d1bd-fb3b-4146-ba1a-c24947b54417                                                                                                                                                                                     le présidenf du burundi semble avoir un long chemin à parcourir avant de redresser la barre. politique <url>    0.041197         FN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "required = [\"train_en_model\", \"test_idx\", \"post_prob_test\"]\n",
    "missing = [name for name in required if name not in globals()]\n",
    "if missing:\n",
    "    raise ValueError(f\"Run the training and booster cells first. Missing: {missing}\")\n",
    "\n",
    "analysis_threshold = float(SELECTED_THRESHOLD if \"SELECTED_THRESHOLD\" in globals() else PREDICTION_THRESHOLD)\n",
    "\n",
    "posts_eval = train_en_model.iloc[test_idx].copy().reset_index(drop=True)\n",
    "posts_eval[\"true_is_bot\"] = posts_eval[\"is_bot\"].astype(np.int64)\n",
    "posts_eval[\"pred_prob\"] = np.asarray(post_prob_test, dtype=np.float32)\n",
    "posts_eval[\"pred_is_bot\"] = (posts_eval[\"pred_prob\"] >= analysis_threshold).astype(np.int64)\n",
    "\n",
    "posts_eval[\"error_type\"] = np.where(\n",
    "    (posts_eval[\"true_is_bot\"] == 1) & (posts_eval[\"pred_is_bot\"] == 0),\n",
    "    \"FN\",\n",
    "    np.where(\n",
    "        (posts_eval[\"true_is_bot\"] == 0) & (posts_eval[\"pred_is_bot\"] == 1),\n",
    "        \"FP\",\n",
    "        np.where(posts_eval[\"true_is_bot\"] == 1, \"TP\", \"TN\"),\n",
    "    ),\n",
    ")\n",
    "\n",
    "post_error_summary_en = (\n",
    "    posts_eval[\"error_type\"]\n",
    "    .value_counts()\n",
    "    .rename_axis(\"error_type\")\n",
    "    .reset_index(name=\"n_posts\")\n",
    ")\n",
    "\n",
    "print(\"Post-level error summary (first-stage ensemble):\")\n",
    "print(post_error_summary_en.to_string(index=False))\n",
    "\n",
    "if topic_feature_cols_en:\n",
    "    topic_rows = []\n",
    "    for topic_col in topic_feature_cols_en:\n",
    "        subset = posts_eval[posts_eval[topic_col] > 0]\n",
    "        if subset.empty:\n",
    "            continue\n",
    "\n",
    "        fp = int(((subset[\"true_is_bot\"] == 0) & (subset[\"pred_is_bot\"] == 1)).sum())\n",
    "        fn = int(((subset[\"true_is_bot\"] == 1) & (subset[\"pred_is_bot\"] == 0)).sum())\n",
    "        tp = int(((subset[\"true_is_bot\"] == 1) & (subset[\"pred_is_bot\"] == 1)).sum())\n",
    "        tn = int(((subset[\"true_is_bot\"] == 0) & (subset[\"pred_is_bot\"] == 0)).sum())\n",
    "        n = int(len(subset))\n",
    "\n",
    "        topic_rows.append(\n",
    "            {\n",
    "                \"topic\": topic_col.replace(\"topic_\", \"\"),\n",
    "                \"n_posts\": n,\n",
    "                \"fp_posts\": fp,\n",
    "                \"fn_posts\": fn,\n",
    "                \"tp_posts\": tp,\n",
    "                \"tn_posts\": tn,\n",
    "                \"error_rate\": (fp + fn) / n if n else 0.0,\n",
    "                \"bot_rate\": subset[\"true_is_bot\"].mean(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    topic_post_error_report_en = pd.DataFrame(topic_rows)\n",
    "    if not topic_post_error_report_en.empty:\n",
    "        min_topic_posts = int(EXPERIMENT_CONFIG.get(\"error_analysis_min_topic_posts\", 20))\n",
    "        topic_post_error_report_en = topic_post_error_report_en.sort_values(\n",
    "            by=[\"error_rate\", \"n_posts\"], ascending=[False, False]\n",
    "        )\n",
    "\n",
    "        print(\"\\nTopic-level post errors (filtering tiny buckets):\")\n",
    "        print(\n",
    "            topic_post_error_report_en[topic_post_error_report_en[\"n_posts\"] >= min_topic_posts]\n",
    "            .head(15)\n",
    "            .to_string(index=False)\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nNo topic matches found in the test split.\")\n",
    "else:\n",
    "    print(\"\\nTopic features are disabled, so no topic-level error table was computed.\")\n",
    "\n",
    "if \"second_stage_account_predictions_en\" in globals() and isinstance(second_stage_account_predictions_en, pd.DataFrame):\n",
    "    account_eval = second_stage_account_predictions_en.copy()\n",
    "    if \"true_is_bot\" not in account_eval.columns and \"is_bot\" in account_eval.columns:\n",
    "        account_eval = account_eval.rename(columns={\"is_bot\": \"true_is_bot\"})\n",
    "else:\n",
    "    account_eval = (\n",
    "        posts_eval.groupby(\"author_id\", as_index=False)\n",
    "        .agg(\n",
    "            true_is_bot=(\"true_is_bot\", \"max\"),\n",
    "            mean_prob=(\"pred_prob\", \"mean\"),\n",
    "            any_pred=(\"pred_is_bot\", \"max\"),\n",
    "        )\n",
    "        .copy()\n",
    "    )\n",
    "\n",
    "    if ACCOUNT_DECISION_RULE == \"any\":\n",
    "        account_eval[\"pred_is_bot\"] = account_eval[\"any_pred\"].astype(np.int64)\n",
    "        account_eval[\"pred_prob\"] = account_eval[\"any_pred\"].astype(np.float32)\n",
    "    else:\n",
    "        account_eval[\"pred_is_bot\"] = (account_eval[\"mean_prob\"] >= analysis_threshold).astype(np.int64)\n",
    "        account_eval[\"pred_prob\"] = account_eval[\"mean_prob\"].astype(np.float32)\n",
    "\n",
    "if topic_feature_cols_en:\n",
    "    topic_strength = posts_eval.groupby(\"author_id\")[topic_feature_cols_en].mean()\n",
    "    dominant_topic = topic_strength.idxmax(axis=1).str.replace(\"topic_\", \"\", regex=False)\n",
    "    dominant_topic = dominant_topic.where(topic_strength.max(axis=1) > 0, \"no_topic_match\")\n",
    "else:\n",
    "    dominant_topic = pd.Series(\"no_topic_features\", index=account_eval[\"author_id\"])\n",
    "\n",
    "account_eval = account_eval.merge(\n",
    "    dominant_topic.rename(\"dominant_topic\"),\n",
    "    left_on=\"author_id\",\n",
    "    right_index=True,\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "account_eval[\"error_type\"] = np.where(\n",
    "    (account_eval[\"true_is_bot\"] == 1) & (account_eval[\"pred_is_bot\"] == 0),\n",
    "    \"FN\",\n",
    "    np.where(\n",
    "        (account_eval[\"true_is_bot\"] == 0) & (account_eval[\"pred_is_bot\"] == 1),\n",
    "        \"FP\",\n",
    "        np.where(account_eval[\"true_is_bot\"] == 1, \"TP\", \"TN\"),\n",
    "    ),\n",
    ")\n",
    "\n",
    "account_tp = int((account_eval[\"error_type\"] == \"TP\").sum())\n",
    "account_fn = int((account_eval[\"error_type\"] == \"FN\").sum())\n",
    "account_fp = int((account_eval[\"error_type\"] == \"FP\").sum())\n",
    "account_tn = int((account_eval[\"error_type\"] == \"TN\").sum())\n",
    "account_score = (4 * account_tp) - account_fn - (2 * account_fp)\n",
    "print(f\"\\nAccount-level summary: score={account_score} TP={account_tp} FN={account_fn} FP={account_fp} TN={account_tn}\")\n",
    "account_topic_report_en = (\n",
    "    account_eval.groupby(\"dominant_topic\", as_index=False)\n",
    "    .agg(\n",
    "        n_accounts=(\"author_id\", \"size\"),\n",
    "        bot_rate=(\"true_is_bot\", \"mean\"),\n",
    "        fp_accounts=(\"error_type\", lambda s: int((s == \"FP\").sum())),\n",
    "        fn_accounts=(\"error_type\", lambda s: int((s == \"FN\").sum())),\n",
    "        tp_accounts=(\"error_type\", lambda s: int((s == \"TP\").sum())),\n",
    "        tn_accounts=(\"error_type\", lambda s: int((s == \"TN\").sum())),\n",
    "    )\n",
    ")\n",
    "\n",
    "account_topic_report_en[\"error_rate\"] = (\n",
    "    account_topic_report_en[\"fp_accounts\"] + account_topic_report_en[\"fn_accounts\"]\n",
    ") / account_topic_report_en[\"n_accounts\"]\n",
    "account_topic_report_en = account_topic_report_en.sort_values(\n",
    "    by=[\"error_rate\", \"n_accounts\"], ascending=[False, False]\n",
    ")\n",
    "\n",
    "min_topic_accounts = int(EXPERIMENT_CONFIG.get(\"error_analysis_min_topic_accounts\", 5))\n",
    "print(\"\\nAccount-level errors by dominant topic:\")\n",
    "print(\n",
    "    account_topic_report_en[account_topic_report_en[\"n_accounts\"] >= min_topic_accounts]\n",
    "    .head(15)\n",
    "    .to_string(index=False)\n",
    ")\n",
    "\n",
    "fp_examples = posts_eval[\n",
    "    (posts_eval[\"true_is_bot\"] == 0) & (posts_eval[\"pred_is_bot\"] == 1)\n",
    "][[\"author_id\", \"text_clean\", \"pred_prob\"]].sort_values(\"pred_prob\", ascending=False).head(10)\n",
    "fn_examples = posts_eval[\n",
    "    (posts_eval[\"true_is_bot\"] == 1) & (posts_eval[\"pred_is_bot\"] == 0)\n",
    "][[\"author_id\", \"text_clean\", \"pred_prob\"]].sort_values(\"pred_prob\", ascending=True).head(10)\n",
    "\n",
    "hard_case_examples_en = pd.concat(\n",
    "    [\n",
    "        fp_examples.assign(error_type=\"FP\"),\n",
    "        fn_examples.assign(error_type=\"FN\"),\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "if not hard_case_examples_en.empty:\n",
    "    print(\"\\nSample hard cases (for pattern inspection, not rule memorization):\")\n",
    "    print(hard_case_examples_en.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391775aa",
   "metadata": {},
   "source": [
    "## Booster search prep (do not run long sweep yet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "06d34ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space prepared for later long run.\n",
      "- dimensions: 13\n",
      "- artifacts dir: /Users/adrienbelanger/Library/GitHub/bot_or_not_McHacks_2026/artifacts\n",
      "- results file: /Users/adrienbelanger/Library/GitHub/bot_or_not_McHacks_2026/artifacts/booster_search_results.csv\n",
      "- notebook path for sweep source: /Users/adrienbelanger/Library/GitHub/bot_or_not_McHacks_2026/script_fr.ipynb\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "base_root = Path(globals().get(\"PROJECT_ROOT\", Path(\".\")))\n",
    "ARTIFACTS_DIR = Path(globals().get(\"ARTIFACTS_DIR\", base_root / \"artifacts\")).resolve()\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "globals()[\"ARTIFACTS_DIR\"] = ARTIFACTS_DIR\n",
    "\n",
    "SEARCH_RESULTS_PATH = ARTIFACTS_DIR / \"booster_search_results.csv\"\n",
    "SEARCH_BEST_PATH = ARTIFACTS_DIR / \"booster_best_config.json\"\n",
    "\n",
    "SWEEP_NOTEBOOK_PATH = Path(\n",
    "    globals().get(\"SWEEP_NOTEBOOK_PATH\", base_root / \"script_fr.ipynb\")\n",
    ").resolve()\n",
    "globals()[\"SWEEP_NOTEBOOK_PATH\"] = SWEEP_NOTEBOOK_PATH\n",
    "\n",
    "BOOSTER_SEARCH_SPACE = {\n",
    "    \"second_stage_profile\": [\"auto\", \"legacy\", \"regularized\"],\n",
    "    \"second_stage_max_iter\": [200, 300, 450],\n",
    "    \"second_stage_max_depth\": [3, 4, 5],\n",
    "    \"second_stage_l2\": [0.1, 0.2, 0.5, 1.0],\n",
    "    \"second_stage_min_data_in_leaf\": [10, 20, 40],\n",
    "    \"second_stage_subsample\": [0.7, 0.8, 1.0],\n",
    "    \"second_stage_rsm\": [0.7, 0.8, 1.0],\n",
    "    \"second_stage_blend_alphas\": [[1.0], [1.0, 0.9], [1.0, 0.9, 0.8, 0.7]],\n",
    "    \"second_stage_min_gain_vs_ensemble\": [0, 1, 2],\n",
    "    \"second_stage_use_oof_features\": [True],\n",
    "    \"second_stage_oof_folds\": [3, 4],\n",
    "    \"second_stage_oof_epochs\": [3, 4],\n",
    "    \"second_stage_oof_seeds\": [[13], [13, 42]],\n",
    "}\n",
    "\n",
    "print(\"Search space prepared for later long run.\")\n",
    "print(f\"- dimensions: {len(BOOSTER_SEARCH_SPACE)}\")\n",
    "print(f\"- artifacts dir: {ARTIFACTS_DIR}\")\n",
    "print(f\"- results file: {SEARCH_RESULTS_PATH}\")\n",
    "print(f\"- notebook path for sweep source: {SWEEP_NOTEBOOK_PATH}\")\n",
    "if not SWEEP_NOTEBOOK_PATH.exists():\n",
    "    print(\n",
    "        \"Warning: notebook source file not found now. \"\n",
    "        \"Before running sweep, set SWEEP_SETTINGS['notebook_path'] to your .ipynb path if needed.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _safe_float(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def log_current_booster_run(run_name=None, notes=\"\", verbose=True):\n",
    "    SEARCH_RESULTS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    SEARCH_BEST_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    timestamp = datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    if run_name is None:\n",
    "        run_name = f\"run_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "    row = {\n",
    "        \"timestamp_utc\": timestamp,\n",
    "        \"run_name\": run_name,\n",
    "        \"notes\": notes,\n",
    "        \"ensemble_score\": _safe_float(globals().get(\"test_score_ensemble_en\", np.nan)),\n",
    "        \"booster_score\": _safe_float(globals().get(\"second_stage_test_score_en\", np.nan)),\n",
    "        \"score_gain\": _safe_float(globals().get(\"second_stage_test_score_en\", np.nan)) - _safe_float(globals().get(\"test_score_ensemble_en\", np.nan)),\n",
    "        \"seed_mean_score\": _safe_float(globals().get(\"seed_mean_score_en\", np.nan)),\n",
    "        \"seed_std_score\": _safe_float(globals().get(\"seed_std_score_en\", np.nan)),\n",
    "        \"selected_profile\": globals().get(\"second_stage_selected_profile_en\", \"\"),\n",
    "        \"selected_alpha\": _safe_float(globals().get(\"second_stage_alpha_en\", np.nan)),\n",
    "        \"selected_threshold\": _safe_float(globals().get(\"second_stage_selected_threshold_en\", np.nan)),\n",
    "        \"feature_source\": globals().get(\"second_stage_fit_feature_source_en\", \"\"),\n",
    "        \"config_json\": json.dumps(EXPERIMENT_CONFIG, sort_keys=True),\n",
    "    }\n",
    "\n",
    "    current = pd.DataFrame([row])\n",
    "    if SEARCH_RESULTS_PATH.exists():\n",
    "        history = pd.read_csv(SEARCH_RESULTS_PATH)\n",
    "        history = pd.concat([history, current], ignore_index=True)\n",
    "    else:\n",
    "        history = current\n",
    "\n",
    "    history = history.drop_duplicates(subset=[\"run_name\"], keep=\"last\")\n",
    "    history.to_csv(SEARCH_RESULTS_PATH, index=False)\n",
    "\n",
    "    leaderboard = history.sort_values(\n",
    "        by=[\"booster_score\", \"score_gain\", \"seed_std_score\"],\n",
    "        ascending=[False, False, True],\n",
    "    )\n",
    "\n",
    "    if len(leaderboard):\n",
    "        best_row = leaderboard.iloc[0].to_dict()\n",
    "        best_payload = {\n",
    "            \"saved_at_utc\": timestamp,\n",
    "            \"best_run\": best_row,\n",
    "            \"best_config\": json.loads(best_row.get(\"config_json\", \"{}\")),\n",
    "        }\n",
    "        SEARCH_BEST_PATH.write_text(json.dumps(best_payload, indent=2))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Saved run to {SEARCH_RESULTS_PATH}\")\n",
    "        print(f\"Best config snapshot saved to {SEARCH_BEST_PATH}\")\n",
    "        print(\"\\nTop runs:\")\n",
    "        print(\n",
    "            leaderboard[\n",
    "                [\n",
    "                    \"run_name\",\n",
    "                    \"ensemble_score\",\n",
    "                    \"booster_score\",\n",
    "                    \"score_gain\",\n",
    "                    \"seed_std_score\",\n",
    "                    \"selected_profile\",\n",
    "                    \"feature_source\",\n",
    "                ]\n",
    "            ].head(10).to_string(index=False)\n",
    "        )\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def plot_search_history(top_n=20):\n",
    "    if not SEARCH_RESULTS_PATH.exists():\n",
    "        raise FileNotFoundError(f\"No results found at {SEARCH_RESULTS_PATH}\")\n",
    "\n",
    "    history = pd.read_csv(SEARCH_RESULTS_PATH)\n",
    "    if history.empty:\n",
    "        raise ValueError(\"Search history is empty.\")\n",
    "\n",
    "    leaderboard = history.sort_values(\n",
    "        by=[\"booster_score\", \"score_gain\", \"seed_std_score\"],\n",
    "        ascending=[False, False, True],\n",
    "    ).head(top_n)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    x = np.arange(len(leaderboard))\n",
    "    plt.plot(x, leaderboard[\"ensemble_score\"].to_numpy(), marker=\"o\", label=\"without booster\")\n",
    "    plt.plot(x, leaderboard[\"booster_score\"].to_numpy(), marker=\"o\", label=\"with booster\")\n",
    "    plt.xticks(x, leaderboard[\"run_name\"], rotation=70)\n",
    "    plt.ylabel(\"Competition score\")\n",
    "    plt.title(f\"Top {top_n} logged runs\")\n",
    "    plt.grid(axis=\"y\", linestyle=\":\", alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b89b8e",
   "metadata": {},
   "source": [
    "## Automated booster sweep runner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "0be8474b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sweep runner ready. Edit SWEEP_SETTINGS, then call `run_booster_sweep()`.\n",
      "- trials file: /Users/adrienbelanger/Library/GitHub/bot_or_not_McHacks_2026/artifacts/booster_sweep_trials.csv\n",
      "- notebook path: /Users/adrienbelanger/Library/GitHub/bot_or_not_McHacks_2026/script_fr.ipynb\n",
      "Loaded booster source from: /Users/adrienbelanger/Library/GitHub/bot_or_not_McHacks_2026/script_fr.ipynb\n",
      "Starting sweep: trials=20, include_current=True, without_replacement=True, space_size=209,952\n",
      "\n",
      "[1/20] Running week_sweep_0001 ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-10 12:18:06.114734: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-10 12:18:11.733672: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-10 12:18:22.518975: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[119]\u001b[39m\u001b[32m, line 355\u001b[39m\n\u001b[32m    351\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m sweep_df\n\u001b[32m    354\u001b[39m \u001b[38;5;66;03m# Example usage (manual):\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m355\u001b[39m sweep_results = \u001b[43mrun_booster_sweep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mn_trials\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m20\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrun_prefix\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mweek_sweep\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43minclude_current_config\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msample_without_replacement\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mplot_top_n\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m25\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Optional in Colab: set this if auto-detection does not find your notebook file.\u001b[39;49;00m\n\u001b[32m    362\u001b[39m \u001b[43m     \u001b[49m\u001b[38;5;66;43;03m# \"notebook_path\": \"/content/drive/MyDrive/bot_or_not_McHacks_2026/script_fr.ipynb\",\u001b[39;49;00m\n\u001b[32m    363\u001b[39m \u001b[43m \u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[119]\u001b[39m\u001b[32m, line 275\u001b[39m, in \u001b[36mrun_booster_sweep\u001b[39m\u001b[34m(settings)\u001b[39m\n\u001b[32m    272\u001b[39m run_name = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_prefix\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m04d\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    273\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00midx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(queue)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m] Running \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mrun_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m275\u001b[39m row = \u001b[43m_run_one_trial\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbase_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbase_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m    \u001b[49m\u001b[43moverride_cfg\u001b[49m\u001b[43m=\u001b[49m\u001b[43moverride_cfg\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    278\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrun_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbooster_code\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbooster_code\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    280\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstop_on_error\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstop_on_error\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    281\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    282\u001b[39m rows.append(row)\n\u001b[32m    284\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m row[\u001b[33m\"\u001b[39m\u001b[33mstatus\u001b[39m\u001b[33m\"\u001b[39m] == \u001b[33m\"\u001b[39m\u001b[33mok\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[119]\u001b[39m\u001b[32m, line 176\u001b[39m, in \u001b[36m_run_one_trial\u001b[39m\u001b[34m(base_config, override_cfg, run_name, booster_code, stop_on_error)\u001b[39m\n\u001b[32m    174\u001b[39m start = time.time()\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     \u001b[43mexec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbooster_code\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mglobals\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    177\u001b[39m     duration = time.time() - start\n\u001b[32m    179\u001b[39m     notes_payload = {\n\u001b[32m    180\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mduration_sec\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mround\u001b[39m(\u001b[38;5;28mfloat\u001b[39m(duration), \u001b[32m2\u001b[39m),\n\u001b[32m    181\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33moverride\u001b[39m\u001b[33m\"\u001b[39m: override_cfg,\n\u001b[32m    182\u001b[39m     }\n",
      "\u001b[36mFile \u001b[39m\u001b[32m<string>:535\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/GitHub/bot_or_not_McHacks_2026/venv_botsornot/lib/python3.13/site-packages/keras/src/utils/traceback_utils.py:117\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    116\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m117\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/GitHub/bot_or_not_McHacks_2026/venv_botsornot/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py:399\u001b[39m, in \u001b[36mTensorFlowTrainer.fit\u001b[39m\u001b[34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq)\u001b[39m\n\u001b[32m    397\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m begin_step, end_step, iterator \u001b[38;5;129;01min\u001b[39;00m epoch_iterator:\n\u001b[32m    398\u001b[39m     callbacks.on_train_batch_begin(begin_step)\n\u001b[32m--> \u001b[39m\u001b[32m399\u001b[39m     logs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_function\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    400\u001b[39m     callbacks.on_train_batch_end(end_step, logs)\n\u001b[32m    401\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.stop_training:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/GitHub/bot_or_not_McHacks_2026/venv_botsornot/lib/python3.13/site-packages/keras/src/backend/tensorflow/trainer.py:241\u001b[39m, in \u001b[36mTensorFlowTrainer._make_function.<locals>.function\u001b[39m\u001b[34m(iterator)\u001b[39m\n\u001b[32m    237\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfunction\u001b[39m(iterator):\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\n\u001b[32m    239\u001b[39m         iterator, (tf.data.Iterator, tf.distribute.DistributedIterator)\n\u001b[32m    240\u001b[39m     ):\n\u001b[32m--> \u001b[39m\u001b[32m241\u001b[39m         opt_outputs = \u001b[43mmulti_step_on_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    242\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m opt_outputs.has_value():\n\u001b[32m    243\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/GitHub/bot_or_not_McHacks_2026/venv_botsornot/lib/python3.13/site-packages/tensorflow/python/util/traceback_utils.py:150\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    148\u001b[39m filtered_tb = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    149\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m150\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/GitHub/bot_or_not_McHacks_2026/venv_botsornot/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:833\u001b[39m, in \u001b[36mFunction.__call__\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    830\u001b[39m compiler = \u001b[33m\"\u001b[39m\u001b[33mxla\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mnonXla\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m._jit_compile):\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m   result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    835\u001b[39m new_tracing_count = \u001b[38;5;28mself\u001b[39m.experimental_get_tracing_count()\n\u001b[32m    836\u001b[39m without_tracing = (tracing_count == new_tracing_count)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/GitHub/bot_or_not_McHacks_2026/venv_botsornot/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py:878\u001b[39m, in \u001b[36mFunction._call\u001b[39m\u001b[34m(self, *args, **kwds)\u001b[39m\n\u001b[32m    875\u001b[39m \u001b[38;5;28mself\u001b[39m._lock.release()\n\u001b[32m    876\u001b[39m \u001b[38;5;66;03m# In this case we have not created variables on the first call. So we can\u001b[39;00m\n\u001b[32m    877\u001b[39m \u001b[38;5;66;03m# run the first trace but we should fail if variables are created.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m878\u001b[39m results = \u001b[43mtracing_compilation\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_variable_creation_config\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    881\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._created_variables:\n\u001b[32m    882\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCreating variables on a non-first call to a function\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    883\u001b[39m                    \u001b[33m\"\u001b[39m\u001b[33m decorated with tf.function.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/GitHub/bot_or_not_McHacks_2026/venv_botsornot/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py:139\u001b[39m, in \u001b[36mcall_function\u001b[39m\u001b[34m(args, kwargs, tracing_options)\u001b[39m\n\u001b[32m    137\u001b[39m bound_args = function.function_type.bind(*args, **kwargs)\n\u001b[32m    138\u001b[39m flat_inputs = function.function_type.unpack_inputs(bound_args)\n\u001b[32m--> \u001b[39m\u001b[32m139\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[32m    140\u001b[39m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfunction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[32m    141\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/GitHub/bot_or_not_McHacks_2026/venv_botsornot/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py:1322\u001b[39m, in \u001b[36mConcreteFunction._call_flat\u001b[39m\u001b[34m(self, tensor_inputs, captured_inputs)\u001b[39m\n\u001b[32m   1318\u001b[39m possible_gradient_type = gradients_util.PossibleTapeGradientTypes(args)\n\u001b[32m   1319\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type == gradients_util.POSSIBLE_GRADIENT_TYPES_NONE\n\u001b[32m   1320\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[32m   1321\u001b[39m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1322\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_inference_function\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1323\u001b[39m forward_backward = \u001b[38;5;28mself\u001b[39m._select_forward_and_backward_functions(\n\u001b[32m   1324\u001b[39m     args,\n\u001b[32m   1325\u001b[39m     possible_gradient_type,\n\u001b[32m   1326\u001b[39m     executing_eagerly)\n\u001b[32m   1327\u001b[39m forward_function, args_with_tangents = forward_backward.forward()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/GitHub/bot_or_not_McHacks_2026/venv_botsornot/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:216\u001b[39m, in \u001b[36mAtomicFunction.call_preflattened\u001b[39m\u001b[34m(self, args)\u001b[39m\n\u001b[32m    214\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core.Tensor]) -> Any:\n\u001b[32m    215\u001b[39m \u001b[38;5;250m  \u001b[39m\u001b[33;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m   flat_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.function_type.pack_output(flat_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/GitHub/bot_or_not_McHacks_2026/venv_botsornot/lib/python3.13/site-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py:251\u001b[39m, in \u001b[36mAtomicFunction.call_flat\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m record.stop_recording():\n\u001b[32m    250\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_context.executing_eagerly():\n\u001b[32m--> \u001b[39m\u001b[32m251\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bound_context\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    253\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    254\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfunction_type\u001b[49m\u001b[43m.\u001b[49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    255\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    256\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    257\u001b[39m     outputs = make_call_op_in_graph(\n\u001b[32m    258\u001b[39m         \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    259\u001b[39m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[32m    260\u001b[39m         \u001b[38;5;28mself\u001b[39m._bound_context.function_call_options.as_attrs(),\n\u001b[32m    261\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/GitHub/bot_or_not_McHacks_2026/venv_botsornot/lib/python3.13/site-packages/tensorflow/python/eager/context.py:1688\u001b[39m, in \u001b[36mContext.call_function\u001b[39m\u001b[34m(self, name, tensor_inputs, num_outputs)\u001b[39m\n\u001b[32m   1686\u001b[39m cancellation_context = cancellation.context()\n\u001b[32m   1687\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1688\u001b[39m   outputs = \u001b[43mexecute\u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1689\u001b[39m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mutf-8\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1690\u001b[39m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1691\u001b[39m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1692\u001b[39m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1693\u001b[39m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1694\u001b[39m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1695\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1696\u001b[39m   outputs = execute.execute_with_cancellation(\n\u001b[32m   1697\u001b[39m       name.decode(\u001b[33m\"\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m   1698\u001b[39m       num_outputs=num_outputs,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1702\u001b[39m       cancellation_manager=cancellation_context,\n\u001b[32m   1703\u001b[39m   )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Library/GitHub/bot_or_not_McHacks_2026/venv_botsornot/lib/python3.13/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = \u001b[43mpywrap_tfe\u001b[49m\u001b[43m.\u001b[49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import copy\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if \"BOOSTER_SEARCH_SPACE\" not in globals() or \"log_current_booster_run\" not in globals():\n",
    "    raise ValueError(\"Run the Booster search prep cell first.\")\n",
    "if \"EXPERIMENT_CONFIG\" not in globals():\n",
    "    raise ValueError(\"Run the Experiment parameters cell first.\")\n",
    "if \"train_en_model\" not in globals() or \"fit_idx\" not in globals() or \"test_idx\" not in globals():\n",
    "    raise ValueError(\"Run the Train-Test split and booster baseline cells first.\")\n",
    "\n",
    "ARTIFACTS_DIR = Path(\n",
    "    globals().get(\"ARTIFACTS_DIR\", Path(globals().get(\"PROJECT_ROOT\", Path(\".\"))) / \"artifacts\")\n",
    ").resolve()\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SWEEP_TRIALS_PATH = ARTIFACTS_DIR / \"booster_sweep_trials.csv\"\n",
    "\n",
    "DEFAULT_NOTEBOOK_PATH = Path(\n",
    "    globals().get(\"SWEEP_NOTEBOOK_PATH\", Path(globals().get(\"PROJECT_ROOT\", Path(\".\"))) / \"script_fr.ipynb\")\n",
    ").resolve()\n",
    "\n",
    "SWEEP_SETTINGS = {\n",
    "    \"n_trials\": 80,\n",
    "    \"run_prefix\": \"sweep\",\n",
    "    \"random_seed\": 42,\n",
    "    \"include_current_config\": True,\n",
    "    \"sample_without_replacement\": True,\n",
    "    \"stop_on_error\": False,\n",
    "    \"apply_best_config_at_end\": False,\n",
    "    \"plot_top_n\": 20,\n",
    "    \"notebook_path\": str(DEFAULT_NOTEBOOK_PATH),\n",
    "}\n",
    "\n",
    "print(\"Sweep runner ready. Edit SWEEP_SETTINGS, then call `run_booster_sweep()`.\")\n",
    "print(f\"- trials file: {SWEEP_TRIALS_PATH}\")\n",
    "print(f\"- notebook path: {SWEEP_SETTINGS['notebook_path']}\")\n",
    "\n",
    "\n",
    "def _space_size(space):\n",
    "    return int(math.prod(len(values) for values in space.values()))\n",
    "\n",
    "\n",
    "def _config_signature(cfg):\n",
    "    return json.dumps(cfg, sort_keys=True)\n",
    "\n",
    "\n",
    "def _draw_random_override(space, rng):\n",
    "    out = {}\n",
    "    for key, values in space.items():\n",
    "        out[key] = copy.deepcopy(values[rng.randrange(len(values))])\n",
    "    return out\n",
    "\n",
    "\n",
    "def _extract_space_override(config, space):\n",
    "    out = {}\n",
    "    for key in space.keys():\n",
    "        if key in config:\n",
    "            out[key] = copy.deepcopy(config[key])\n",
    "    return out\n",
    "\n",
    "\n",
    "def _candidate_notebook_paths(nb_path=None):\n",
    "    candidates = []\n",
    "\n",
    "    if nb_path:\n",
    "        candidates.append(Path(nb_path))\n",
    "\n",
    "    sweep_global = globals().get(\"SWEEP_NOTEBOOK_PATH\")\n",
    "    if sweep_global:\n",
    "        candidates.append(Path(sweep_global))\n",
    "\n",
    "    project_root = Path(globals().get(\"PROJECT_ROOT\", Path(\".\")))\n",
    "    candidates.append(project_root / \"script_fr.ipynb\")\n",
    "    candidates.append(Path(\"script_fr.ipynb\"))\n",
    "\n",
    "    out = []\n",
    "    seen = set()\n",
    "    for p in candidates:\n",
    "        try:\n",
    "            rp = p.expanduser().resolve()\n",
    "        except Exception:\n",
    "            continue\n",
    "        key = str(rp)\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        out.append(rp)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _load_booster_cell_source_from_notebook(nb_path=None):\n",
    "    checked = []\n",
    "\n",
    "    for candidate in _candidate_notebook_paths(nb_path=nb_path):\n",
    "        checked.append(str(candidate))\n",
    "        if not candidate.exists():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            nb = json.loads(candidate.read_text())\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        cells = nb.get(\"cells\", [])\n",
    "        for idx, cell in enumerate(cells):\n",
    "            if cell.get(\"cell_type\") != \"markdown\":\n",
    "                continue\n",
    "            md = \"\".join(cell.get(\"source\", []))\n",
    "            if \"## Strongest booster (account-level second stage)\" in md:\n",
    "                if idx + 1 >= len(cells):\n",
    "                    break\n",
    "                code_cell = cells[idx + 1]\n",
    "                if code_cell.get(\"cell_type\") != \"code\":\n",
    "                    break\n",
    "                print(f\"Loaded booster source from: {candidate}\")\n",
    "                return \"\".join(code_cell.get(\"source\", []))\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not locate notebook file containing the booster cell. \"\n",
    "        \"Set SWEEP_SETTINGS['notebook_path'] to your .ipynb path. \"\n",
    "        f\"Checked: {checked}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _build_trial_queue(base_config, space, n_trials, include_current, sample_without_replacement, rng):\n",
    "    queue = []\n",
    "    seen = set()\n",
    "\n",
    "    if include_current:\n",
    "        current_override = _extract_space_override(base_config, space)\n",
    "        sig = _config_signature(current_override)\n",
    "        seen.add(sig)\n",
    "        queue.append(current_override)\n",
    "\n",
    "    max_unique = _space_size(space)\n",
    "    target = int(n_trials)\n",
    "    if include_current and target > 0:\n",
    "        target -= 1\n",
    "\n",
    "    if sample_without_replacement:\n",
    "        target = min(target, max_unique - len(seen))\n",
    "\n",
    "    attempts = 0\n",
    "    max_attempts = max(1000, target * 80)\n",
    "\n",
    "    while len(queue) < (target + (1 if include_current else 0)):\n",
    "        cand = _draw_random_override(space, rng)\n",
    "        sig = _config_signature(cand)\n",
    "\n",
    "        if sample_without_replacement and sig in seen:\n",
    "            attempts += 1\n",
    "            if attempts > max_attempts:\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        seen.add(sig)\n",
    "        queue.append(cand)\n",
    "\n",
    "    return queue\n",
    "\n",
    "\n",
    "def _run_one_trial(base_config, override_cfg, run_name, booster_code, stop_on_error=False):\n",
    "    trial_config = copy.deepcopy(base_config)\n",
    "    trial_config.update(copy.deepcopy(override_cfg))\n",
    "    globals()[\"EXPERIMENT_CONFIG\"] = trial_config\n",
    "\n",
    "    start = time.time()\n",
    "    try:\n",
    "        exec(booster_code, globals())\n",
    "        duration = time.time() - start\n",
    "\n",
    "        notes_payload = {\n",
    "            \"duration_sec\": round(float(duration), 2),\n",
    "            \"override\": override_cfg,\n",
    "        }\n",
    "        log_current_booster_run(\n",
    "            run_name=run_name,\n",
    "            notes=json.dumps(notes_payload, sort_keys=True),\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"run_name\": run_name,\n",
    "            \"status\": \"ok\",\n",
    "            \"duration_sec\": float(duration),\n",
    "            \"ensemble_score\": float(globals().get(\"test_score_ensemble_en\", np.nan)),\n",
    "            \"booster_score\": float(globals().get(\"second_stage_test_score_en\", np.nan)),\n",
    "            \"score_gain\": float(globals().get(\"second_stage_test_score_en\", np.nan))\n",
    "            - float(globals().get(\"test_score_ensemble_en\", np.nan)),\n",
    "            \"seed_std_score\": float(globals().get(\"seed_std_score_en\", np.nan)),\n",
    "            \"selected_profile\": str(globals().get(\"second_stage_selected_profile_en\", \"\")),\n",
    "            \"selected_alpha\": float(globals().get(\"second_stage_alpha_en\", np.nan)),\n",
    "            \"selected_threshold\": float(globals().get(\"second_stage_selected_threshold_en\", np.nan)),\n",
    "            \"feature_source\": str(globals().get(\"second_stage_fit_feature_source_en\", \"\")),\n",
    "            \"error\": \"\",\n",
    "            \"override_json\": json.dumps(override_cfg, sort_keys=True),\n",
    "        }\n",
    "    except Exception as exc:\n",
    "        duration = time.time() - start\n",
    "        err = f\"{type(exc).__name__}: {exc}\"\n",
    "        tb = traceback.format_exc(limit=4)\n",
    "\n",
    "        row = {\n",
    "            \"run_name\": run_name,\n",
    "            \"status\": \"failed\",\n",
    "            \"duration_sec\": float(duration),\n",
    "            \"ensemble_score\": np.nan,\n",
    "            \"booster_score\": np.nan,\n",
    "            \"score_gain\": np.nan,\n",
    "            \"seed_std_score\": np.nan,\n",
    "            \"selected_profile\": \"\",\n",
    "            \"selected_alpha\": np.nan,\n",
    "            \"selected_threshold\": np.nan,\n",
    "            \"feature_source\": \"\",\n",
    "            \"error\": f\"{err} | {tb}\",\n",
    "            \"override_json\": json.dumps(override_cfg, sort_keys=True),\n",
    "        }\n",
    "\n",
    "        if stop_on_error:\n",
    "            raise\n",
    "        return row\n",
    "\n",
    "\n",
    "def run_booster_sweep(settings=None):\n",
    "    cfg = copy.deepcopy(SWEEP_SETTINGS)\n",
    "    if settings is not None:\n",
    "        cfg.update(settings)\n",
    "\n",
    "    n_trials = int(cfg.get(\"n_trials\", 50))\n",
    "    if n_trials < 1:\n",
    "        raise ValueError(\"n_trials must be >= 1\")\n",
    "\n",
    "    rng = random.Random(int(cfg.get(\"random_seed\", 42)))\n",
    "    run_prefix = str(cfg.get(\"run_prefix\", \"sweep\"))\n",
    "    include_current = bool(cfg.get(\"include_current_config\", True))\n",
    "    sample_wo_repl = bool(cfg.get(\"sample_without_replacement\", True))\n",
    "    stop_on_error = bool(cfg.get(\"stop_on_error\", False))\n",
    "    apply_best = bool(cfg.get(\"apply_best_config_at_end\", False))\n",
    "    plot_top_n = int(cfg.get(\"plot_top_n\", 20))\n",
    "    notebook_path = cfg.get(\"notebook_path\", str(DEFAULT_NOTEBOOK_PATH))\n",
    "\n",
    "    base_config = copy.deepcopy(EXPERIMENT_CONFIG)\n",
    "    booster_code = _load_booster_cell_source_from_notebook(nb_path=notebook_path)\n",
    "\n",
    "    queue = _build_trial_queue(\n",
    "        base_config=base_config,\n",
    "        space=BOOSTER_SEARCH_SPACE,\n",
    "        n_trials=n_trials,\n",
    "        include_current=include_current,\n",
    "        sample_without_replacement=sample_wo_repl,\n",
    "        rng=rng,\n",
    "    )\n",
    "\n",
    "    if not queue:\n",
    "        raise ValueError(\"No trials were generated. Check search space and settings.\")\n",
    "\n",
    "    print(\n",
    "        f\"Starting sweep: trials={len(queue)}, include_current={include_current}, \"\n",
    "        f\"without_replacement={sample_wo_repl}, space_size={_space_size(BOOSTER_SEARCH_SPACE):,}\"\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    sweep_start = time.time()\n",
    "    for idx, override_cfg in enumerate(queue, start=1):\n",
    "        run_name = f\"{run_prefix}_{idx:04d}\"\n",
    "        print(f\"\\n[{idx}/{len(queue)}] Running {run_name} ...\")\n",
    "\n",
    "        row = _run_one_trial(\n",
    "            base_config=base_config,\n",
    "            override_cfg=override_cfg,\n",
    "            run_name=run_name,\n",
    "            booster_code=booster_code,\n",
    "            stop_on_error=stop_on_error,\n",
    "        )\n",
    "        rows.append(row)\n",
    "\n",
    "        if row[\"status\"] == \"ok\":\n",
    "            print(\n",
    "                f\"{run_name}: booster={row['booster_score']:.1f}, ensemble={row['ensemble_score']:.1f}, \"\n",
    "                f\"gain={row['score_gain']:.1f}, dur={row['duration_sec']:.1f}s\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"{run_name}: FAILED -> {row['error'].split('|')[0]}\")\n",
    "\n",
    "    total_dur = time.time() - sweep_start\n",
    "    sweep_df = pd.DataFrame(rows)\n",
    "\n",
    "    SWEEP_TRIALS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if SWEEP_TRIALS_PATH.exists():\n",
    "        old = pd.read_csv(SWEEP_TRIALS_PATH)\n",
    "        sweep_df = pd.concat([old, sweep_df], ignore_index=True)\n",
    "\n",
    "    sweep_df = sweep_df.drop_duplicates(subset=[\"run_name\"], keep=\"last\")\n",
    "    sweep_df.to_csv(SWEEP_TRIALS_PATH, index=False)\n",
    "\n",
    "    globals()[\"booster_sweep_last_results_en\"] = sweep_df.copy()\n",
    "\n",
    "    # Restore baseline config to avoid accidental side-effects after sweep.\n",
    "    globals()[\"EXPERIMENT_CONFIG\"] = base_config\n",
    "\n",
    "    ok_df = sweep_df[sweep_df[\"status\"] == \"ok\"].copy()\n",
    "    if not ok_df.empty:\n",
    "        ok_df = ok_df.sort_values(\n",
    "            by=[\"booster_score\", \"score_gain\", \"seed_std_score\"],\n",
    "            ascending=[False, False, True],\n",
    "        )\n",
    "\n",
    "    print(\"\\nSweep finished.\")\n",
    "    print(f\"- total duration: {total_dur/60.0:.1f} min\")\n",
    "    print(f\"- successful trials: {int((sweep_df['status'] == 'ok').sum())}\")\n",
    "    print(f\"- failed trials: {int((sweep_df['status'] == 'failed').sum())}\")\n",
    "    print(f\"- trials csv: {SWEEP_TRIALS_PATH}\")\n",
    "\n",
    "    if not ok_df.empty:\n",
    "        print(\"\\nTop sweep trials:\")\n",
    "        print(\n",
    "            ok_df[\n",
    "                [\n",
    "                    \"run_name\",\n",
    "                    \"ensemble_score\",\n",
    "                    \"booster_score\",\n",
    "                    \"score_gain\",\n",
    "                    \"seed_std_score\",\n",
    "                    \"selected_profile\",\n",
    "                    \"feature_source\",\n",
    "                    \"duration_sec\",\n",
    "                ]\n",
    "            ].head(10).to_string(index=False)\n",
    "        )\n",
    "\n",
    "    if apply_best and SEARCH_BEST_PATH.exists():\n",
    "        best_payload = json.loads(SEARCH_BEST_PATH.read_text())\n",
    "        best_config = best_payload.get(\"best_config\", {})\n",
    "        if best_config:\n",
    "            globals()[\"EXPERIMENT_CONFIG\"] = best_config\n",
    "            print(\"\\nApplied best config from SEARCH_BEST_PATH into EXPERIMENT_CONFIG.\")\n",
    "\n",
    "    if plot_top_n > 0:\n",
    "        try:\n",
    "            plot_search_history(top_n=plot_top_n)\n",
    "        except Exception as exc:\n",
    "            print(f\"Could not render history plot: {exc}\")\n",
    "\n",
    "    return sweep_df\n",
    "\n",
    "\n",
    "# Example usage (manual):\n",
    "sweep_results = run_booster_sweep({\n",
    "    \"n_trials\": 20,\n",
    "    \"run_prefix\": \"week_sweep\",\n",
    "    \"include_current_config\": True,\n",
    "    \"sample_without_replacement\": True,\n",
    "    \"plot_top_n\": 25,\n",
    "    # Optional in Colab: set this if auto-detection does not find your notebook file.\n",
    "     # \"notebook_path\": \"/content/drive/MyDrive/bot_or_not_McHacks_2026/script_fr.ipynb\",\n",
    " })"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_botsornot (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
