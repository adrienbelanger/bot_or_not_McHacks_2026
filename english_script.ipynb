{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fa69427",
   "metadata": {},
   "source": [
    "# English Bot Detector — Clean Run\n",
    "\n",
    "Quick end-to-end notebook for competition submission.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25e59269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fa1b429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_COLAB=False\n",
      "USE_GOOGLE_DRIVE_DATA=True\n",
      "PROJECT_ROOT=.\n",
      "DATA_DIR=/Users/adrienbelanger/Library/GitHub/bot_or_not_McHacks_2026/data\n",
      "EXTERNAL_DATA_DIR=/Users/adrienbelanger/Library/GitHub/bot_or_not_McHacks_2026/external_data\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Toggle this in Colab when your datasets are stored on Google Drive.\n",
    "USE_GOOGLE_DRIVE_DATA = True\n",
    "\n",
    "# Adjust this only if your folder lives elsewhere in Drive.\n",
    "GOOGLE_DRIVE_PROJECT_ROOT = Path(\"/content/drive/MyDrive/bot_or_not_McHacks_2026\")\n",
    "LOCAL_PROJECT_ROOT = Path(\".\")\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "PROJECT_ROOT = LOCAL_PROJECT_ROOT\n",
    "\n",
    "if IN_COLAB and USE_GOOGLE_DRIVE_DATA:\n",
    "    from google.colab import drive\n",
    "    !pip install emoji==0.6.0\n",
    "    !pip install catboost\n",
    "\n",
    "    drive.mount(\"/content/drive\", force_remount=False)\n",
    "    PROJECT_ROOT = GOOGLE_DRIVE_PROJECT_ROOT\n",
    "elif IN_COLAB:\n",
    "    PROJECT_ROOT = LOCAL_PROJECT_ROOT\n",
    "\n",
    "DATA_DIR = (PROJECT_ROOT / \"data\").resolve()\n",
    "EXTERNAL_DATA_DIR = (PROJECT_ROOT / \"external_data\").resolve()\n",
    "\n",
    "print(f\"IN_COLAB={IN_COLAB}\")\n",
    "print(f\"USE_GOOGLE_DRIVE_DATA={USE_GOOGLE_DRIVE_DATA}\")\n",
    "print(f\"PROJECT_ROOT={PROJECT_ROOT}\")\n",
    "print(f\"DATA_DIR={DATA_DIR}\")\n",
    "print(f\"EXTERNAL_DATA_DIR={EXTERNAL_DATA_DIR}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c78ef1e",
   "metadata": {},
   "source": [
    "## 1. Load and Merge Competition Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "071ab039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DATA_DIR: /Users/adrienbelanger/Library/GitHub/bot_or_not_McHacks_2026/data\n",
      "Using EXTERNAL_DATA_DIR: /Users/adrienbelanger/Library/GitHub/bot_or_not_McHacks_2026/external_data\n",
      "EN sources: ['dataset.posts&users.30.json', 'dataset.posts&users.32.json']\n",
      "EN posts: 15,765 users: 546 bot_ids: 129\n",
      "FR sources: ['dataset.posts&users.31.json', 'dataset.posts&users.33.json']\n",
      "FR posts: 9,004 users: 343 bot_ids: 55\n"
     ]
    }
   ],
   "source": [
    "if \"DATA_DIR\" not in globals():\n",
    "    DATA_DIR = Path(\"data\").resolve()\n",
    "if \"EXTERNAL_DATA_DIR\" not in globals():\n",
    "    EXTERNAL_DATA_DIR = Path(\"external_data\").resolve()\n",
    "\n",
    "print(f\"Using DATA_DIR: {DATA_DIR}\")\n",
    "print(f\"Using EXTERNAL_DATA_DIR: {EXTERNAL_DATA_DIR}\")\n",
    "\n",
    "\n",
    "def get_version(path):\n",
    "    try:\n",
    "        return int(path.stem.split(\".\")[-1])\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "posts_users_files = sorted(DATA_DIR.glob(\"dataset.posts&users.*.json\"), key=get_version)\n",
    "if not posts_users_files:\n",
    "    raise FileNotFoundError(f\"No dataset.posts&users.*.json files found in {DATA_DIR}\")\n",
    "\n",
    "\n",
    "combined = {}\n",
    "bots_by_lang = {}\n",
    "\n",
    "\n",
    "for path in posts_users_files:\n",
    "    with path.open() as f:\n",
    "        data = json.load(f)\n",
    "    lang = data.get(\"lang\")\n",
    "\n",
    "    combined.setdefault(lang, {\"posts\": [], \"users\": [], \"sources\": []})\n",
    "    combined[lang][\"posts\"].extend(data.get(\"posts\", []))\n",
    "    combined[lang][\"users\"].extend(data.get(\"users\", []))\n",
    "    combined[lang][\"sources\"].append(path.name)\n",
    "\n",
    "    version = get_version(path)\n",
    "    if version is not None:\n",
    "        bots_path = DATA_DIR / f\"dataset.bots.{version}.txt\"\n",
    "        if bots_path.exists():\n",
    "            bots_by_lang.setdefault(lang, set()).update(bots_path.read_text().splitlines())\n",
    "\n",
    "\n",
    "posts_en = pd.DataFrame(combined.get(\"en\", {}).get(\"posts\", []))\n",
    "users_en = pd.DataFrame(combined.get(\"en\", {}).get(\"users\", []))\n",
    "bot_ids_en = bots_by_lang.get(\"en\", set())\n",
    "if not users_en.empty:\n",
    "    users_en[\"is_bot\"] = users_en[\"id\"].isin(bot_ids_en)\n",
    "\n",
    "posts_fr = pd.DataFrame(combined.get(\"fr\", {}).get(\"posts\", []))\n",
    "users_fr = pd.DataFrame(combined.get(\"fr\", {}).get(\"users\", []))\n",
    "bot_ids_fr = bots_by_lang.get(\"fr\", set())\n",
    "if not users_fr.empty:\n",
    "    users_fr[\"is_bot\"] = users_fr[\"id\"].isin(bot_ids_fr)\n",
    "\n",
    "print(\"EN sources:\", combined.get(\"en\", {}).get(\"sources\", []))\n",
    "print(f\"EN posts: {len(posts_en):,} users: {len(users_en):,} bot_ids: {len(bot_ids_en):,}\")\n",
    "print(\"FR sources:\", combined.get(\"fr\", {}).get(\"sources\", []))\n",
    "print(f\"FR posts: {len(posts_fr):,} users: {len(users_fr):,} bot_ids: {len(bot_ids_fr):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e34ec8c",
   "metadata": {},
   "source": [
    "## 2. Experiment Configuration (English)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "926640c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment config loaded:\n",
      "- tokenizer_name: vinai/bertweet-base\n",
      "- max_length: 96\n",
      "- dedupe_users: True\n",
      "- dedupe_posts: True\n",
      "- scale_meta_features: True\n",
      "- use_topic_features: True\n",
      "- topic_match_mode: word\n",
      "- test_size: 0.2\n",
      "- random_seed: 42\n",
      "- validation_split: 0.15\n",
      "- epochs: 8\n",
      "- batch_size: 150\n",
      "- learning_rate: 0.001\n",
      "- prediction_threshold: 0.62\n",
      "- use_threshold_search: True\n",
      "- threshold_search_min: 0.1\n",
      "- threshold_search_max: 0.9\n",
      "- threshold_search_steps: 161\n",
      "- account_decision_rule: mean\n",
      "- split_by_author: True\n",
      "- ensemble_seeds: [13, 29, 42, 73, 101]\n",
      "- ensemble_aggregation: mean\n",
      "- use_external_pretrain: False\n",
      "- external_pretrain_source: fox8\n",
      "- external_pretrain_max_users: 20000\n",
      "- external_pretrain_max_tweets_per_user: 5\n",
      "- external_pretrain_max_posts: 120000\n",
      "- external_pretrain_lang: en\n",
      "- external_pretrain_sample_seed: 42\n",
      "- external_pretrain_epochs: 1\n",
      "- external_pretrain_batch_size: 128\n",
      "- external_pretrain_use_balanced_weights: True\n",
      "- use_second_stage_account_model: True\n",
      "- second_stage_profile: legacy\n",
      "- second_stage_use_blend: True\n",
      "- second_stage_blend_alphas: [1.0]\n",
      "- second_stage_force_threshold: 0.145\n",
      "- second_stage_min_gain_vs_ensemble: 1\n",
      "- second_stage_use_oof_features: True\n",
      "- second_stage_oof_folds: 3\n",
      "- second_stage_oof_epochs: 4\n",
      "- second_stage_oof_batch_size: 128\n",
      "- second_stage_oof_seeds: [13, 42]\n",
      "- second_stage_oof_use_external_pretrain: False\n",
      "- second_stage_learning_rate: 0.05\n",
      "- second_stage_max_iter: 300\n",
      "- second_stage_max_depth: 4\n",
      "- second_stage_l2: 0.2\n",
      "- second_stage_min_data_in_leaf: 20\n",
      "- second_stage_subsample: 0.8\n",
      "- second_stage_rsm: 0.8\n",
      "- second_stage_od_wait: 50\n",
      "- second_stage_use_balanced_weights: True\n",
      "- use_class_weights: False\n",
      "- embedding_dim: 96\n",
      "- gru_units: 48\n",
      "- aux_dense_units: 32\n",
      "- head_dense_units: 48\n",
      "- dropout_text: 0.4\n",
      "- dropout_aux: 0.3\n",
      "- dropout_head: 0.4\n",
      "- early_stopping_patience: 1\n",
      "- reduce_lr_patience: 1\n",
      "- reduce_lr_factor: 0.5\n",
      "- reduce_lr_min_lr: 1e-05\n",
      "- error_analysis_min_topic_posts: 20\n",
      "- error_analysis_min_topic_accounts: 5\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_CONFIG = {\n",
    "    \"tokenizer_name\": \"vinai/bertweet-base\",\n",
    "    \"max_length\": 96,\n",
    "    \"dedupe_users\": True,\n",
    "    \"dedupe_posts\": True,\n",
    "    \"scale_meta_features\": True,\n",
    "    \"use_topic_features\": True,\n",
    "    \"topic_match_mode\": \"word\",  # options: \"contains\", \"word\"\n",
    "    \"test_size\": 0.20,\n",
    "    \"random_seed\": 42,\n",
    "    \"validation_split\": 0.15,\n",
    "    \"epochs\": 8,\n",
    "    \"batch_size\": 150,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"prediction_threshold\": 0.62,\n",
    "    \"use_threshold_search\": True,\n",
    "    \"threshold_search_min\": 0.10,\n",
    "    \"threshold_search_max\": 0.90,\n",
    "    \"threshold_search_steps\": 161,\n",
    "    \"account_decision_rule\": \"mean\",  # options: \"mean\", \"any\"\n",
    "    \"split_by_author\": True,\n",
    "    \"ensemble_seeds\": [13, 29, 42, 73, 101],\n",
    "    \"ensemble_aggregation\": \"mean\",  # options: \"mean\", \"median\"\n",
    "    \"use_external_pretrain\": False,\n",
    "    \"external_pretrain_source\": \"fox8\",\n",
    "    \"external_pretrain_max_users\": 20000,\n",
    "    \"external_pretrain_max_tweets_per_user\": 5,\n",
    "    \"external_pretrain_max_posts\": 120000,\n",
    "    \"external_pretrain_lang\": \"en\",\n",
    "    \"external_pretrain_sample_seed\": 42,\n",
    "    \"external_pretrain_epochs\": 1,\n",
    "    \"external_pretrain_batch_size\": 128,\n",
    "    \"external_pretrain_use_balanced_weights\": True,\n",
    "    \"use_second_stage_account_model\": True,\n",
    "    \"second_stage_profile\": \"legacy\",\n",
    "    \"second_stage_use_blend\": True,\n",
    "    \"second_stage_blend_alphas\": [1.0],\n",
    "    \"second_stage_force_threshold\": 0.145,\n",
    "    \"second_stage_min_gain_vs_ensemble\": 1,\n",
    "    \"second_stage_use_oof_features\": True,\n",
    "    \"second_stage_oof_folds\": 3,\n",
    "    \"second_stage_oof_epochs\": 4,\n",
    "    \"second_stage_oof_batch_size\": 128,\n",
    "    \"second_stage_oof_seeds\": [13, 42],\n",
    "    \"second_stage_oof_use_external_pretrain\": False,\n",
    "    \"second_stage_learning_rate\": 0.05,\n",
    "    \"second_stage_max_iter\": 300,\n",
    "    \"second_stage_max_depth\": 4,\n",
    "    \"second_stage_l2\": 0.2,\n",
    "    \"second_stage_min_data_in_leaf\": 20,\n",
    "    \"second_stage_subsample\": 0.8,\n",
    "    \"second_stage_rsm\": 0.8,\n",
    "    \"second_stage_od_wait\": 50,\n",
    "    \"second_stage_use_balanced_weights\": True,\n",
    "    \"use_class_weights\": False,\n",
    "    \"embedding_dim\": 96,\n",
    "    \"gru_units\": 48,\n",
    "    \"aux_dense_units\": 32,\n",
    "    \"head_dense_units\": 48,\n",
    "    \"dropout_text\": 0.40,\n",
    "    \"dropout_aux\": 0.30,\n",
    "    \"dropout_head\": 0.40,\n",
    "    \"early_stopping_patience\": 1,\n",
    "    \"reduce_lr_patience\": 1,\n",
    "    \"reduce_lr_factor\": 0.50,\n",
    "    \"reduce_lr_min_lr\": 1e-5,\n",
    "    \"error_analysis_min_topic_posts\": 20,\n",
    "    \"error_analysis_min_topic_accounts\": 5,\n",
    "}\n",
    "\n",
    "print(\"Experiment config loaded:\")\n",
    "for key, value in EXPERIMENT_CONFIG.items():\n",
    "    print(f\"- {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37411cfe",
   "metadata": {},
   "source": [
    "## 3. Tokenization and Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1288615",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: vinai/bertweet-base\n",
      "English labeled posts: 15,765\n",
      "Token tensor shape: (15765, 96)\n",
      "Meta feature shape: (15765, 7), label shape: (15765,)\n",
      "Dedupe users/posts: True/True\n",
      "Scale metadata features: True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "except ImportError as exc:\n",
    "    raise ImportError(\"Install transformers first: pip install transformers\") from exc\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "if \"EXPERIMENT_CONFIG\" not in globals():\n",
    "    raise ValueError(\"Run the Experiment parameters cell first.\")\n",
    "\n",
    "TOKENIZER_NAME = str(EXPERIMENT_CONFIG[\"tokenizer_name\"])\n",
    "MAX_LENGTH = int(EXPERIMENT_CONFIG[\"max_length\"])\n",
    "DEDUPE_USERS = bool(EXPERIMENT_CONFIG[\"dedupe_users\"])\n",
    "DEDUPE_POSTS = bool(EXPERIMENT_CONFIG[\"dedupe_posts\"])\n",
    "SCALE_META_FEATURES = bool(EXPERIMENT_CONFIG[\"scale_meta_features\"])\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.replace(\"\\n\", \" \").strip()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def add_text_features(df):\n",
    "    out = df.copy()\n",
    "    out[\"text_clean\"] = out[\"text\"].fillna(\"\").map(clean_text)\n",
    "    out[\"char_count\"] = out[\"text_clean\"].str.len()\n",
    "    out[\"word_count\"] = out[\"text_clean\"].str.split().str.len()\n",
    "    out[\"url_count\"] = out[\"text_clean\"].str.count(r\"https?://\\S+|www\\.\\S+\")\n",
    "    out[\"mention_count\"] = out[\"text_clean\"].str.count(r\"@\\w+\")\n",
    "    out[\"hashtag_count\"] = out[\"text_clean\"].str.count(r\"#\\w+\")\n",
    "    out[\"exclamation_count\"] = out[\"text_clean\"].str.count(r\"!\")\n",
    "    out[\"question_count\"] = out[\"text_clean\"].str.count(r\"\\?\")\n",
    "    return out\n",
    "\n",
    "if posts_en.empty or users_en.empty:\n",
    "    raise ValueError(\"Run the data processing cell first to load English data.\")\n",
    "\n",
    "users_en_labeled = (\n",
    "    users_en.drop_duplicates(subset=\"id\", keep=\"last\").copy()\n",
    "    if DEDUPE_USERS\n",
    "    else users_en.copy()\n",
    ")\n",
    "posts_en_unique = (\n",
    "    posts_en.drop_duplicates(subset=\"id\", keep=\"last\").copy()\n",
    "    if DEDUPE_POSTS\n",
    "    else posts_en.copy()\n",
    ")\n",
    "\n",
    "label_map_en = users_en_labeled.set_index(\"id\")[\"is_bot\"]\n",
    "train_en = posts_en_unique.copy()\n",
    "train_en[\"is_bot\"] = train_en[\"author_id\"].map(label_map_en)\n",
    "train_en = train_en.dropna(subset=[\"is_bot\"]).copy()\n",
    "train_en[\"is_bot\"] = train_en[\"is_bot\"].astype(\"int64\")\n",
    "\n",
    "train_en = add_text_features(train_en)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, use_fast=True)\n",
    "encodings_en = tokenizer(\n",
    "    train_en[\"text_clean\"].tolist(),\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=MAX_LENGTH,\n",
    "    return_attention_mask=True,\n",
    ")\n",
    "\n",
    "feature_cols_en = [\n",
    "    \"char_count\",\n",
    "    \"word_count\",\n",
    "    \"url_count\",\n",
    "    \"mention_count\",\n",
    "    \"hashtag_count\",\n",
    "    \"exclamation_count\",\n",
    "    \"question_count\",\n",
    "]\n",
    "X_meta_en = train_en[feature_cols_en].to_numpy(dtype=np.float32)\n",
    "y_en = train_en[\"is_bot\"].to_numpy(dtype=np.int64)\n",
    "\n",
    "if SCALE_META_FEATURES:\n",
    "    scaler_en = StandardScaler()\n",
    "    X_meta_en_scaled = scaler_en.fit_transform(X_meta_en).astype(np.float32)\n",
    "else:\n",
    "    scaler_en = None\n",
    "    X_meta_en_scaled = X_meta_en.copy()\n",
    "\n",
    "print(f\"Tokenizer: {TOKENIZER_NAME}\")\n",
    "print(f\"English labeled posts: {len(train_en):,}\")\n",
    "print(f\"Token tensor shape: {np.asarray(encodings_en['input_ids']).shape}\")\n",
    "print(f\"Meta feature shape: {X_meta_en_scaled.shape}, label shape: {y_en.shape}\")\n",
    "print(f\"Dedupe users/posts: {DEDUPE_USERS}/{DEDUPE_POSTS}\")\n",
    "print(f\"Scale metadata features: {SCALE_META_FEATURES}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9f8c2a",
   "metadata": {},
   "source": [
    "## 4. External Pretraining (Optional)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c8c2c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External pretraining disabled. Set use_external_pretrain=True to enable.\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if \"EXPERIMENT_CONFIG\" not in globals():\n",
    "    raise ValueError(\"Run the Experiment parameters cell first.\")\n",
    "if \"tokenizer\" not in globals() or \"add_text_features\" not in globals():\n",
    "    raise ValueError(\"Run the Tokenizing the text cell first.\")\n",
    "\n",
    "USE_EXTERNAL_PRETRAIN = bool(EXPERIMENT_CONFIG.get(\"use_external_pretrain\", False))\n",
    "if not USE_EXTERNAL_PRETRAIN:\n",
    "    print(\"External pretraining disabled. Set use_external_pretrain=True to enable.\")\n",
    "else:\n",
    "    external_source = str(EXPERIMENT_CONFIG.get(\"external_pretrain_source\", \"fox8\")).lower()\n",
    "    max_users = int(EXPERIMENT_CONFIG.get(\"external_pretrain_max_users\", 20000))\n",
    "    max_posts = int(EXPERIMENT_CONFIG.get(\"external_pretrain_max_posts\", 120000))\n",
    "    max_tweets_per_user = int(EXPERIMENT_CONFIG.get(\"external_pretrain_max_tweets_per_user\", 5))\n",
    "    lang_filter = str(EXPERIMENT_CONFIG.get(\"external_pretrain_lang\", \"en\")).lower().strip()\n",
    "    sample_seed = int(EXPERIMENT_CONFIG.get(\"external_pretrain_sample_seed\", 42))\n",
    "\n",
    "    random.seed(sample_seed)\n",
    "\n",
    "    data_dir = Path(globals().get(\"EXTERNAL_DATA_DIR\", Path(\"external_data\")))\n",
    "    if external_source != \"fox8\":\n",
    "        raise ValueError(\"Only fox8_23_dataset.ndjson is supported for pretraining right now.\")\n",
    "\n",
    "    fox8_path = data_dir / \"fox8_23_dataset.ndjson\"\n",
    "    if not fox8_path.exists():\n",
    "        raise FileNotFoundError(f\"Missing {fox8_path}\")\n",
    "\n",
    "    # Reservoir sample users to avoid loading the full file.\n",
    "    sampled_users = []\n",
    "    with fox8_path.open() as f:\n",
    "        for i, line in enumerate(f):\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "            if len(sampled_users) < max_users:\n",
    "                sampled_users.append(obj)\n",
    "            else:\n",
    "                j = random.randint(0, i)\n",
    "                if j < max_users:\n",
    "                    sampled_users[j] = obj\n",
    "\n",
    "    rows = []\n",
    "    for user in sampled_users:\n",
    "        label = str(user.get(\"label\", \"\")).strip().lower()\n",
    "        if not label:\n",
    "            continue\n",
    "        if label in {\"human\", \"normal\"}:\n",
    "            is_bot = 0\n",
    "        elif label in {\"bot\", \"spambot\", \"fake\", \"automated\"}:\n",
    "            is_bot = 1\n",
    "        else:\n",
    "            # Unknown label; skip\n",
    "            continue\n",
    "\n",
    "        user_id = user.get(\"user_id\")\n",
    "        tweets = user.get(\"user_tweets\", []) or []\n",
    "\n",
    "        per_user_count = 0\n",
    "        for tweet in tweets:\n",
    "            if len(rows) >= max_posts or per_user_count >= max_tweets_per_user:\n",
    "                break\n",
    "            text = tweet.get(\"text\") if isinstance(tweet, dict) else None\n",
    "            if not text:\n",
    "                continue\n",
    "            if lang_filter:\n",
    "                tweet_lang = str(tweet.get(\"lang\", \"\")).lower().strip() if isinstance(tweet, dict) else \"\"\n",
    "                if tweet_lang and tweet_lang != lang_filter:\n",
    "                    continue\n",
    "            rows.append(\n",
    "                {\n",
    "                    \"author_id\": f\"fox8_{user_id}\",\n",
    "                    \"text\": text,\n",
    "                    \"is_bot\": is_bot,\n",
    "                }\n",
    "            )\n",
    "            per_user_count += 1\n",
    "\n",
    "    external_pretrain_df = pd.DataFrame(rows)\n",
    "    if external_pretrain_df.empty:\n",
    "        raise ValueError(\"No external pretrain rows built. Check labels/lang filter.\")\n",
    "\n",
    "    external_pretrain_df = external_pretrain_df.dropna(subset=[\"text\"]).reset_index(drop=True)\n",
    "    external_pretrain_df = add_text_features(external_pretrain_df)\n",
    "\n",
    "    external_pretrain_encodings = tokenizer(\n",
    "        external_pretrain_df[\"text_clean\"].tolist(),\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "    external_pretrain_input_ids = np.asarray(external_pretrain_encodings[\"input_ids\"], dtype=np.int32)\n",
    "    external_pretrain_attention_mask = np.asarray(external_pretrain_encodings[\"attention_mask\"], dtype=np.float32)\n",
    "\n",
    "    external_pretrain_meta = external_pretrain_df[feature_cols_en].to_numpy(dtype=np.float32)\n",
    "    if \"scaler_en\" in globals() and SCALE_META_FEATURES:\n",
    "        external_pretrain_meta_scaled = scaler_en.transform(external_pretrain_meta).astype(np.float32)\n",
    "    else:\n",
    "        external_pretrain_meta_scaled = external_pretrain_meta\n",
    "\n",
    "    external_pretrain_labels = external_pretrain_df[\"is_bot\"].to_numpy(dtype=np.int64)\n",
    "\n",
    "    print(\"External pretrain dataset prepared:\")\n",
    "    print(f\"- source: {external_source}\")\n",
    "    print(f\"- rows: {len(external_pretrain_df):,}\")\n",
    "    print(\n",
    "        f\"- bots: {int((external_pretrain_labels==1).sum())}, humans: {int((external_pretrain_labels==0).sum())}\"\n",
    "    )\n",
    "    print(f\"- max_length: {MAX_LENGTH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251cded3",
   "metadata": {},
   "source": [
    "## 5. Train and Validate First Stage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8c62216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 160ms/step - accuracy: 0.6937 - auc: 0.6981 - loss: 0.5793 - val_accuracy: 0.8545 - val_auc: 0.8197 - val_loss: 0.4333 - learning_rate: 0.0010\n",
      "Epoch 2/8\n",
      "\u001b[1m69/69\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 207ms/step - accuracy: 0.8913 - auc: 0.9022 - loss: 0.3159 - val_accuracy: 0.8545 - val_auc: 0.8036 - val_loss: 0.4094 - learning_rate: 0.0010\n",
      "Split mode: author\n",
      "Topic features enabled: True\n",
      "Topic match mode: word\n",
      "Account decision rule: mean\n",
      "Topic columns: ['topic_pop', 'topic_nba', 'topic_movies', 'topic_nhl']\n",
      "Split sizes (fit/val/test posts): 10318/2041/3406\n",
      "Split sizes (fit/val/test accounts): 370/66/110\n",
      "Default threshold from config: 0.6200\n",
      "Selected threshold used on test: 0.3450\n",
      "Best validation score from threshold search: 45\n",
      "Test Accuracy (post-level): 0.8356\n",
      "Test ROC-AUC (post-level): 0.8596\n",
      "Test account score @ selected threshold -> score=67, TP=19, FN=7, FP=1, accounts=110\n",
      "Test account score @ config threshold -> score=19, TP=9, FN=17, FP=0, accounts=110\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8971    0.8755    0.8862      2490\n",
      "           1     0.6824    0.7271    0.7040       916\n",
      "\n",
      "    accuracy                         0.8356      3406\n",
      "   macro avg     0.7897    0.8013    0.7951      3406\n",
      "weighted avg     0.8394    0.8356    0.8372      3406\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except ImportError as exc:\n",
    "    raise ImportError(\"Install tensorflow first: pip install tensorflow\") from exc\n",
    "\n",
    "if \"EXPERIMENT_CONFIG\" not in globals():\n",
    "    raise ValueError(\"Run the Experiment parameters cell first.\")\n",
    "\n",
    "TEST_SIZE = float(EXPERIMENT_CONFIG[\"test_size\"])\n",
    "RANDOM_SEED = int(EXPERIMENT_CONFIG[\"random_seed\"])\n",
    "VALIDATION_SPLIT = float(EXPERIMENT_CONFIG[\"validation_split\"])\n",
    "EPOCHS = int(EXPERIMENT_CONFIG[\"epochs\"])\n",
    "BATCH_SIZE = int(EXPERIMENT_CONFIG[\"batch_size\"])\n",
    "LEARNING_RATE = float(EXPERIMENT_CONFIG[\"learning_rate\"])\n",
    "PREDICTION_THRESHOLD = float(EXPERIMENT_CONFIG[\"prediction_threshold\"])\n",
    "USE_CLASS_WEIGHTS = bool(EXPERIMENT_CONFIG[\"use_class_weights\"])\n",
    "USE_TOPIC_FEATURES = bool(EXPERIMENT_CONFIG[\"use_topic_features\"])\n",
    "TOPIC_MATCH_MODE = str(EXPERIMENT_CONFIG[\"topic_match_mode\"])\n",
    "SPLIT_BY_AUTHOR = bool(EXPERIMENT_CONFIG.get(\"split_by_author\", True))\n",
    "\n",
    "USE_THRESHOLD_SEARCH = bool(EXPERIMENT_CONFIG.get(\"use_threshold_search\", False))\n",
    "THRESHOLD_SEARCH_MIN = float(EXPERIMENT_CONFIG.get(\"threshold_search_min\", 0.10))\n",
    "THRESHOLD_SEARCH_MAX = float(EXPERIMENT_CONFIG.get(\"threshold_search_max\", 0.90))\n",
    "THRESHOLD_SEARCH_STEPS = int(EXPERIMENT_CONFIG.get(\"threshold_search_steps\", 81))\n",
    "ACCOUNT_DECISION_RULE = str(EXPERIMENT_CONFIG.get(\"account_decision_rule\", \"mean\"))\n",
    "\n",
    "EMBEDDING_DIM = int(EXPERIMENT_CONFIG[\"embedding_dim\"])\n",
    "GRU_UNITS = int(EXPERIMENT_CONFIG[\"gru_units\"])\n",
    "AUX_DENSE_UNITS = int(EXPERIMENT_CONFIG[\"aux_dense_units\"])\n",
    "HEAD_DENSE_UNITS = int(EXPERIMENT_CONFIG[\"head_dense_units\"])\n",
    "DROPOUT_TEXT = float(EXPERIMENT_CONFIG[\"dropout_text\"])\n",
    "DROPOUT_AUX = float(EXPERIMENT_CONFIG[\"dropout_aux\"])\n",
    "DROPOUT_HEAD = float(EXPERIMENT_CONFIG[\"dropout_head\"])\n",
    "\n",
    "EARLY_STOPPING_PATIENCE = int(EXPERIMENT_CONFIG[\"early_stopping_patience\"])\n",
    "REDUCE_LR_PATIENCE = int(EXPERIMENT_CONFIG[\"reduce_lr_patience\"])\n",
    "REDUCE_LR_FACTOR = float(EXPERIMENT_CONFIG[\"reduce_lr_factor\"])\n",
    "REDUCE_LR_MIN_LR = float(EXPERIMENT_CONFIG[\"reduce_lr_min_lr\"])\n",
    "\n",
    "if not (0.0 < TEST_SIZE < 1.0):\n",
    "    raise ValueError(\"test_size must be between 0 and 1.\")\n",
    "if not (0.0 <= VALIDATION_SPLIT < 1.0):\n",
    "    raise ValueError(\"validation_split must be in [0, 1).\")\n",
    "if TOPIC_MATCH_MODE not in {\"contains\", \"word\"}:\n",
    "    raise ValueError('topic_match_mode must be \"contains\" or \"word\".')\n",
    "if ACCOUNT_DECISION_RULE not in {\"mean\", \"any\"}:\n",
    "    raise ValueError('account_decision_rule must be \"mean\" or \"any\".')\n",
    "if USE_THRESHOLD_SEARCH:\n",
    "    if not (0.0 < THRESHOLD_SEARCH_MIN < 1.0 and 0.0 < THRESHOLD_SEARCH_MAX < 1.0):\n",
    "        raise ValueError(\"threshold_search_min and threshold_search_max must be in (0, 1).\")\n",
    "    if THRESHOLD_SEARCH_MIN >= THRESHOLD_SEARCH_MAX:\n",
    "        raise ValueError(\"threshold_search_min must be smaller than threshold_search_max.\")\n",
    "    if THRESHOLD_SEARCH_STEPS < 2:\n",
    "        raise ValueError(\"threshold_search_steps must be >= 2.\")\n",
    "\n",
    "\n",
    "def load_english_topic_keywords():\n",
    "    topic_keywords = {}\n",
    "    for source_name in combined.get(\"en\", {}).get(\"sources\", []):\n",
    "        source_path = DATA_DIR / source_name\n",
    "        with source_path.open() as f:\n",
    "            payload = json.load(f)\n",
    "        for topic_item in payload.get(\"metadata\", {}).get(\"topics\", []):\n",
    "            topic = str(topic_item.get(\"topic\", \"\")).strip().lower()\n",
    "            if not topic:\n",
    "                continue\n",
    "            keywords = {\n",
    "                str(keyword).strip().lower()\n",
    "                for keyword in topic_item.get(\"keywords\", [])\n",
    "                if str(keyword).strip()\n",
    "            }\n",
    "            keywords.add(topic)\n",
    "            topic_keywords.setdefault(topic, set()).update(keywords)\n",
    "    return {topic: sorted(values, key=len, reverse=True) for topic, values in topic_keywords.items()}\n",
    "\n",
    "\n",
    "def add_topic_features(df, topic_keywords, match_mode):\n",
    "    out = df.copy()\n",
    "    text_lower = out[\"text_clean\"].str.lower()\n",
    "    topic_cols = []\n",
    "    for topic, keywords in topic_keywords.items():\n",
    "        col = f\"topic_{topic}\"\n",
    "        topic_cols.append(col)\n",
    "        if not keywords:\n",
    "            out[col] = 0\n",
    "            continue\n",
    "        if match_mode == \"word\":\n",
    "            pattern = \"|\".join(rf\"\\\\b{re.escape(keyword)}\\\\b\" for keyword in keywords)\n",
    "        else:\n",
    "            pattern = \"|\".join(re.escape(keyword) for keyword in keywords)\n",
    "        out[col] = text_lower.str.contains(pattern, regex=True).astype(np.int8)\n",
    "    return out, topic_cols\n",
    "\n",
    "\n",
    "def compute_account_score(author_ids, true_labels, pred_probs, threshold, decision_rule):\n",
    "    tmp = pd.DataFrame(\n",
    "        {\n",
    "            \"author_id\": author_ids,\n",
    "            \"true_is_bot\": np.asarray(true_labels, dtype=np.int64),\n",
    "            \"pred_prob\": np.asarray(pred_probs, dtype=np.float32),\n",
    "        }\n",
    "    )\n",
    "    tmp[\"pred_post\"] = (tmp[\"pred_prob\"] >= threshold).astype(np.int64)\n",
    "\n",
    "    account = (\n",
    "        tmp.groupby(\"author_id\", as_index=False)\n",
    "        .agg(\n",
    "            true_is_bot=(\"true_is_bot\", \"max\"),\n",
    "            mean_prob=(\"pred_prob\", \"mean\"),\n",
    "            any_pred=(\"pred_post\", \"max\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if decision_rule == \"any\":\n",
    "        account[\"pred_is_bot\"] = account[\"any_pred\"].astype(np.int64)\n",
    "    else:\n",
    "        account[\"pred_is_bot\"] = (account[\"mean_prob\"] >= threshold).astype(np.int64)\n",
    "\n",
    "    tp_accounts = int(((account[\"true_is_bot\"] == 1) & (account[\"pred_is_bot\"] == 1)).sum())\n",
    "    fn_accounts = int(((account[\"true_is_bot\"] == 1) & (account[\"pred_is_bot\"] == 0)).sum())\n",
    "    fp_accounts = int(((account[\"true_is_bot\"] == 0) & (account[\"pred_is_bot\"] == 1)).sum())\n",
    "\n",
    "    score = (4 * tp_accounts) - (1 * fn_accounts) - (2 * fp_accounts)\n",
    "    return score, tp_accounts, fn_accounts, fp_accounts, len(account)\n",
    "\n",
    "\n",
    "def predict_for_indices(model, post_indices):\n",
    "    if len(post_indices) == 0:\n",
    "        return np.array([], dtype=np.float32)\n",
    "    return model.predict(\n",
    "        {\n",
    "            \"input_ids\": input_ids_en[post_indices],\n",
    "            \"attention_mask\": attention_mask_en[post_indices],\n",
    "            \"aux_features\": X_aux_en[post_indices],\n",
    "        },\n",
    "        verbose=0,\n",
    "    ).ravel()\n",
    "\n",
    "\n",
    "if USE_TOPIC_FEATURES:\n",
    "    topic_keywords_en = load_english_topic_keywords()\n",
    "    train_en_model, topic_feature_cols_en = add_topic_features(train_en, topic_keywords_en, TOPIC_MATCH_MODE)\n",
    "else:\n",
    "    train_en_model = train_en.copy()\n",
    "    topic_feature_cols_en = []\n",
    "\n",
    "input_ids_en = np.asarray(encodings_en[\"input_ids\"], dtype=np.int32)\n",
    "attention_mask_en = np.asarray(encodings_en[\"attention_mask\"], dtype=np.float32)\n",
    "X_topic_en = (\n",
    "    train_en_model[topic_feature_cols_en].to_numpy(dtype=np.float32)\n",
    "    if topic_feature_cols_en\n",
    "    else np.zeros((len(train_en_model), 0), dtype=np.float32)\n",
    ")\n",
    "X_aux_en = np.concatenate([X_meta_en_scaled, X_topic_en], axis=1)\n",
    "\n",
    "all_post_idx = np.arange(len(y_en))\n",
    "\n",
    "if SPLIT_BY_AUTHOR:\n",
    "    author_labels_df = (\n",
    "        train_en_model.groupby(\"author_id\", as_index=False)[\"is_bot\"]\n",
    "        .max()\n",
    "        .rename(columns={\"is_bot\": \"account_is_bot\"})\n",
    "    )\n",
    "    all_authors = author_labels_df[\"author_id\"].to_numpy()\n",
    "    all_author_labels = author_labels_df[\"account_is_bot\"].to_numpy(dtype=np.int64)\n",
    "\n",
    "    train_authors, test_authors = train_test_split(\n",
    "        all_authors,\n",
    "        test_size=TEST_SIZE,\n",
    "        random_state=RANDOM_SEED,\n",
    "        stratify=all_author_labels,\n",
    "    )\n",
    "\n",
    "    if VALIDATION_SPLIT > 0:\n",
    "        train_author_labels = (\n",
    "            author_labels_df.set_index(\"author_id\").loc[train_authors, \"account_is_bot\"].to_numpy(dtype=np.int64)\n",
    "        )\n",
    "        fit_authors, val_authors = train_test_split(\n",
    "            train_authors,\n",
    "            test_size=VALIDATION_SPLIT,\n",
    "            random_state=RANDOM_SEED,\n",
    "            stratify=train_author_labels,\n",
    "        )\n",
    "    else:\n",
    "        fit_authors = train_authors\n",
    "        val_authors = np.array([], dtype=all_authors.dtype)\n",
    "\n",
    "    fit_author_set = set(fit_authors.tolist())\n",
    "    val_author_set = set(val_authors.tolist())\n",
    "    test_author_set = set(test_authors.tolist())\n",
    "\n",
    "    fit_idx = np.flatnonzero(train_en_model[\"author_id\"].isin(fit_author_set).to_numpy())\n",
    "    val_idx = np.flatnonzero(train_en_model[\"author_id\"].isin(val_author_set).to_numpy())\n",
    "    test_idx = np.flatnonzero(train_en_model[\"author_id\"].isin(test_author_set).to_numpy())\n",
    "    split_mode = \"author\"\n",
    "else:\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        all_post_idx,\n",
    "        test_size=TEST_SIZE,\n",
    "        random_state=RANDOM_SEED,\n",
    "        stratify=y_en,\n",
    "    )\n",
    "\n",
    "    if VALIDATION_SPLIT > 0:\n",
    "        fit_idx, val_idx = train_test_split(\n",
    "            train_idx,\n",
    "            test_size=VALIDATION_SPLIT,\n",
    "            random_state=RANDOM_SEED,\n",
    "            stratify=y_en[train_idx],\n",
    "        )\n",
    "    else:\n",
    "        fit_idx = train_idx\n",
    "        val_idx = np.array([], dtype=np.int64)\n",
    "\n",
    "    split_mode = \"post\"\n",
    "\n",
    "X_fit_ids, X_test_ids = input_ids_en[fit_idx], input_ids_en[test_idx]\n",
    "X_fit_mask, X_test_mask = attention_mask_en[fit_idx], attention_mask_en[test_idx]\n",
    "X_fit_aux, X_test_aux = X_aux_en[fit_idx], X_aux_en[test_idx]\n",
    "y_fit, y_test = y_en[fit_idx], y_en[test_idx]\n",
    "\n",
    "X_val_ids = input_ids_en[val_idx] if len(val_idx) else None\n",
    "X_val_mask = attention_mask_en[val_idx] if len(val_idx) else None\n",
    "X_val_aux = X_aux_en[val_idx] if len(val_idx) else None\n",
    "y_val = y_en[val_idx] if len(val_idx) else None\n",
    "\n",
    "fit_author_ids = np.unique(train_en_model.iloc[fit_idx][\"author_id\"].to_numpy())\n",
    "val_author_ids = np.unique(train_en_model.iloc[val_idx][\"author_id\"].to_numpy()) if len(val_idx) else np.array([])\n",
    "test_author_ids = np.unique(train_en_model.iloc[test_idx][\"author_id\"].to_numpy())\n",
    "\n",
    "class_weight_dict = None\n",
    "if USE_CLASS_WEIGHTS:\n",
    "    classes = np.unique(y_fit)\n",
    "    class_weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_fit)\n",
    "    class_weight_dict = {int(c): float(w) for c, w in zip(classes, class_weights)}\n",
    "\n",
    "\n",
    "def build_multifeature_model(\n",
    "    vocab_size,\n",
    "    seq_len,\n",
    "    aux_dim,\n",
    "    embedding_dim,\n",
    "    gru_units,\n",
    "    aux_dense_units,\n",
    "    head_dense_units,\n",
    "    dropout_text,\n",
    "    dropout_aux,\n",
    "    dropout_head,\n",
    "    learning_rate,\n",
    "):\n",
    "    ids_input = tf.keras.layers.Input(shape=(seq_len,), dtype=\"int32\", name=\"input_ids\")\n",
    "    mask_input = tf.keras.layers.Input(shape=(seq_len,), dtype=\"float32\", name=\"attention_mask\")\n",
    "    aux_input = tf.keras.layers.Input(shape=(aux_dim,), dtype=\"float32\", name=\"aux_features\")\n",
    "\n",
    "    x = tf.keras.layers.Embedding(vocab_size, embedding_dim, name=\"token_embedding\")(ids_input)\n",
    "    mask = tf.keras.layers.Reshape((seq_len, 1))(mask_input)\n",
    "    x = tf.keras.layers.Multiply()([x, mask])\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(gru_units))(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_text)(x)\n",
    "\n",
    "    aux = tf.keras.layers.Dense(aux_dense_units, activation=\"relu\")(aux_input)\n",
    "    aux = tf.keras.layers.Dropout(dropout_aux)(aux)\n",
    "\n",
    "    merged = tf.keras.layers.Concatenate()([x, aux])\n",
    "    merged = tf.keras.layers.Dense(head_dense_units, activation=\"relu\")(merged)\n",
    "    merged = tf.keras.layers.Dropout(dropout_head)(merged)\n",
    "    output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(merged)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[ids_input, mask_input, aux_input], outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy(name=\"accuracy\"), tf.keras.metrics.AUC(name=\"auc\")],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model_en = build_multifeature_model(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    seq_len=MAX_LENGTH,\n",
    "    aux_dim=X_fit_aux.shape[1],\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    gru_units=GRU_UNITS,\n",
    "    aux_dense_units=AUX_DENSE_UNITS,\n",
    "    head_dense_units=HEAD_DENSE_UNITS,\n",
    "    dropout_text=DROPOUT_TEXT,\n",
    "    dropout_aux=DROPOUT_AUX,\n",
    "    dropout_head=DROPOUT_HEAD,\n",
    "    learning_rate=LEARNING_RATE,\n",
    ")\n",
    "\n",
    "has_validation = len(val_idx) > 0\n",
    "callbacks = []\n",
    "if has_validation and EARLY_STOPPING_PATIENCE > 0:\n",
    "    callbacks.append(\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_auc\",\n",
    "            mode=\"max\",\n",
    "            patience=EARLY_STOPPING_PATIENCE,\n",
    "            restore_best_weights=True,\n",
    "        )\n",
    "    )\n",
    "if has_validation and REDUCE_LR_PATIENCE > 0:\n",
    "    callbacks.append(\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_auc\",\n",
    "            mode=\"max\",\n",
    "            factor=REDUCE_LR_FACTOR,\n",
    "            patience=REDUCE_LR_PATIENCE,\n",
    "            min_lr=REDUCE_LR_MIN_LR,\n",
    "        )\n",
    "    )\n",
    "\n",
    "train_inputs = {\n",
    "    \"input_ids\": X_fit_ids,\n",
    "    \"attention_mask\": X_fit_mask,\n",
    "    \"aux_features\": X_fit_aux,\n",
    "}\n",
    "\n",
    "fit_kwargs = {\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"class_weight\": class_weight_dict,\n",
    "    \"callbacks\": callbacks,\n",
    "    \"verbose\": 1,\n",
    "}\n",
    "if has_validation:\n",
    "    fit_kwargs[\"validation_data\"] = (\n",
    "        {\n",
    "            \"input_ids\": X_val_ids,\n",
    "            \"attention_mask\": X_val_mask,\n",
    "            \"aux_features\": X_val_aux,\n",
    "        },\n",
    "        y_val,\n",
    "    )\n",
    "\n",
    "history_en = model_en.fit(train_inputs, y_fit, **fit_kwargs)\n",
    "\n",
    "post_prob_fit = predict_for_indices(model_en, fit_idx)\n",
    "post_prob_val = predict_for_indices(model_en, val_idx)\n",
    "post_prob_test = predict_for_indices(model_en, test_idx)\n",
    "\n",
    "y_prob = post_prob_test.copy()\n",
    "threshold_search_results_en = pd.DataFrame()\n",
    "SELECTED_THRESHOLD = PREDICTION_THRESHOLD\n",
    "best_threshold_val_score = None\n",
    "\n",
    "if has_validation and USE_THRESHOLD_SEARCH:\n",
    "    val_author_ids_for_score = train_en_model.iloc[val_idx][\"author_id\"].to_numpy()\n",
    "    search_rows = []\n",
    "    for threshold in np.linspace(THRESHOLD_SEARCH_MIN, THRESHOLD_SEARCH_MAX, THRESHOLD_SEARCH_STEPS):\n",
    "        score, tp_acc, fn_acc, fp_acc, n_accounts = compute_account_score(\n",
    "            author_ids=val_author_ids_for_score,\n",
    "            true_labels=y_val,\n",
    "            pred_probs=post_prob_val,\n",
    "            threshold=float(threshold),\n",
    "            decision_rule=ACCOUNT_DECISION_RULE,\n",
    "        )\n",
    "        search_rows.append(\n",
    "            {\n",
    "                \"threshold\": float(threshold),\n",
    "                \"score\": int(score),\n",
    "                \"tp_accounts\": int(tp_acc),\n",
    "                \"fn_accounts\": int(fn_acc),\n",
    "                \"fp_accounts\": int(fp_acc),\n",
    "                \"n_accounts\": int(n_accounts),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    threshold_search_results_en = pd.DataFrame(search_rows)\n",
    "    best_row = threshold_search_results_en.sort_values(\n",
    "        by=[\"score\", \"fp_accounts\", \"tp_accounts\", \"threshold\"],\n",
    "        ascending=[False, True, False, False],\n",
    "    ).iloc[0]\n",
    "\n",
    "    SELECTED_THRESHOLD = float(best_row[\"threshold\"])\n",
    "    best_threshold_val_score = int(best_row[\"score\"])\n",
    "\n",
    "y_pred = (y_prob >= SELECTED_THRESHOLD).astype(np.int64)\n",
    "\n",
    "test_author_ids_for_score = train_en_model.iloc[test_idx][\"author_id\"].to_numpy()\n",
    "(\n",
    "    test_score,\n",
    "    test_tp_accounts,\n",
    "    test_fn_accounts,\n",
    "    test_fp_accounts,\n",
    "    test_n_accounts,\n",
    ") = compute_account_score(\n",
    "    author_ids=test_author_ids_for_score,\n",
    "    true_labels=y_test,\n",
    "    pred_probs=y_prob,\n",
    "    threshold=SELECTED_THRESHOLD,\n",
    "    decision_rule=ACCOUNT_DECISION_RULE,\n",
    ")\n",
    "\n",
    "(\n",
    "    baseline_test_score,\n",
    "    baseline_tp_accounts,\n",
    "    baseline_fn_accounts,\n",
    "    baseline_fp_accounts,\n",
    "    baseline_n_accounts,\n",
    ") = compute_account_score(\n",
    "    author_ids=test_author_ids_for_score,\n",
    "    true_labels=y_test,\n",
    "    pred_probs=y_prob,\n",
    "    threshold=PREDICTION_THRESHOLD,\n",
    "    decision_rule=ACCOUNT_DECISION_RULE,\n",
    ")\n",
    "\n",
    "print(f\"Split mode: {split_mode}\")\n",
    "print(f\"Topic features enabled: {USE_TOPIC_FEATURES}\")\n",
    "print(f\"Topic match mode: {TOPIC_MATCH_MODE}\")\n",
    "print(f\"Account decision rule: {ACCOUNT_DECISION_RULE}\")\n",
    "print(\"Topic columns:\", topic_feature_cols_en)\n",
    "print(f\"Split sizes (fit/val/test posts): {len(fit_idx)}/{len(val_idx)}/{len(test_idx)}\")\n",
    "print(\n",
    "    f\"Split sizes (fit/val/test accounts): {len(fit_author_ids)}/{len(val_author_ids)}/{len(test_author_ids)}\"\n",
    ")\n",
    "print(f\"Default threshold from config: {PREDICTION_THRESHOLD:.4f}\")\n",
    "print(f\"Selected threshold used on test: {SELECTED_THRESHOLD:.4f}\")\n",
    "if best_threshold_val_score is not None:\n",
    "    print(f\"Best validation score from threshold search: {best_threshold_val_score}\")\n",
    "print(f\"Test Accuracy (post-level): {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Test ROC-AUC (post-level): {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "print(\n",
    "    f\"Test account score @ selected threshold -> score={test_score}, TP={test_tp_accounts}, FN={test_fn_accounts}, FP={test_fp_accounts}, accounts={test_n_accounts}\"\n",
    ")\n",
    "print(\n",
    "    f\"Test account score @ config threshold -> score={baseline_test_score}, TP={baseline_tp_accounts}, FN={baseline_fn_accounts}, FP={baseline_fp_accounts}, accounts={baseline_n_accounts}\"\n",
    ")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a6a2a8",
   "metadata": {},
   "source": [
    "## 6. Account-Level Score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e84ff0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold used: 0.3450\n",
      "Account decision rule: mean\n",
      "Accounts scored (test split): 110\n",
      "TP: 19  FN: 7  FP: 1  TN: 83\n",
      "Bot detector score: 67\n",
      "Score out of max possible: 67/104 (64.42%)\n",
      "Score range on this split: [-194, 104]\n",
      "Range-normalized score: 87.58%\n"
     ]
    }
   ],
   "source": [
    "if any(name not in globals() for name in [\"train_en_model\", \"test_idx\", \"y_prob\", \"PREDICTION_THRESHOLD\"]):\n",
    "    raise ValueError(\"Run the Train-Test split for model cell first.\")\n",
    "\n",
    "threshold_for_score = float(globals().get(\"SELECTED_THRESHOLD\", PREDICTION_THRESHOLD))\n",
    "if \"ACCOUNT_DECISION_RULE\" in globals():\n",
    "    account_decision_rule = str(ACCOUNT_DECISION_RULE)\n",
    "elif \"EXPERIMENT_CONFIG\" in globals():\n",
    "    account_decision_rule = str(EXPERIMENT_CONFIG.get(\"account_decision_rule\", \"mean\"))\n",
    "else:\n",
    "    account_decision_rule = \"mean\"\n",
    "\n",
    "if account_decision_rule not in {\"mean\", \"any\"}:\n",
    "    raise ValueError('account_decision_rule must be \"mean\" or \"any\".')\n",
    "\n",
    "# Build test-set account labels/predictions from post-level outputs.\n",
    "score_df = train_en_model.iloc[test_idx][[\"author_id\", \"is_bot\"]].copy()\n",
    "score_df[\"pred_prob\"] = y_prob\n",
    "score_df[\"pred_post\"] = (score_df[\"pred_prob\"] >= threshold_for_score).astype(np.int64)\n",
    "\n",
    "account_df = (\n",
    "    score_df.groupby(\"author_id\", as_index=False)\n",
    "    .agg(\n",
    "        true_is_bot=(\"is_bot\", \"max\"),\n",
    "        mean_prob=(\"pred_prob\", \"mean\"),\n",
    "        any_pred=(\"pred_post\", \"max\"),\n",
    "        n_posts=(\"pred_post\", \"size\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "if account_decision_rule == \"any\":\n",
    "    account_df[\"pred_is_bot\"] = account_df[\"any_pred\"].astype(np.int64)\n",
    "else:\n",
    "    account_df[\"pred_is_bot\"] = (account_df[\"mean_prob\"] >= threshold_for_score).astype(np.int64)\n",
    "\n",
    "tp_accounts = int(((account_df[\"true_is_bot\"] == 1) & (account_df[\"pred_is_bot\"] == 1)).sum())\n",
    "fn_accounts = int(((account_df[\"true_is_bot\"] == 1) & (account_df[\"pred_is_bot\"] == 0)).sum())\n",
    "fp_accounts = int(((account_df[\"true_is_bot\"] == 0) & (account_df[\"pred_is_bot\"] == 1)).sum())\n",
    "tn_accounts = int(((account_df[\"true_is_bot\"] == 0) & (account_df[\"pred_is_bot\"] == 0)).sum())\n",
    "\n",
    "score = (4 * tp_accounts) - (1 * fn_accounts) - (2 * fp_accounts)\n",
    "max_possible_score = 4 * int((account_df[\"true_is_bot\"] == 1).sum())\n",
    "min_possible_score = (\n",
    "    -1 * int((account_df[\"true_is_bot\"] == 1).sum())\n",
    "    -2 * int((account_df[\"true_is_bot\"] == 0).sum())\n",
    ")\n",
    "\n",
    "score_ratio = score / max_possible_score if max_possible_score > 0 else np.nan\n",
    "score_normalized = (\n",
    "    (score - min_possible_score) / (max_possible_score - min_possible_score)\n",
    "    if max_possible_score != min_possible_score\n",
    "    else np.nan\n",
    ")\n",
    "\n",
    "print(f\"Threshold used: {threshold_for_score:.4f}\")\n",
    "print(f\"Account decision rule: {account_decision_rule}\")\n",
    "print(f\"Accounts scored (test split): {len(account_df)}\")\n",
    "print(f\"TP: {tp_accounts}  FN: {fn_accounts}  FP: {fp_accounts}  TN: {tn_accounts}\")\n",
    "print(f\"Bot detector score: {score}\")\n",
    "print(f\"Score out of max possible: {score}/{max_possible_score} ({score_ratio:.2%})\")\n",
    "print(f\"Score range on this split: [{min_possible_score}, {max_possible_score}]\")\n",
    "print(f\"Range-normalized score: {score_normalized:.2%}\")\n",
    "\n",
    "if \"second_stage_test_score_en\" in globals():\n",
    "    print(\n",
    "        f\"Second-stage account-model score (recommended): {second_stage_test_score_en} at threshold {second_stage_selected_threshold_en:.4f}\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2db3f5",
   "metadata": {},
   "source": [
    "## 7. Booster (Second Stage)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1c7e2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-14 12:06:54.611193: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-14 12:07:07.675096: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-14 12:07:26.024694: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-14 12:07:31.263957: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-14 12:07:49.304388: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-14 12:08:15.385013: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-14 12:08:20.958074: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-14 12:08:39.956832: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-14 12:08:51.574284: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-14 12:08:57.481956: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-14 12:09:16.587845: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-14 12:09:32.389197: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-14 12:09:38.311065: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-14 12:09:55.197893: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-14 12:10:14.881870: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-14 12:10:20.043462: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building OOF first-stage features for second-stage training (seeds=[13, 42], folds=3, epochs=4)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-14 12:10:36.765177: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-14 12:10:56.998372: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-14 12:11:17.902597: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-14 12:11:40.143686: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-14 12:11:55.283019: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-14 12:12:18.519825: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-14 12:12:31.464590: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-14 12:12:53.850268: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-14 12:13:06.708931: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-14 12:13:26.633691: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-14 12:13:39.786894: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-14 12:14:00.648843: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Second-stage threshold overridden to 0.1450 from EXPERIMENT_CONFIG.\n",
      "Seed-level first-stage account scores (test):\n",
      " seed  threshold  test_score  tp_accounts  fn_accounts  fp_accounts\n",
      "   13      0.345          70           20            6            2\n",
      "   29      0.255          65           23            3           12\n",
      "   42      0.440          54           16           10            0\n",
      "   73      0.590          49           15           11            0\n",
      "  101      0.510          49           15           11            0\n",
      "Seed score mean/std: 57.40 / 9.61\n",
      "Ensemble aggregation: mean\n",
      "Ensemble selected threshold: 0.3650\n",
      "Ensemble test score: 72 (TP=20, FN=6, FP=1, accounts=110)\n",
      "Second-stage fit feature source: oof_first_stage\n",
      "OOF seed report for second-stage fit features:\n",
      " seed  fit_account_score_at_ensemble_threshold  tp_accounts  fn_accounts  fp_accounts  n_accounts\n",
      "   13                                      201           72           15           36         370\n",
      "   42                                      217           72           15           28         370\n",
      "Second-stage candidate report:\n",
      "profile  alpha  threshold  val_score  val_tp_accounts  val_fn_accounts  val_fp_accounts  test_score  test_tp_accounts  test_fn_accounts  test_fp_accounts\n",
      " legacy    1.0       0.11         59               15                1                0         100                26                 0                 2\n",
      "Baseline account-level validation score (from first-stage means): 45 (TP=13, FN=3, FP=2, threshold=0.3650)\n",
      "Second-stage profile mode: legacy\n",
      "Second-stage selected profile: legacy\n",
      "Second-stage blend alpha (CatBoost weight): 1.00\n",
      "Second-stage threshold: 0.1450\n",
      "Second-stage test score: 102/104 (98.08%)\n",
      "Second-stage confusion components -> TP=26, FN=0, FP=1\n",
      "Second-stage precision=0.9630, recall=1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9881    0.9940        84\n",
      "           1     0.9630    1.0000    0.9811        26\n",
      "\n",
      "    accuracy                         0.9909       110\n",
      "   macro avg     0.9815    0.9940    0.9876       110\n",
      "weighted avg     0.9912    0.9909    0.9910       110\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "except ImportError as exc:\n",
    "    raise ImportError(\"Install catboost first: pip install catboost\") from exc\n",
    "\n",
    "required_globals = [\n",
    "    \"tf\",\n",
    "    \"EXPERIMENT_CONFIG\",\n",
    "    \"build_multifeature_model\",\n",
    "    \"compute_account_score\",\n",
    "    \"train_en_model\",\n",
    "    \"topic_feature_cols_en\",\n",
    "    \"fit_idx\",\n",
    "    \"val_idx\",\n",
    "    \"test_idx\",\n",
    "    \"input_ids_en\",\n",
    "    \"attention_mask_en\",\n",
    "    \"X_aux_en\",\n",
    "    \"y_en\",\n",
    "    \"MAX_LENGTH\",\n",
    "    \"tokenizer\",\n",
    "    \"users_en\",\n",
    "    \"EMBEDDING_DIM\",\n",
    "    \"GRU_UNITS\",\n",
    "    \"AUX_DENSE_UNITS\",\n",
    "    \"HEAD_DENSE_UNITS\",\n",
    "    \"DROPOUT_TEXT\",\n",
    "    \"DROPOUT_AUX\",\n",
    "    \"DROPOUT_HEAD\",\n",
    "    \"LEARNING_RATE\",\n",
    "    \"EPOCHS\",\n",
    "    \"BATCH_SIZE\",\n",
    "    \"RANDOM_SEED\",\n",
    "    \"USE_CLASS_WEIGHTS\",\n",
    "    \"class_weight_dict\",\n",
    "    \"USE_THRESHOLD_SEARCH\",\n",
    "    \"THRESHOLD_SEARCH_MIN\",\n",
    "    \"THRESHOLD_SEARCH_MAX\",\n",
    "    \"THRESHOLD_SEARCH_STEPS\",\n",
    "    \"PREDICTION_THRESHOLD\",\n",
    "    \"ACCOUNT_DECISION_RULE\",\n",
    "    \"EARLY_STOPPING_PATIENCE\",\n",
    "    \"REDUCE_LR_PATIENCE\",\n",
    "    \"REDUCE_LR_FACTOR\",\n",
    "    \"REDUCE_LR_MIN_LR\",\n",
    "]\n",
    "missing = [name for name in required_globals if name not in globals()]\n",
    "if missing:\n",
    "    raise ValueError(f\"Run the Train-Test split for model cell first. Missing: {missing}\")\n",
    "\n",
    "if ACCOUNT_DECISION_RULE not in {\"mean\", \"any\"}:\n",
    "    raise ValueError('account_decision_rule must be \"mean\" or \"any\".')\n",
    "\n",
    "ensemble_seeds_cfg = EXPERIMENT_CONFIG.get(\"ensemble_seeds\", [RANDOM_SEED])\n",
    "if isinstance(ensemble_seeds_cfg, (int, np.integer)):\n",
    "    ensemble_seeds = [int(ensemble_seeds_cfg)]\n",
    "elif isinstance(ensemble_seeds_cfg, (list, tuple)):\n",
    "    ensemble_seeds = [int(seed) for seed in ensemble_seeds_cfg]\n",
    "else:\n",
    "    raise ValueError(\"ensemble_seeds must be an int or a list of ints.\")\n",
    "ensemble_seeds = list(dict.fromkeys(ensemble_seeds))\n",
    "if not ensemble_seeds:\n",
    "    raise ValueError(\"ensemble_seeds cannot be empty.\")\n",
    "\n",
    "ENSEMBLE_AGGREGATION = str(EXPERIMENT_CONFIG.get(\"ensemble_aggregation\", \"mean\")).lower()\n",
    "if ENSEMBLE_AGGREGATION not in {\"mean\", \"median\"}:\n",
    "    raise ValueError('ensemble_aggregation must be \"mean\" or \"median\".')\n",
    "\n",
    "USE_EXTERNAL_PRETRAIN = bool(EXPERIMENT_CONFIG.get(\"use_external_pretrain\", False))\n",
    "EXTERNAL_PRETRAIN_EPOCHS = int(EXPERIMENT_CONFIG.get(\"external_pretrain_epochs\", 1))\n",
    "EXTERNAL_PRETRAIN_BATCH_SIZE = int(EXPERIMENT_CONFIG.get(\"external_pretrain_batch_size\", 128))\n",
    "EXTERNAL_PRETRAIN_USE_BALANCED_WEIGHTS = bool(\n",
    "    EXPERIMENT_CONFIG.get(\"external_pretrain_use_balanced_weights\", True)\n",
    ")\n",
    "\n",
    "USE_SECOND_STAGE_ACCOUNT_MODEL = bool(EXPERIMENT_CONFIG.get(\"use_second_stage_account_model\", True))\n",
    "SECOND_STAGE_PROFILE = str(EXPERIMENT_CONFIG.get(\"second_stage_profile\", \"auto\")).lower()\n",
    "if SECOND_STAGE_PROFILE not in {\"auto\", \"legacy\", \"regularized\", \"custom\"}:\n",
    "    raise ValueError('second_stage_profile must be one of: \"auto\", \"legacy\", \"regularized\", \"custom\".')\n",
    "\n",
    "SECOND_STAGE_USE_BLEND = bool(EXPERIMENT_CONFIG.get(\"second_stage_use_blend\", True))\n",
    "blend_alphas_cfg = EXPERIMENT_CONFIG.get(\"second_stage_blend_alphas\", [1.0, 0.85, 0.70, 0.55])\n",
    "if isinstance(blend_alphas_cfg, (int, float, np.floating, np.integer)):\n",
    "    SECOND_STAGE_BLEND_ALPHAS = [float(blend_alphas_cfg)]\n",
    "elif isinstance(blend_alphas_cfg, (list, tuple)):\n",
    "    SECOND_STAGE_BLEND_ALPHAS = [float(alpha) for alpha in blend_alphas_cfg]\n",
    "else:\n",
    "    raise ValueError(\"second_stage_blend_alphas must be a number or a list of numbers.\")\n",
    "SECOND_STAGE_BLEND_ALPHAS = [a for a in SECOND_STAGE_BLEND_ALPHAS if 0.0 <= a <= 1.0]\n",
    "if not SECOND_STAGE_BLEND_ALPHAS:\n",
    "    SECOND_STAGE_BLEND_ALPHAS = [1.0]\n",
    "if 1.0 not in SECOND_STAGE_BLEND_ALPHAS:\n",
    "    SECOND_STAGE_BLEND_ALPHAS.append(1.0)\n",
    "SECOND_STAGE_BLEND_ALPHAS = sorted(set(SECOND_STAGE_BLEND_ALPHAS), reverse=True)\n",
    "SECOND_STAGE_MIN_GAIN_VS_ENSEMBLE = float(EXPERIMENT_CONFIG.get(\"second_stage_min_gain_vs_ensemble\", 0.0))\n",
    "SECOND_STAGE_FORCE_THRESHOLD = EXPERIMENT_CONFIG.get(\"second_stage_force_threshold\", None)\n",
    "\n",
    "SECOND_STAGE_USE_OOF_FEATURES = bool(EXPERIMENT_CONFIG.get(\"second_stage_use_oof_features\", True))\n",
    "SECOND_STAGE_OOF_FOLDS = int(EXPERIMENT_CONFIG.get(\"second_stage_oof_folds\", 4))\n",
    "SECOND_STAGE_OOF_EPOCHS = int(EXPERIMENT_CONFIG.get(\"second_stage_oof_epochs\", max(2, min(EPOCHS, 4))))\n",
    "SECOND_STAGE_OOF_BATCH_SIZE = int(EXPERIMENT_CONFIG.get(\"second_stage_oof_batch_size\", BATCH_SIZE))\n",
    "SECOND_STAGE_OOF_USE_EXTERNAL_PRETRAIN = bool(\n",
    "    EXPERIMENT_CONFIG.get(\"second_stage_oof_use_external_pretrain\", False)\n",
    ")\n",
    "\n",
    "SECOND_STAGE_LEARNING_RATE = float(EXPERIMENT_CONFIG.get(\"second_stage_learning_rate\", 0.05))\n",
    "SECOND_STAGE_MAX_ITER = int(EXPERIMENT_CONFIG.get(\"second_stage_max_iter\", 300))\n",
    "SECOND_STAGE_MAX_DEPTH = int(EXPERIMENT_CONFIG.get(\"second_stage_max_depth\", 4))\n",
    "SECOND_STAGE_L2 = float(EXPERIMENT_CONFIG.get(\"second_stage_l2\", 0.2))\n",
    "SECOND_STAGE_MIN_DATA_IN_LEAF = int(EXPERIMENT_CONFIG.get(\"second_stage_min_data_in_leaf\", 20))\n",
    "SECOND_STAGE_SUBSAMPLE = float(EXPERIMENT_CONFIG.get(\"second_stage_subsample\", 0.8))\n",
    "SECOND_STAGE_RSM = float(EXPERIMENT_CONFIG.get(\"second_stage_rsm\", 0.8))\n",
    "SECOND_STAGE_OD_WAIT = int(EXPERIMENT_CONFIG.get(\"second_stage_od_wait\", 50))\n",
    "SECOND_STAGE_USE_BALANCED_WEIGHTS = bool(EXPERIMENT_CONFIG.get(\"second_stage_use_balanced_weights\", True))\n",
    "\n",
    "oof_seeds_cfg = EXPERIMENT_CONFIG.get(\n",
    "    \"second_stage_oof_seeds\",\n",
    "    ensemble_seeds[: min(3, len(ensemble_seeds))],\n",
    ")\n",
    "if isinstance(oof_seeds_cfg, (int, np.integer)):\n",
    "    second_stage_oof_seeds = [int(oof_seeds_cfg)]\n",
    "elif isinstance(oof_seeds_cfg, (list, tuple)):\n",
    "    second_stage_oof_seeds = [int(seed) for seed in oof_seeds_cfg]\n",
    "else:\n",
    "    raise ValueError(\"second_stage_oof_seeds must be an int or a list of ints.\")\n",
    "second_stage_oof_seeds = list(dict.fromkeys(second_stage_oof_seeds))\n",
    "if SECOND_STAGE_USE_OOF_FEATURES and not second_stage_oof_seeds:\n",
    "    raise ValueError(\"second_stage_oof_seeds cannot be empty when second_stage_use_oof_features=True.\")\n",
    "\n",
    "has_validation = len(val_idx) > 0\n",
    "fit_author_ids_for_score = train_en_model.iloc[fit_idx][\"author_id\"].to_numpy()\n",
    "val_author_ids_for_score = train_en_model.iloc[val_idx][\"author_id\"].to_numpy() if has_validation else np.array([])\n",
    "test_author_ids_for_score = train_en_model.iloc[test_idx][\"author_id\"].to_numpy()\n",
    "\n",
    "def _predict_post_probs(model, indices):\n",
    "    if len(indices) == 0:\n",
    "        return np.array([], dtype=np.float32)\n",
    "    return model.predict(\n",
    "        {\n",
    "            \"input_ids\": input_ids_en[indices],\n",
    "            \"attention_mask\": attention_mask_en[indices],\n",
    "            \"aux_features\": X_aux_en[indices],\n",
    "        },\n",
    "        verbose=0,\n",
    "    ).ravel()\n",
    "\n",
    "def _build_callbacks():\n",
    "    callbacks = []\n",
    "    if has_validation and EARLY_STOPPING_PATIENCE > 0:\n",
    "        callbacks.append(\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_auc\",\n",
    "                mode=\"max\",\n",
    "                patience=EARLY_STOPPING_PATIENCE,\n",
    "                restore_best_weights=True,\n",
    "            )\n",
    "        )\n",
    "    if has_validation and REDUCE_LR_PATIENCE > 0:\n",
    "        callbacks.append(\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor=\"val_auc\",\n",
    "                mode=\"max\",\n",
    "                factor=REDUCE_LR_FACTOR,\n",
    "                patience=REDUCE_LR_PATIENCE,\n",
    "                min_lr=REDUCE_LR_MIN_LR,\n",
    "            )\n",
    "        )\n",
    "    return callbacks\n",
    "\n",
    "def _build_fold_callbacks():\n",
    "    callbacks = []\n",
    "    if EARLY_STOPPING_PATIENCE > 0:\n",
    "        callbacks.append(\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_auc\",\n",
    "                mode=\"max\",\n",
    "                patience=EARLY_STOPPING_PATIENCE,\n",
    "                restore_best_weights=True,\n",
    "            )\n",
    "        )\n",
    "    if REDUCE_LR_PATIENCE > 0:\n",
    "        callbacks.append(\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor=\"val_auc\",\n",
    "                mode=\"max\",\n",
    "                factor=REDUCE_LR_FACTOR,\n",
    "                patience=REDUCE_LR_PATIENCE,\n",
    "                min_lr=REDUCE_LR_MIN_LR,\n",
    "            )\n",
    "        )\n",
    "    return callbacks\n",
    "\n",
    "def _search_best_threshold(author_ids, labels, probs):\n",
    "    if not (has_validation and USE_THRESHOLD_SEARCH):\n",
    "        return float(PREDICTION_THRESHOLD), None, pd.DataFrame()\n",
    "\n",
    "    rows = []\n",
    "    for threshold in np.linspace(THRESHOLD_SEARCH_MIN, THRESHOLD_SEARCH_MAX, THRESHOLD_SEARCH_STEPS):\n",
    "        score, tp, fn, fp, n_accounts = compute_account_score(\n",
    "            author_ids=author_ids,\n",
    "            true_labels=labels,\n",
    "            pred_probs=probs,\n",
    "            threshold=float(threshold),\n",
    "            decision_rule=ACCOUNT_DECISION_RULE,\n",
    "        )\n",
    "        rows.append(\n",
    "            {\n",
    "                \"threshold\": float(threshold),\n",
    "                \"score\": int(score),\n",
    "                \"tp_accounts\": int(tp),\n",
    "                \"fn_accounts\": int(fn),\n",
    "                \"fp_accounts\": int(fp),\n",
    "                \"n_accounts\": int(n_accounts),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    best_row = df.sort_values(\n",
    "        by=[\"score\", \"fp_accounts\", \"tp_accounts\", \"threshold\"],\n",
    "        ascending=[False, True, False, False],\n",
    "    ).iloc[0]\n",
    "    return float(best_row[\"threshold\"]), int(best_row[\"score\"]), df\n",
    "\n",
    "def _aggregate_probabilities(prob_list, mode):\n",
    "    if not prob_list:\n",
    "        return np.array([], dtype=np.float32)\n",
    "    stack = np.vstack(prob_list)\n",
    "    if mode == \"median\":\n",
    "        return np.median(stack, axis=0).astype(np.float32)\n",
    "    return np.mean(stack, axis=0).astype(np.float32)\n",
    "\n",
    "def _prepare_external_pretrain_inputs():\n",
    "    if not USE_EXTERNAL_PRETRAIN:\n",
    "        return None, None, None\n",
    "\n",
    "    required_external = [\n",
    "        \"external_pretrain_input_ids\",\n",
    "        \"external_pretrain_attention_mask\",\n",
    "        \"external_pretrain_meta_scaled\",\n",
    "        \"external_pretrain_labels\",\n",
    "    ]\n",
    "    missing_external = [name for name in required_external if name not in globals()]\n",
    "    if missing_external:\n",
    "        raise ValueError(\n",
    "            f\"External pretrain enabled, but missing variables: {missing_external}. Run the External data pretraining cell.\"\n",
    "        )\n",
    "\n",
    "    external_meta = external_pretrain_meta_scaled\n",
    "    aux_dim = X_aux_en.shape[1]\n",
    "    meta_dim = external_meta.shape[1]\n",
    "    if aux_dim > meta_dim:\n",
    "        padding = np.zeros((external_meta.shape[0], aux_dim - meta_dim), dtype=np.float32)\n",
    "        external_aux = np.concatenate([external_meta, padding], axis=1)\n",
    "    else:\n",
    "        external_aux = external_meta[:, :aux_dim]\n",
    "\n",
    "    external_inputs = {\n",
    "        \"input_ids\": external_pretrain_input_ids,\n",
    "        \"attention_mask\": external_pretrain_attention_mask,\n",
    "        \"aux_features\": external_aux,\n",
    "    }\n",
    "    external_labels = np.asarray(globals()[\"external_pretrain_labels\"], dtype=np.int64)\n",
    "\n",
    "    external_sample_weight = None\n",
    "    if EXTERNAL_PRETRAIN_USE_BALANCED_WEIGHTS:\n",
    "        external_sample_weight = compute_sample_weight(class_weight=\"balanced\", y=external_labels)\n",
    "\n",
    "    return external_inputs, external_labels, external_sample_weight\n",
    "\n",
    "def _build_author_oof_folds(post_indices, n_splits, seed):\n",
    "    fit_posts = (\n",
    "        train_en_model.iloc[post_indices][[\"author_id\", \"is_bot\"]]\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"post_index\"})\n",
    "    )\n",
    "    author_df = fit_posts.groupby(\"author_id\", as_index=False).agg(\n",
    "        account_label=(\"is_bot\", \"max\"),\n",
    "        post_index=(\"post_index\", list),\n",
    "    )\n",
    "\n",
    "    if len(author_df) < 2:\n",
    "        return [np.asarray(post_indices, dtype=np.int64)]\n",
    "\n",
    "    label_counts = author_df[\"account_label\"].value_counts()\n",
    "    min_class_count = int(label_counts.min()) if not label_counts.empty else 0\n",
    "    n_splits_eff = min(int(n_splits), len(author_df), min_class_count)\n",
    "\n",
    "    if n_splits_eff < 2:\n",
    "        return [np.asarray(post_indices, dtype=np.int64)]\n",
    "\n",
    "    splitter = StratifiedKFold(n_splits=n_splits_eff, shuffle=True, random_state=int(seed))\n",
    "\n",
    "    folds = []\n",
    "    author_ids = author_df[\"author_id\"].to_numpy()\n",
    "    author_labels = author_df[\"account_label\"].to_numpy(dtype=np.int64)\n",
    "\n",
    "    for _, hold_author_pos in splitter.split(author_ids, author_labels):\n",
    "        hold_lists = author_df.iloc[hold_author_pos][\"post_index\"].tolist()\n",
    "        hold_posts = np.asarray([idx for lst in hold_lists for idx in lst], dtype=np.int64)\n",
    "        folds.append(hold_posts)\n",
    "\n",
    "    return folds\n",
    "\n",
    "def _compute_oof_post_probs_for_seed(seed, post_indices, use_external_pretrain_for_oof):\n",
    "    post_indices = np.asarray(post_indices, dtype=np.int64)\n",
    "    folds = _build_author_oof_folds(post_indices, SECOND_STAGE_OOF_FOLDS, RANDOM_SEED + int(seed))\n",
    "\n",
    "    if len(folds) == 1 and len(folds[0]) == len(post_indices):\n",
    "        print(\n",
    "            \"[OOF] Not enough account diversity for stratified folds; using in-sample fallback for second-stage training features.\"\n",
    "        )\n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.keras.utils.set_random_seed(int(seed))\n",
    "        np.random.seed(int(seed))\n",
    "\n",
    "        fallback_model = build_multifeature_model(\n",
    "            vocab_size=tokenizer.vocab_size,\n",
    "            seq_len=MAX_LENGTH,\n",
    "            aux_dim=X_aux_en.shape[1],\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            gru_units=GRU_UNITS,\n",
    "            aux_dense_units=AUX_DENSE_UNITS,\n",
    "            head_dense_units=HEAD_DENSE_UNITS,\n",
    "            dropout_text=DROPOUT_TEXT,\n",
    "            dropout_aux=DROPOUT_AUX,\n",
    "            dropout_head=DROPOUT_HEAD,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "        )\n",
    "\n",
    "        fit_inputs_local = {\n",
    "            \"input_ids\": input_ids_en[post_indices],\n",
    "            \"attention_mask\": attention_mask_en[post_indices],\n",
    "            \"aux_features\": X_aux_en[post_indices],\n",
    "        }\n",
    "\n",
    "        fit_kwargs_local = {\n",
    "            \"epochs\": SECOND_STAGE_OOF_EPOCHS,\n",
    "            \"batch_size\": SECOND_STAGE_OOF_BATCH_SIZE,\n",
    "            \"class_weight\": class_weight_dict if USE_CLASS_WEIGHTS else None,\n",
    "            \"verbose\": 0,\n",
    "        }\n",
    "\n",
    "        fallback_model.fit(fit_inputs_local, y_en[post_indices], **fit_kwargs_local)\n",
    "        return _predict_post_probs(fallback_model, post_indices)\n",
    "\n",
    "    index_map = np.full(len(y_en), -1, dtype=np.int64)\n",
    "    index_map[post_indices] = np.arange(len(post_indices), dtype=np.int64)\n",
    "    oof_local = np.full(len(post_indices), np.nan, dtype=np.float32)\n",
    "\n",
    "    for fold_id, hold_posts in enumerate(folds, start=1):\n",
    "        train_posts = post_indices[~np.isin(post_indices, hold_posts)]\n",
    "        if len(train_posts) == 0 or len(hold_posts) == 0:\n",
    "            continue\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.keras.utils.set_random_seed(int(seed) * 100 + fold_id)\n",
    "        np.random.seed(int(seed) * 100 + fold_id)\n",
    "\n",
    "        fold_model = build_multifeature_model(\n",
    "            vocab_size=tokenizer.vocab_size,\n",
    "            seq_len=MAX_LENGTH,\n",
    "            aux_dim=X_aux_en.shape[1],\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            gru_units=GRU_UNITS,\n",
    "            aux_dense_units=AUX_DENSE_UNITS,\n",
    "            head_dense_units=HEAD_DENSE_UNITS,\n",
    "            dropout_text=DROPOUT_TEXT,\n",
    "            dropout_aux=DROPOUT_AUX,\n",
    "            dropout_head=DROPOUT_HEAD,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "        )\n",
    "\n",
    "        if use_external_pretrain_for_oof and external_pretrain_inputs is not None:\n",
    "            pretrain_kwargs = {\n",
    "                \"epochs\": EXTERNAL_PRETRAIN_EPOCHS,\n",
    "                \"batch_size\": EXTERNAL_PRETRAIN_BATCH_SIZE,\n",
    "                \"shuffle\": True,\n",
    "                \"verbose\": 0,\n",
    "            }\n",
    "            if external_pretrain_sample_weight is not None:\n",
    "                pretrain_kwargs[\"sample_weight\"] = external_pretrain_sample_weight\n",
    "            fold_model.fit(external_pretrain_inputs, external_pretrain_labels_arr, **pretrain_kwargs)\n",
    "\n",
    "        train_inputs_fold = {\n",
    "            \"input_ids\": input_ids_en[train_posts],\n",
    "            \"attention_mask\": attention_mask_en[train_posts],\n",
    "            \"aux_features\": X_aux_en[train_posts],\n",
    "        }\n",
    "        hold_inputs_fold = {\n",
    "            \"input_ids\": input_ids_en[hold_posts],\n",
    "            \"attention_mask\": attention_mask_en[hold_posts],\n",
    "            \"aux_features\": X_aux_en[hold_posts],\n",
    "        }\n",
    "\n",
    "        fit_kwargs_fold = {\n",
    "            \"epochs\": SECOND_STAGE_OOF_EPOCHS,\n",
    "            \"batch_size\": SECOND_STAGE_OOF_BATCH_SIZE,\n",
    "            \"class_weight\": class_weight_dict if USE_CLASS_WEIGHTS else None,\n",
    "            \"callbacks\": _build_fold_callbacks(),\n",
    "            \"validation_data\": (hold_inputs_fold, y_en[hold_posts]),\n",
    "            \"verbose\": 0,\n",
    "        }\n",
    "\n",
    "        fold_model.fit(train_inputs_fold, y_en[train_posts], **fit_kwargs_fold)\n",
    "        hold_probs = fold_model.predict(hold_inputs_fold, verbose=0).ravel()\n",
    "\n",
    "        hold_local = index_map[hold_posts]\n",
    "        valid_mask = hold_local >= 0\n",
    "        oof_local[hold_local[valid_mask]] = hold_probs[valid_mask]\n",
    "\n",
    "    missing_mask = np.isnan(oof_local)\n",
    "    if missing_mask.any():\n",
    "        print(f\"[OOF] Filling {int(missing_mask.sum())} missing predictions with in-sample fallback for seed {seed}.\")\n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.keras.utils.set_random_seed(int(seed) + 999)\n",
    "        np.random.seed(int(seed) + 999)\n",
    "\n",
    "        fallback_model = build_multifeature_model(\n",
    "            vocab_size=tokenizer.vocab_size,\n",
    "            seq_len=MAX_LENGTH,\n",
    "            aux_dim=X_aux_en.shape[1],\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            gru_units=GRU_UNITS,\n",
    "            aux_dense_units=AUX_DENSE_UNITS,\n",
    "            head_dense_units=HEAD_DENSE_UNITS,\n",
    "            dropout_text=DROPOUT_TEXT,\n",
    "            dropout_aux=DROPOUT_AUX,\n",
    "            dropout_head=DROPOUT_HEAD,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "        )\n",
    "\n",
    "        fit_inputs_local = {\n",
    "            \"input_ids\": input_ids_en[post_indices],\n",
    "            \"attention_mask\": attention_mask_en[post_indices],\n",
    "            \"aux_features\": X_aux_en[post_indices],\n",
    "        }\n",
    "\n",
    "        fit_kwargs_local = {\n",
    "            \"epochs\": SECOND_STAGE_OOF_EPOCHS,\n",
    "            \"batch_size\": SECOND_STAGE_OOF_BATCH_SIZE,\n",
    "            \"class_weight\": class_weight_dict if USE_CLASS_WEIGHTS else None,\n",
    "            \"verbose\": 0,\n",
    "        }\n",
    "        fallback_model.fit(fit_inputs_local, y_en[post_indices], **fit_kwargs_local)\n",
    "        fallback_probs = fallback_model.predict(fit_inputs_local, verbose=0).ravel()\n",
    "        oof_local[missing_mask] = fallback_probs[missing_mask]\n",
    "\n",
    "    return oof_local.astype(np.float32)\n",
    "\n",
    "fit_inputs = {\n",
    "    \"input_ids\": input_ids_en[fit_idx],\n",
    "    \"attention_mask\": attention_mask_en[fit_idx],\n",
    "    \"aux_features\": X_aux_en[fit_idx],\n",
    "}\n",
    "val_inputs = (\n",
    "    {\n",
    "        \"input_ids\": input_ids_en[val_idx],\n",
    "        \"attention_mask\": attention_mask_en[val_idx],\n",
    "        \"aux_features\": X_aux_en[val_idx],\n",
    "    }\n",
    "    if has_validation\n",
    "    else None\n",
    ")\n",
    "\n",
    "post_prob_fit_list = []\n",
    "post_prob_val_list = []\n",
    "post_prob_test_list = []\n",
    "seed_rows = []\n",
    "ensemble_models_en = []\n",
    "\n",
    "if hasattr(tf.config.experimental, \"enable_op_determinism\"):\n",
    "    try:\n",
    "        tf.config.experimental.enable_op_determinism()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "external_pretrain_inputs = None\n",
    "external_pretrain_labels_arr = None\n",
    "external_pretrain_sample_weight = None\n",
    "if USE_EXTERNAL_PRETRAIN:\n",
    "    external_pretrain_inputs, external_pretrain_labels_arr, external_pretrain_sample_weight = _prepare_external_pretrain_inputs()\n",
    "\n",
    "for seed in ensemble_seeds:\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.keras.utils.set_random_seed(int(seed))\n",
    "    np.random.seed(int(seed))\n",
    "\n",
    "    model_seed = build_multifeature_model(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        seq_len=MAX_LENGTH,\n",
    "        aux_dim=X_aux_en.shape[1],\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        gru_units=GRU_UNITS,\n",
    "        aux_dense_units=AUX_DENSE_UNITS,\n",
    "        head_dense_units=HEAD_DENSE_UNITS,\n",
    "        dropout_text=DROPOUT_TEXT,\n",
    "        dropout_aux=DROPOUT_AUX,\n",
    "        dropout_head=DROPOUT_HEAD,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "    )\n",
    "\n",
    "    if USE_EXTERNAL_PRETRAIN:\n",
    "        pretrain_kwargs = {\n",
    "            \"epochs\": EXTERNAL_PRETRAIN_EPOCHS,\n",
    "            \"batch_size\": EXTERNAL_PRETRAIN_BATCH_SIZE,\n",
    "            \"shuffle\": True,\n",
    "            \"verbose\": 0,\n",
    "        }\n",
    "        if external_pretrain_sample_weight is not None:\n",
    "            pretrain_kwargs[\"sample_weight\"] = external_pretrain_sample_weight\n",
    "        model_seed.fit(external_pretrain_inputs, external_pretrain_labels_arr, **pretrain_kwargs)\n",
    "\n",
    "    fit_kwargs = {\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"class_weight\": class_weight_dict if USE_CLASS_WEIGHTS else None,\n",
    "        \"callbacks\": _build_callbacks(),\n",
    "        \"verbose\": 0,\n",
    "    }\n",
    "    if has_validation:\n",
    "        fit_kwargs[\"validation_data\"] = (val_inputs, y_en[val_idx])\n",
    "\n",
    "    model_seed.fit(fit_inputs, y_en[fit_idx], **fit_kwargs)\n",
    "\n",
    "    fit_probs = _predict_post_probs(model_seed, fit_idx)\n",
    "    val_probs = _predict_post_probs(model_seed, val_idx)\n",
    "    test_probs = _predict_post_probs(model_seed, test_idx)\n",
    "\n",
    "    threshold_seed, val_score_seed, _ = _search_best_threshold(val_author_ids_for_score, y_en[val_idx], val_probs)\n",
    "\n",
    "    test_score_seed, tp_seed, fn_seed, fp_seed, n_accounts_seed = compute_account_score(\n",
    "        author_ids=test_author_ids_for_score,\n",
    "        true_labels=y_en[test_idx],\n",
    "        pred_probs=test_probs,\n",
    "        threshold=threshold_seed,\n",
    "        decision_rule=ACCOUNT_DECISION_RULE,\n",
    "    )\n",
    "\n",
    "    seed_rows.append(\n",
    "        {\n",
    "            \"seed\": int(seed),\n",
    "            \"threshold\": float(threshold_seed),\n",
    "            \"val_best_score\": val_score_seed,\n",
    "            \"test_score\": int(test_score_seed),\n",
    "            \"tp_accounts\": int(tp_seed),\n",
    "            \"fn_accounts\": int(fn_seed),\n",
    "            \"fp_accounts\": int(fp_seed),\n",
    "            \"n_accounts\": int(n_accounts_seed),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    post_prob_fit_list.append(fit_probs)\n",
    "    post_prob_val_list.append(val_probs)\n",
    "    post_prob_test_list.append(test_probs)\n",
    "    ensemble_models_en.append(model_seed)\n",
    "\n",
    "seed_report_df = pd.DataFrame(seed_rows)\n",
    "ensemble_seed_report_en = seed_report_df.copy()\n",
    "\n",
    "post_prob_fit_ensemble_en = _aggregate_probabilities(post_prob_fit_list, ENSEMBLE_AGGREGATION)\n",
    "post_prob_val_ensemble_en = _aggregate_probabilities(post_prob_val_list, ENSEMBLE_AGGREGATION)\n",
    "post_prob_test_ensemble_en = _aggregate_probabilities(post_prob_test_list, ENSEMBLE_AGGREGATION)\n",
    "\n",
    "selected_threshold_ensemble, best_ensemble_val_score, threshold_search_results_ensemble_en = _search_best_threshold(\n",
    "    val_author_ids_for_score,\n",
    "    y_en[val_idx],\n",
    "    post_prob_val_ensemble_en,\n",
    ")\n",
    "\n",
    "(\n",
    "    test_score_ensemble_en,\n",
    "    ensemble_tp_accounts_en,\n",
    "    ensemble_fn_accounts_en,\n",
    "    ensemble_fp_accounts_en,\n",
    "    ensemble_n_accounts_en,\n",
    ") = compute_account_score(\n",
    "    author_ids=test_author_ids_for_score,\n",
    "    true_labels=y_en[test_idx],\n",
    "    pred_probs=post_prob_test_ensemble_en,\n",
    "    threshold=selected_threshold_ensemble,\n",
    "    decision_rule=ACCOUNT_DECISION_RULE,\n",
    ")\n",
    "\n",
    "# Make the ensemble outputs the default baseline for downstream score/plot cells.\n",
    "post_prob_fit = post_prob_fit_ensemble_en\n",
    "post_prob_val = post_prob_val_ensemble_en\n",
    "post_prob_test = post_prob_test_ensemble_en\n",
    "y_prob = post_prob_test_ensemble_en\n",
    "SELECTED_THRESHOLD = float(selected_threshold_ensemble)\n",
    "test_score = int(test_score_ensemble_en)\n",
    "test_tp_accounts = int(ensemble_tp_accounts_en)\n",
    "test_fn_accounts = int(ensemble_fn_accounts_en)\n",
    "test_fp_accounts = int(ensemble_fp_accounts_en)\n",
    "test_n_accounts = int(ensemble_n_accounts_en)\n",
    "\n",
    "seed_mean_score_en = float(seed_report_df[\"test_score\"].mean())\n",
    "seed_std_score_en = float(seed_report_df[\"test_score\"].std(ddof=1)) if len(seed_report_df) > 1 else 0.0\n",
    "\n",
    "def build_account_feature_table(post_indices, post_probs, bot_threshold):\n",
    "    posts = train_en_model.iloc[post_indices].copy()\n",
    "    posts[\"post_prob\"] = np.asarray(post_probs, dtype=np.float32)\n",
    "\n",
    "    posts[\"has_url_post\"] = (posts[\"url_count\"] > 0).astype(np.float32)\n",
    "    posts[\"has_mention_post\"] = (posts[\"mention_count\"] > 0).astype(np.float32)\n",
    "    posts[\"has_hashtag_post\"] = (posts[\"hashtag_count\"] > 0).astype(np.float32)\n",
    "    posts[\"pred_bot_post\"] = (posts[\"post_prob\"] >= bot_threshold).astype(np.float32)\n",
    "\n",
    "    agg_spec = {\n",
    "        \"is_bot\": [\"max\"],\n",
    "        \"text_clean\": [\"size\"],\n",
    "        \"post_prob\": [\"mean\", \"std\", \"max\", \"min\"],\n",
    "        \"char_count\": [\"mean\", \"std\", \"max\"],\n",
    "        \"word_count\": [\"mean\", \"std\", \"max\"],\n",
    "        \"url_count\": [\"mean\", \"max\"],\n",
    "        \"mention_count\": [\"mean\", \"max\"],\n",
    "        \"hashtag_count\": [\"mean\", \"max\"],\n",
    "        \"exclamation_count\": [\"mean\", \"max\"],\n",
    "        \"question_count\": [\"mean\", \"max\"],\n",
    "        \"has_url_post\": [\"mean\"],\n",
    "        \"has_mention_post\": [\"mean\"],\n",
    "        \"has_hashtag_post\": [\"mean\"],\n",
    "        \"pred_bot_post\": [\"mean\"],\n",
    "    }\n",
    "\n",
    "    for topic_col in topic_feature_cols_en:\n",
    "        agg_spec[topic_col] = [\"mean\"]\n",
    "\n",
    "    account = posts.groupby(\"author_id\", as_index=False).agg(agg_spec)\n",
    "    account.columns = [\n",
    "        \"author_id\" if col == (\"author_id\", \"\") else f\"{col[0]}_{col[1]}\"\n",
    "        for col in account.columns.to_flat_index()\n",
    "    ]\n",
    "\n",
    "    account = account.rename(columns={\"is_bot_max\": \"true_is_bot\", \"text_clean_size\": \"n_posts\"})\n",
    "    for col in [\"post_prob_std\", \"char_count_std\", \"word_count_std\"]:\n",
    "        if col in account.columns:\n",
    "            account[col] = account[col].fillna(0.0)\n",
    "    account[\"n_posts_log1p\"] = np.log1p(account[\"n_posts\"].astype(np.float32))\n",
    "\n",
    "    user_source = (\n",
    "        users_en_labeled.copy()\n",
    "        if \"users_en_labeled\" in globals()\n",
    "        else users_en.drop_duplicates(subset=\"id\", keep=\"last\").copy()\n",
    "    )\n",
    "    user_source[\"username_len\"] = user_source[\"username\"].fillna(\"\").str.len()\n",
    "    user_source[\"name_len\"] = user_source[\"name\"].fillna(\"\").str.len()\n",
    "    user_source[\"description_len\"] = user_source[\"description\"].fillna(\"\").str.len()\n",
    "    user_source[\"has_location\"] = user_source[\"location\"].fillna(\"\").str.strip().ne(\"\").astype(np.float32)\n",
    "\n",
    "    user_features = user_source[\n",
    "        [\n",
    "            \"id\",\n",
    "            \"tweet_count\",\n",
    "            \"z_score\",\n",
    "            \"username_len\",\n",
    "            \"name_len\",\n",
    "            \"description_len\",\n",
    "            \"has_location\",\n",
    "        ]\n",
    "    ].copy()\n",
    "\n",
    "    account = account.merge(user_features, left_on=\"author_id\", right_on=\"id\", how=\"left\").drop(columns=[\"id\"])\n",
    "\n",
    "    numeric_cols = [col for col in account.columns if col != \"author_id\"]\n",
    "    account[numeric_cols] = account[numeric_cols].apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
    "    return account\n",
    "\n",
    "def score_from_account_probs(true_labels, pred_probs, threshold):\n",
    "    pred_labels = (pred_probs >= threshold).astype(np.int64)\n",
    "    tp = int(((true_labels == 1) & (pred_labels == 1)).sum())\n",
    "    fn = int(((true_labels == 1) & (pred_labels == 0)).sum())\n",
    "    fp = int(((true_labels == 0) & (pred_labels == 1)).sum())\n",
    "    score = (4 * tp) - (1 * fn) - (2 * fp)\n",
    "    return score, tp, fn, fp, pred_labels\n",
    "\n",
    "def _search_threshold_for_account_probs(y_true, y_prob):\n",
    "    if len(y_true) == 0:\n",
    "        return float(PREDICTION_THRESHOLD), None, 0, 0, 0, pd.DataFrame()\n",
    "\n",
    "    if not USE_THRESHOLD_SEARCH:\n",
    "        score, tp, fn, fp, _ = score_from_account_probs(y_true, y_prob, float(PREDICTION_THRESHOLD))\n",
    "        return float(PREDICTION_THRESHOLD), int(score), int(tp), int(fn), int(fp), pd.DataFrame()\n",
    "\n",
    "    rows = []\n",
    "    for threshold in np.linspace(THRESHOLD_SEARCH_MIN, THRESHOLD_SEARCH_MAX, THRESHOLD_SEARCH_STEPS):\n",
    "        score, tp, fn, fp, _ = score_from_account_probs(y_true, y_prob, float(threshold))\n",
    "        rows.append(\n",
    "            {\n",
    "                \"threshold\": float(threshold),\n",
    "                \"score\": int(score),\n",
    "                \"tp_accounts\": int(tp),\n",
    "                \"fn_accounts\": int(fn),\n",
    "                \"fp_accounts\": int(fp),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    best_row = df.sort_values(\n",
    "        by=[\"score\", \"fp_accounts\", \"tp_accounts\", \"threshold\"],\n",
    "        ascending=[False, True, False, False],\n",
    "    ).iloc[0]\n",
    "\n",
    "    return (\n",
    "        float(best_row[\"threshold\"]),\n",
    "        int(best_row[\"score\"]),\n",
    "        int(best_row[\"tp_accounts\"]),\n",
    "        int(best_row[\"fn_accounts\"]),\n",
    "        int(best_row[\"fp_accounts\"]),\n",
    "        df,\n",
    "    )\n",
    "\n",
    "def _make_second_stage_candidates():\n",
    "    candidates = {}\n",
    "\n",
    "    legacy_params = {\n",
    "        \"iterations\": 300,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"depth\": 4,\n",
    "        \"l2_leaf_reg\": 0.2,\n",
    "        \"loss_function\": \"Logloss\",\n",
    "        \"eval_metric\": \"AUC\",\n",
    "        \"random_seed\": RANDOM_SEED,\n",
    "        \"verbose\": False,\n",
    "    }\n",
    "\n",
    "    regularized_params = {\n",
    "        \"iterations\": SECOND_STAGE_MAX_ITER,\n",
    "        \"learning_rate\": SECOND_STAGE_LEARNING_RATE,\n",
    "        \"depth\": SECOND_STAGE_MAX_DEPTH,\n",
    "        \"l2_leaf_reg\": SECOND_STAGE_L2,\n",
    "        \"min_data_in_leaf\": SECOND_STAGE_MIN_DATA_IN_LEAF,\n",
    "        \"loss_function\": \"Logloss\",\n",
    "        \"eval_metric\": \"AUC\",\n",
    "        \"random_seed\": RANDOM_SEED,\n",
    "        \"verbose\": False,\n",
    "        \"od_type\": \"Iter\",\n",
    "        \"od_wait\": SECOND_STAGE_OD_WAIT,\n",
    "    }\n",
    "    if 0.0 < SECOND_STAGE_SUBSAMPLE < 1.0:\n",
    "        regularized_params[\"subsample\"] = SECOND_STAGE_SUBSAMPLE\n",
    "        regularized_params[\"bootstrap_type\"] = \"Bernoulli\"\n",
    "    if 0.0 < SECOND_STAGE_RSM <= 1.0:\n",
    "        regularized_params[\"rsm\"] = SECOND_STAGE_RSM\n",
    "\n",
    "    custom_params = {\n",
    "        \"iterations\": SECOND_STAGE_MAX_ITER,\n",
    "        \"learning_rate\": SECOND_STAGE_LEARNING_RATE,\n",
    "        \"depth\": SECOND_STAGE_MAX_DEPTH,\n",
    "        \"l2_leaf_reg\": SECOND_STAGE_L2,\n",
    "        \"min_data_in_leaf\": SECOND_STAGE_MIN_DATA_IN_LEAF,\n",
    "        \"loss_function\": \"Logloss\",\n",
    "        \"eval_metric\": \"AUC\",\n",
    "        \"random_seed\": RANDOM_SEED,\n",
    "        \"verbose\": False,\n",
    "        \"od_type\": \"Iter\",\n",
    "        \"od_wait\": SECOND_STAGE_OD_WAIT,\n",
    "    }\n",
    "    if 0.0 < SECOND_STAGE_SUBSAMPLE < 1.0:\n",
    "        custom_params[\"subsample\"] = SECOND_STAGE_SUBSAMPLE\n",
    "        custom_params[\"bootstrap_type\"] = \"Bernoulli\"\n",
    "    if 0.0 < SECOND_STAGE_RSM <= 1.0:\n",
    "        custom_params[\"rsm\"] = SECOND_STAGE_RSM\n",
    "\n",
    "    if SECOND_STAGE_PROFILE in {\"auto\", \"legacy\"}:\n",
    "        candidates[\"legacy\"] = legacy_params\n",
    "    if SECOND_STAGE_PROFILE in {\"auto\", \"regularized\"}:\n",
    "        candidates[\"regularized\"] = regularized_params\n",
    "    if SECOND_STAGE_PROFILE == \"custom\":\n",
    "        candidates[\"custom\"] = custom_params\n",
    "\n",
    "    if SECOND_STAGE_USE_BALANCED_WEIGHTS:\n",
    "        for params in candidates.values():\n",
    "            params[\"auto_class_weights\"] = \"Balanced\"\n",
    "\n",
    "    return candidates\n",
    "\n",
    "if USE_SECOND_STAGE_ACCOUNT_MODEL:\n",
    "    post_prob_fit_for_second_stage_en = post_prob_fit_ensemble_en.copy()\n",
    "    second_stage_fit_feature_source_en = \"in_sample_ensemble\"\n",
    "    second_stage_oof_report_en = pd.DataFrame()\n",
    "\n",
    "    if SECOND_STAGE_USE_OOF_FEATURES:\n",
    "        print(\n",
    "            f\"Building OOF first-stage features for second-stage training (seeds={second_stage_oof_seeds}, folds={SECOND_STAGE_OOF_FOLDS}, epochs={SECOND_STAGE_OOF_EPOCHS})...\"\n",
    "        )\n",
    "        oof_fit_prob_list = []\n",
    "        oof_rows = []\n",
    "\n",
    "        for oof_seed in second_stage_oof_seeds:\n",
    "            oof_probs_seed = _compute_oof_post_probs_for_seed(\n",
    "                seed=int(oof_seed),\n",
    "                post_indices=fit_idx,\n",
    "                use_external_pretrain_for_oof=bool(USE_EXTERNAL_PRETRAIN and SECOND_STAGE_OOF_USE_EXTERNAL_PRETRAIN),\n",
    "            )\n",
    "\n",
    "            oof_score_seed, oof_tp, oof_fn, oof_fp, oof_accounts = compute_account_score(\n",
    "                author_ids=fit_author_ids_for_score,\n",
    "                true_labels=y_en[fit_idx],\n",
    "                pred_probs=oof_probs_seed,\n",
    "                threshold=selected_threshold_ensemble,\n",
    "                decision_rule=ACCOUNT_DECISION_RULE,\n",
    "            )\n",
    "\n",
    "            oof_rows.append(\n",
    "                {\n",
    "                    \"seed\": int(oof_seed),\n",
    "                    \"fit_account_score_at_ensemble_threshold\": int(oof_score_seed),\n",
    "                    \"tp_accounts\": int(oof_tp),\n",
    "                    \"fn_accounts\": int(oof_fn),\n",
    "                    \"fp_accounts\": int(oof_fp),\n",
    "                    \"n_accounts\": int(oof_accounts),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            oof_fit_prob_list.append(oof_probs_seed)\n",
    "\n",
    "        post_prob_fit_for_second_stage_en = _aggregate_probabilities(oof_fit_prob_list, ENSEMBLE_AGGREGATION)\n",
    "        second_stage_fit_feature_source_en = \"oof_first_stage\"\n",
    "        second_stage_oof_report_en = pd.DataFrame(oof_rows)\n",
    "\n",
    "    fit_account_df = build_account_feature_table(fit_idx, post_prob_fit_for_second_stage_en, selected_threshold_ensemble)\n",
    "    val_account_df = (\n",
    "        build_account_feature_table(val_idx, post_prob_val_ensemble_en, selected_threshold_ensemble)\n",
    "        if has_validation\n",
    "        else pd.DataFrame()\n",
    "    )\n",
    "    test_account_df = build_account_feature_table(test_idx, post_prob_test_ensemble_en, selected_threshold_ensemble)\n",
    "\n",
    "    target_col = \"true_is_bot\"\n",
    "    feature_cols_account = [col for col in fit_account_df.columns if col not in {\"author_id\", target_col}]\n",
    "\n",
    "    for df in [fit_account_df, val_account_df, test_account_df]:\n",
    "        if df.empty:\n",
    "            continue\n",
    "        missing_cols = [col for col in feature_cols_account if col not in df.columns]\n",
    "        for col in missing_cols:\n",
    "            df[col] = 0.0\n",
    "\n",
    "    X_fit_acc = fit_account_df[feature_cols_account].to_numpy(dtype=np.float32)\n",
    "    y_fit_acc = fit_account_df[target_col].to_numpy(dtype=np.int64)\n",
    "    X_test_acc = test_account_df[feature_cols_account].to_numpy(dtype=np.float32)\n",
    "    y_test_acc = test_account_df[target_col].to_numpy(dtype=np.int64)\n",
    "\n",
    "    if len(val_account_df):\n",
    "        X_val_acc = val_account_df[feature_cols_account].to_numpy(dtype=np.float32)\n",
    "        y_val_acc = val_account_df[target_col].to_numpy(dtype=np.int64)\n",
    "    else:\n",
    "        X_val_acc = np.zeros((0, len(feature_cols_account)), dtype=np.float32)\n",
    "        y_val_acc = np.zeros((0,), dtype=np.int64)\n",
    "\n",
    "    baseline_account_val_score = None\n",
    "    baseline_account_val_threshold = float(selected_threshold_ensemble)\n",
    "    baseline_account_val_tp = 0\n",
    "    baseline_account_val_fn = 0\n",
    "    baseline_account_val_fp = 0\n",
    "\n",
    "    base_val_prob_acc = val_account_df[\"post_prob_mean\"].to_numpy(dtype=np.float32) if len(y_val_acc) else np.array([])\n",
    "    base_test_prob_acc = test_account_df[\"post_prob_mean\"].to_numpy(dtype=np.float32)\n",
    "\n",
    "    if len(y_val_acc):\n",
    "        (\n",
    "            baseline_account_val_threshold,\n",
    "            baseline_account_val_score,\n",
    "            baseline_account_val_tp,\n",
    "            baseline_account_val_fn,\n",
    "            baseline_account_val_fp,\n",
    "            baseline_account_threshold_search_en,\n",
    "        ) = _search_threshold_for_account_probs(y_val_acc, base_val_prob_acc)\n",
    "    else:\n",
    "        baseline_account_threshold_search_en = pd.DataFrame()\n",
    "\n",
    "    candidate_params = _make_second_stage_candidates()\n",
    "    candidate_rows = []\n",
    "    candidate_artifacts = []\n",
    "\n",
    "    for profile_name, params in candidate_params.items():\n",
    "        model = CatBoostClassifier(**params)\n",
    "\n",
    "        fit_kwargs = {}\n",
    "        if len(y_val_acc):\n",
    "            fit_kwargs[\"eval_set\"] = (X_val_acc, y_val_acc)\n",
    "            fit_kwargs[\"use_best_model\"] = True\n",
    "\n",
    "        model.fit(X_fit_acc, y_fit_acc, **fit_kwargs)\n",
    "\n",
    "        test_raw_prob = model.predict_proba(X_test_acc)[:, 1]\n",
    "        val_raw_prob = model.predict_proba(X_val_acc)[:, 1] if len(y_val_acc) else np.array([])\n",
    "\n",
    "        if len(y_val_acc):\n",
    "            best_candidate = None\n",
    "            for alpha in (SECOND_STAGE_BLEND_ALPHAS if SECOND_STAGE_USE_BLEND else [1.0]):\n",
    "                val_candidate_prob = (alpha * val_raw_prob) + ((1.0 - alpha) * base_val_prob_acc)\n",
    "                (\n",
    "                    cand_threshold,\n",
    "                    cand_val_score,\n",
    "                    cand_val_tp,\n",
    "                    cand_val_fn,\n",
    "                    cand_val_fp,\n",
    "                    cand_search_df,\n",
    "                ) = _search_threshold_for_account_probs(y_val_acc, val_candidate_prob)\n",
    "\n",
    "                if (\n",
    "                    best_candidate is None\n",
    "                    or cand_val_score > best_candidate[\"val_score\"]\n",
    "                    or (\n",
    "                        cand_val_score == best_candidate[\"val_score\"]\n",
    "                        and cand_val_fp < best_candidate[\"val_fp_accounts\"]\n",
    "                    )\n",
    "                    or (\n",
    "                        cand_val_score == best_candidate[\"val_score\"]\n",
    "                        and cand_val_fp == best_candidate[\"val_fp_accounts\"]\n",
    "                        and cand_val_tp > best_candidate[\"val_tp_accounts\"]\n",
    "                    )\n",
    "                ):\n",
    "                    best_candidate = {\n",
    "                        \"alpha\": float(alpha),\n",
    "                        \"threshold\": float(cand_threshold),\n",
    "                        \"val_score\": int(cand_val_score),\n",
    "                        \"val_tp_accounts\": int(cand_val_tp),\n",
    "                        \"val_fn_accounts\": int(cand_val_fn),\n",
    "                        \"val_fp_accounts\": int(cand_val_fp),\n",
    "                        \"search_df\": cand_search_df,\n",
    "                    }\n",
    "\n",
    "            chosen_alpha = best_candidate[\"alpha\"]\n",
    "            chosen_threshold = best_candidate[\"threshold\"]\n",
    "            chosen_val_score = best_candidate[\"val_score\"]\n",
    "            chosen_val_tp = best_candidate[\"val_tp_accounts\"]\n",
    "            chosen_val_fn = best_candidate[\"val_fn_accounts\"]\n",
    "            chosen_val_fp = best_candidate[\"val_fp_accounts\"]\n",
    "            threshold_df = best_candidate[\"search_df\"]\n",
    "        else:\n",
    "            chosen_alpha = 1.0\n",
    "            chosen_threshold = float(selected_threshold_ensemble)\n",
    "            chosen_val_score = np.nan\n",
    "            chosen_val_tp = np.nan\n",
    "            chosen_val_fn = np.nan\n",
    "            chosen_val_fp = np.nan\n",
    "            threshold_df = pd.DataFrame()\n",
    "\n",
    "        test_blend_prob = (chosen_alpha * test_raw_prob) + ((1.0 - chosen_alpha) * base_test_prob_acc)\n",
    "        (\n",
    "            test_score_profile,\n",
    "            test_tp_profile,\n",
    "            test_fn_profile,\n",
    "            test_fp_profile,\n",
    "            test_pred_profile,\n",
    "        ) = score_from_account_probs(y_test_acc, test_blend_prob, chosen_threshold)\n",
    "\n",
    "        candidate_rows.append(\n",
    "            {\n",
    "                \"profile\": profile_name,\n",
    "                \"alpha\": float(chosen_alpha),\n",
    "                \"threshold\": float(chosen_threshold),\n",
    "                \"val_score\": None if pd.isna(chosen_val_score) else int(chosen_val_score),\n",
    "                \"val_tp_accounts\": None if pd.isna(chosen_val_tp) else int(chosen_val_tp),\n",
    "                \"val_fn_accounts\": None if pd.isna(chosen_val_fn) else int(chosen_val_fn),\n",
    "                \"val_fp_accounts\": None if pd.isna(chosen_val_fp) else int(chosen_val_fp),\n",
    "                \"test_score\": int(test_score_profile),\n",
    "                \"test_tp_accounts\": int(test_tp_profile),\n",
    "                \"test_fn_accounts\": int(test_fn_profile),\n",
    "                \"test_fp_accounts\": int(test_fp_profile),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        candidate_artifacts.append(\n",
    "            {\n",
    "                \"profile\": profile_name,\n",
    "                \"model\": model,\n",
    "                \"alpha\": float(chosen_alpha),\n",
    "                \"threshold\": float(chosen_threshold),\n",
    "                \"val_score\": chosen_val_score,\n",
    "                \"val_tp_accounts\": chosen_val_tp,\n",
    "                \"val_fn_accounts\": chosen_val_fn,\n",
    "                \"val_fp_accounts\": chosen_val_fp,\n",
    "                \"threshold_search_df\": threshold_df,\n",
    "                \"test_prob\": test_blend_prob,\n",
    "                \"test_pred\": test_pred_profile,\n",
    "                \"test_score\": int(test_score_profile),\n",
    "                \"test_tp_accounts\": int(test_tp_profile),\n",
    "                \"test_fn_accounts\": int(test_fn_profile),\n",
    "                \"test_fp_accounts\": int(test_fp_profile),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    second_stage_candidate_report_en = pd.DataFrame(candidate_rows)\n",
    "\n",
    "    if second_stage_candidate_report_en.empty:\n",
    "        raise ValueError(\"No second-stage candidate was trained. Check second_stage_profile.\")\n",
    "\n",
    "    if len(y_val_acc):\n",
    "        sorted_candidates = second_stage_candidate_report_en.sort_values(\n",
    "            by=[\"val_score\", \"val_fp_accounts\", \"val_tp_accounts\", \"threshold\", \"alpha\"],\n",
    "            ascending=[False, True, False, False, False],\n",
    "            na_position=\"last\",\n",
    "        )\n",
    "    else:\n",
    "        sorted_candidates = second_stage_candidate_report_en.sort_values(\n",
    "            by=[\"test_score\", \"test_fp_accounts\", \"test_tp_accounts\"],\n",
    "            ascending=[False, True, False],\n",
    "        )\n",
    "\n",
    "    best_profile_name = str(sorted_candidates.iloc[0][\"profile\"])\n",
    "    selected_artifact = next(item for item in candidate_artifacts if item[\"profile\"] == best_profile_name)\n",
    "\n",
    "    second_stage_selected_profile_en = best_profile_name\n",
    "    second_stage_used_fallback_en = False\n",
    "\n",
    "    if len(y_val_acc):\n",
    "        best_val_score = int(selected_artifact[\"val_score\"])\n",
    "        baseline_ref_score = int(baseline_account_val_score) if baseline_account_val_score is not None else int(best_ensemble_val_score or 0)\n",
    "\n",
    "        if best_val_score < (baseline_ref_score + SECOND_STAGE_MIN_GAIN_VS_ENSEMBLE):\n",
    "            second_stage_used_fallback_en = True\n",
    "            second_stage_selected_profile_en = \"fallback_ensemble\"\n",
    "\n",
    "    if second_stage_used_fallback_en:\n",
    "        second_stage_model_en = None\n",
    "        second_stage_selected_threshold_en = float(selected_threshold_ensemble)\n",
    "        second_stage_alpha_en = 0.0\n",
    "        second_stage_test_score_en = int(test_score_ensemble_en)\n",
    "        second_stage_tp_accounts_en = int(ensemble_tp_accounts_en)\n",
    "        second_stage_fn_accounts_en = int(ensemble_fn_accounts_en)\n",
    "        second_stage_fp_accounts_en = int(ensemble_fp_accounts_en)\n",
    "\n",
    "        baseline_account_eval = pd.DataFrame(\n",
    "            {\n",
    "                \"author_id\": test_author_ids_for_score,\n",
    "                \"true_is_bot\": y_en[test_idx].astype(np.int64),\n",
    "                \"post_prob\": post_prob_test_ensemble_en.astype(np.float32),\n",
    "            }\n",
    "        )\n",
    "        baseline_account_eval[\"pred_post\"] = (baseline_account_eval[\"post_prob\"] >= selected_threshold_ensemble).astype(np.int64)\n",
    "        baseline_account_eval = (\n",
    "            baseline_account_eval.groupby(\"author_id\", as_index=False)\n",
    "            .agg(\n",
    "                true_is_bot=(\"true_is_bot\", \"max\"),\n",
    "                mean_prob=(\"post_prob\", \"mean\"),\n",
    "                any_pred=(\"pred_post\", \"max\"),\n",
    "            )\n",
    "        )\n",
    "\n",
    "        if ACCOUNT_DECISION_RULE == \"any\":\n",
    "            baseline_account_eval[\"pred_is_bot\"] = baseline_account_eval[\"any_pred\"].astype(np.int64)\n",
    "            baseline_account_eval[\"pred_prob\"] = baseline_account_eval[\"any_pred\"].astype(np.float32)\n",
    "        else:\n",
    "            baseline_account_eval[\"pred_is_bot\"] = (\n",
    "                baseline_account_eval[\"mean_prob\"] >= selected_threshold_ensemble\n",
    "            ).astype(np.int64)\n",
    "            baseline_account_eval[\"pred_prob\"] = baseline_account_eval[\"mean_prob\"].astype(np.float32)\n",
    "\n",
    "        second_stage_test_prob_en = baseline_account_eval[\"pred_prob\"].to_numpy(dtype=np.float32)\n",
    "        second_stage_test_pred_en = baseline_account_eval[\"pred_is_bot\"].to_numpy(dtype=np.int64)\n",
    "\n",
    "        second_stage_account_predictions_en = baseline_account_eval[\n",
    "            [\"author_id\", \"true_is_bot\", \"pred_prob\", \"pred_is_bot\"]\n",
    "        ].copy()\n",
    "\n",
    "        second_stage_threshold_search_results_en = pd.DataFrame()\n",
    "    else:\n",
    "        second_stage_model_en = selected_artifact[\"model\"]\n",
    "        second_stage_selected_threshold_en = float(selected_artifact[\"threshold\"])\n",
    "        second_stage_alpha_en = float(selected_artifact[\"alpha\"])\n",
    "        second_stage_threshold_search_results_en = selected_artifact[\"threshold_search_df\"]\n",
    "        second_stage_test_prob_en = np.asarray(selected_artifact[\"test_prob\"], dtype=np.float32)\n",
    "        second_stage_test_pred_en = np.asarray(selected_artifact[\"test_pred\"], dtype=np.int64)\n",
    "        second_stage_test_score_en = int(selected_artifact[\"test_score\"])\n",
    "        second_stage_tp_accounts_en = int(selected_artifact[\"test_tp_accounts\"])\n",
    "        second_stage_fn_accounts_en = int(selected_artifact[\"test_fn_accounts\"])\n",
    "        second_stage_fp_accounts_en = int(selected_artifact[\"test_fp_accounts\"])\n",
    "\n",
    "        second_stage_account_predictions_en = test_account_df[[\"author_id\", target_col]].copy()\n",
    "        second_stage_account_predictions_en[\"pred_prob\"] = second_stage_test_prob_en\n",
    "        second_stage_account_predictions_en[\"pred_is_bot\"] = second_stage_test_pred_en\n",
    "        if SECOND_STAGE_FORCE_THRESHOLD is not None:\n",
    "            forced_threshold = float(SECOND_STAGE_FORCE_THRESHOLD)\n",
    "            second_stage_selected_threshold_en = forced_threshold\n",
    "            second_stage_test_pred_en = (second_stage_test_prob_en >= forced_threshold).astype(np.int64)\n",
    "            second_stage_tp_accounts_en = int(((y_test_acc == 1) & (second_stage_test_pred_en == 1)).sum())\n",
    "            second_stage_fn_accounts_en = int(((y_test_acc == 1) & (second_stage_test_pred_en == 0)).sum())\n",
    "            second_stage_fp_accounts_en = int(((y_test_acc == 0) & (second_stage_test_pred_en == 1)).sum())\n",
    "            second_stage_test_score_en = int((4 * second_stage_tp_accounts_en) - (1 * second_stage_fn_accounts_en) - (2 * second_stage_fp_accounts_en))\n",
    "            second_stage_account_predictions_en[\"pred_is_bot\"] = second_stage_test_pred_en\n",
    "            print(f\"Second-stage threshold overridden to {forced_threshold:.4f} from EXPERIMENT_CONFIG.\")\n",
    "\n",
    "    max_score_accounts = 4 * int((y_test_acc == 1).sum())\n",
    "    second_stage_score_ratio_en = (\n",
    "        second_stage_test_score_en / max_score_accounts if max_score_accounts > 0 else np.nan\n",
    "    )\n",
    "\n",
    "    max_possible_score = max_score_accounts\n",
    "\n",
    "print(\"Seed-level first-stage account scores (test):\")\n",
    "print(\n",
    "    seed_report_df[\n",
    "        [\n",
    "            \"seed\",\n",
    "            \"threshold\",\n",
    "            \"test_score\",\n",
    "            \"tp_accounts\",\n",
    "            \"fn_accounts\",\n",
    "            \"fp_accounts\",\n",
    "        ]\n",
    "    ].to_string(index=False)\n",
    ")\n",
    "print(f\"Seed score mean/std: {seed_mean_score_en:.2f} / {seed_std_score_en:.2f}\")\n",
    "print(f\"Ensemble aggregation: {ENSEMBLE_AGGREGATION}\")\n",
    "print(f\"Ensemble selected threshold: {selected_threshold_ensemble:.4f}\")\n",
    "print(\n",
    "    f\"Ensemble test score: {test_score_ensemble_en} (TP={ensemble_tp_accounts_en}, FN={ensemble_fn_accounts_en}, FP={ensemble_fp_accounts_en}, accounts={ensemble_n_accounts_en})\"\n",
    ")\n",
    "\n",
    "if USE_SECOND_STAGE_ACCOUNT_MODEL:\n",
    "    print(f\"Second-stage fit feature source: {second_stage_fit_feature_source_en}\")\n",
    "    if not second_stage_oof_report_en.empty:\n",
    "        print(\"OOF seed report for second-stage fit features:\")\n",
    "        print(second_stage_oof_report_en.to_string(index=False))\n",
    "\n",
    "    print(\"Second-stage candidate report:\")\n",
    "    print(second_stage_candidate_report_en.to_string(index=False))\n",
    "\n",
    "    if has_validation and baseline_account_val_score is not None:\n",
    "        print(\n",
    "            f\"Baseline account-level validation score (from first-stage means): {baseline_account_val_score} \"\n",
    "            f\"(TP={baseline_account_val_tp}, FN={baseline_account_val_fn}, FP={baseline_account_val_fp}, \"\n",
    "            f\"threshold={baseline_account_val_threshold:.4f})\"\n",
    "        )\n",
    "\n",
    "    print(f\"Second-stage profile mode: {SECOND_STAGE_PROFILE}\")\n",
    "    print(f\"Second-stage selected profile: {second_stage_selected_profile_en}\")\n",
    "    if second_stage_selected_profile_en != \"fallback_ensemble\":\n",
    "        print(f\"Second-stage blend alpha (CatBoost weight): {second_stage_alpha_en:.2f}\")\n",
    "        print(f\"Second-stage threshold: {second_stage_selected_threshold_en:.4f}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"Second-stage fallback triggered: no candidate beat baseline validation score by the configured margin.\"\n",
    "        )\n",
    "\n",
    "    print(\n",
    "        f\"Second-stage test score: {second_stage_test_score_en}/{max_possible_score} ({second_stage_score_ratio_en:.2%})\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Second-stage confusion components -> TP={second_stage_tp_accounts_en}, FN={second_stage_fn_accounts_en}, FP={second_stage_fp_accounts_en}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Second-stage precision={precision_score(y_test_acc, second_stage_test_pred_en, zero_division=0):.4f}, \"\n",
    "        f\"recall={recall_score(y_test_acc, second_stage_test_pred_en, zero_division=0):.4f}\"\n",
    "    )\n",
    "    print(classification_report(y_test_acc, second_stage_test_pred_en, digits=4))\n",
    "else:\n",
    "    print(\"Second-stage account model disabled in EXPERIMENT_CONFIG.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26c4f9a",
   "metadata": {},
   "source": [
    "## 8. Final Score Plot\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c477e2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x450 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without booster: 72\n",
      "With booster: 102\n",
      "Competition top (max possible on this split): 104\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if \"train_en_model\" not in globals() or \"test_idx\" not in globals():\n",
    "    raise ValueError(\"Run the training cell first.\")\n",
    "\n",
    "score_without_booster = None\n",
    "if \"test_score\" in globals():\n",
    "    score_without_booster = float(test_score)\n",
    "elif \"score\" in globals():\n",
    "    score_without_booster = float(score)\n",
    "else:\n",
    "    raise ValueError(\"No baseline score found. Run the model scoring cells first.\")\n",
    "\n",
    "score_with_booster = float(second_stage_test_score_en) if \"second_stage_test_score_en\" in globals() else None\n",
    "\n",
    "if \"max_possible_score\" in globals():\n",
    "    competition_top = float(max_possible_score)\n",
    "else:\n",
    "    account_truth = (\n",
    "        train_en_model.iloc[test_idx][[\"author_id\", \"is_bot\"]]\n",
    "        .groupby(\"author_id\", as_index=False)[\"is_bot\"]\n",
    "        .max()\n",
    "    )\n",
    "    competition_top = float(4 * int(account_truth[\"is_bot\"].sum()))\n",
    "\n",
    "labels = [\"Without booster\"]\n",
    "scores = [score_without_booster]\n",
    "colors = [\"#4c78a8\"]\n",
    "\n",
    "if score_with_booster is not None:\n",
    "    labels.append(\"With booster\")\n",
    "    scores.append(score_with_booster)\n",
    "    colors.append(\"#f58518\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4.5))\n",
    "bars = ax.bar(labels, scores, color=colors, alpha=0.9)\n",
    "ax.axhline(competition_top, color=\"#54a24b\", linestyle=\"--\", linewidth=2, label=f\"Competition top: {competition_top:.0f}\")\n",
    "\n",
    "for bar, value in zip(bars, scores):\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        value,\n",
    "        f\"{value:.0f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=10,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "ax.set_title(\"Competition Score Comparison\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_ylim(0, max(max(scores), competition_top) * 1.15)\n",
    "ax.grid(axis=\"y\", linestyle=\":\", alpha=0.4)\n",
    "ax.legend(loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Without booster: {score_without_booster:.0f}\")\n",
    "if score_with_booster is not None:\n",
    "    print(f\"With booster: {score_with_booster:.0f}\")\n",
    "print(f\"Competition top (max possible on this split): {competition_top:.0f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef7ab6",
   "metadata": {},
   "source": [
    "## 9. Error Analysis (Aggregate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc8171e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-level error summary (first-stage ensemble):\n",
      "error_type  n_posts\n",
      "        TN     2180\n",
      "        TP      694\n",
      "        FP      310\n",
      "        FN      222\n",
      "\n",
      "No topic matches found in the test split.\n",
      "\n",
      "Account-level errors by dominant topic:\n",
      "dominant_topic  n_accounts  bot_rate  fp_accounts  fn_accounts  tp_accounts  tn_accounts  error_rate\n",
      "no_topic_match         110  0.236364            1            0           26           83    0.009091\n",
      "\n",
      "Sample hard cases (for pattern inspection, not rule memorization):\n",
      "                           author_id                                                                                                                                                                                                                                                                                                                   text_clean  pred_prob error_type\n",
      "cb3becd6-ecd9-a9ed-9f0a-f9e652c39596                                                                                                                                     GOOD MORNING???? I open X to find this???? That's the mysterious Foot-on -sofa IG pic revealed! It's a set!! And live performances??? And Cartier!!!! Omgggggg https://t.co/twitter_link   0.902241         FP\n",
      "8d511f4a-2574-98c1-a10d-fb80642d7ffc                                                                                                                                                                                                                                                                                                Another deflection???????????   0.817729         FP\n",
      "4d03e547-f849-8c68-bbbe-8dd7974855ca                                     volcanos, oceans, jungles, ice, sun, moon, food from the ground, trees, berries, papayas, fish, potatoes, cows, bread fruit, pigs, lettuce, and spices, and people paying rent. amazing how brains are fooled, like a spell casted dark magic over populations keeping us from maturing.   0.790490         FP\n",
      "cb3becd6-ecd9-a9ed-9f0a-f9e652c39596                                                                              How did they manage to post a post no one saw and it’s not on their profile?????? What new malarkey is this?? Darlings, you better hire a new, competent and most importantly UNBIASED community manager before 2025… https://t.co/twitter_link   0.777673         FP\n",
      "293cef2c-e668-ad7a-8441-caeccb9d4faf                                                                                                                                                 Why is my heart still broken after an entire year? I’ve put in more self work on myself in this past 12 months than I have in over a decade. Yet I can’t move on. Why? Geez.   0.771086         FP\n",
      "1af28907-1303-bb1c-92ed-f57c6c8b8c2a                                                                                                                                                                                                                                     babe, look! the world is celebrating my everyday! my birthday! https://t.co/twitter_link   0.762405         FP\n",
      "3dd13f5a-7d7f-923a-a7e4-22ac42b0fd90                                                                                                                                                                                                                     Why am I just hearing Taylor’s demo for This Is What You Came For for the first time today? Soooooo good   0.738639         FP\n",
      "78b8d71a-c0e7-a00f-a4a5-630f2165a9ce                                                                                                                                                                                                                                                Describing to my bf which of my sims should be the heir to the family fortune   0.736679         FP\n",
      "983af1b7-6870-a9ba-b036-ca05d8ddbd1a                                                               The first Korean Soloist in history to debut at #1 on Billboard Charts simultaneously, achieving a historic All-Kill with Seven. Congratulations Jungkook, the giant pop star whose music is loved all over the world, we are so proud! #SevenBillboardAllKill   0.732987         FP\n",
      "78b8d71a-c0e7-a00f-a4a5-630f2165a9ce                                                                                                                                                                                                                                    Just watched a disney movie and now I will play the corresponding world on kingdom hearts   0.721232         FP\n",
      "8b02b826-fdda-400f-90ad-8ff07c57b89b                                                                                                                                                                                                                                        not sure what i was thinking here https://t.co/twitter_link https://t.co/twitter_link   0.013081         FN\n",
      "c2af8c73-2bc2-415b-abaa-2fd5083b56e8                                                                                                                                                                                                                                                Good grab from Mo. 😉 #LGRW https://t.co/witter_link https://t.co/twitter_link   0.022309         FN\n",
      "affbdacc-ecce-4e88-b671-2e1ff85f0034 TW: ED * * * * * * I was looking study &amp; book twt since oomf recommended But I tried searching for study &amp; book communities on here &amp; this is the very first result. No judgment. It’s a disease &amp; I’ve been there but it sad to see this pro-ED culture prevalent and poisonous.🥺 https://t.co/twitter_link   0.025325         FN\n",
      "8b02b826-fdda-400f-90ad-8ff07c57b89b                                                                                                                                                                                                                                   I’m a big fan of John David Washington https://t.co/twitter_link https://t.co/twitter_link   0.029910         FN\n",
      "affbdacc-ecce-4e88-b671-2e1ff85f0034                                                                                                                                                                                                                                     they look so chill here acting like they don’t hate each other https://t.co/twitter_link   0.031853         FN\n",
      "8b02b826-fdda-400f-90ad-8ff07c57b89b                                        Your man a bird he heard you and your 2 sisters arguing over him in the bedroom drunk and you said BITCH TALK ABOUT MY MAN SHABLIM and she said BITCH YOU THE ONE THAT TOLD ME HE HAD THE OLIVE DICK THAT NIGHT and he walked in the bedroom quiet with jordans on dressed nice Lmfao   0.034338         FN\n",
      "affbdacc-ecce-4e88-b671-2e1ff85f0034                                                                                                                                                                                                               Manchester United vs Liverpool is constantly toxic. We will be there no depend what! https://t.co/twitter_link   0.036084         FN\n",
      "8b02b826-fdda-400f-90ad-8ff07c57b89b                                                                                                                                                                                                                                                                              Kyle Busch this year: https://t.co/twitter_link   0.037443         FN\n",
      "10c39255-c132-4a1b-b898-e304da85e166                                                                                                                                                                                                                                                                   this sucks man. eury is awesome. https://t.co/twitter_link   0.037695         FN\n",
      "c2af8c73-2bc2-415b-abaa-2fd5083b56e8                                                                                                                                                                                                                                                                                Adore this clip lol https://t.co/twitter_link   0.038273         FN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "required = [\"train_en_model\", \"test_idx\", \"post_prob_test\"]\n",
    "missing = [name for name in required if name not in globals()]\n",
    "if missing:\n",
    "    raise ValueError(f\"Run the training and booster cells first. Missing: {missing}\")\n",
    "\n",
    "analysis_threshold = float(SELECTED_THRESHOLD if \"SELECTED_THRESHOLD\" in globals() else PREDICTION_THRESHOLD)\n",
    "\n",
    "posts_eval = train_en_model.iloc[test_idx].copy().reset_index(drop=True)\n",
    "posts_eval[\"true_is_bot\"] = posts_eval[\"is_bot\"].astype(np.int64)\n",
    "posts_eval[\"pred_prob\"] = np.asarray(post_prob_test, dtype=np.float32)\n",
    "posts_eval[\"pred_is_bot\"] = (posts_eval[\"pred_prob\"] >= analysis_threshold).astype(np.int64)\n",
    "\n",
    "posts_eval[\"error_type\"] = np.where(\n",
    "    (posts_eval[\"true_is_bot\"] == 1) & (posts_eval[\"pred_is_bot\"] == 0),\n",
    "    \"FN\",\n",
    "    np.where(\n",
    "        (posts_eval[\"true_is_bot\"] == 0) & (posts_eval[\"pred_is_bot\"] == 1),\n",
    "        \"FP\",\n",
    "        np.where(posts_eval[\"true_is_bot\"] == 1, \"TP\", \"TN\"),\n",
    "    ),\n",
    ")\n",
    "\n",
    "post_error_summary_en = (\n",
    "    posts_eval[\"error_type\"]\n",
    "    .value_counts()\n",
    "    .rename_axis(\"error_type\")\n",
    "    .reset_index(name=\"n_posts\")\n",
    ")\n",
    "\n",
    "print(\"Post-level error summary (first-stage ensemble):\")\n",
    "print(post_error_summary_en.to_string(index=False))\n",
    "\n",
    "if topic_feature_cols_en:\n",
    "    topic_rows = []\n",
    "    for topic_col in topic_feature_cols_en:\n",
    "        subset = posts_eval[posts_eval[topic_col] > 0]\n",
    "        if subset.empty:\n",
    "            continue\n",
    "\n",
    "        fp = int(((subset[\"true_is_bot\"] == 0) & (subset[\"pred_is_bot\"] == 1)).sum())\n",
    "        fn = int(((subset[\"true_is_bot\"] == 1) & (subset[\"pred_is_bot\"] == 0)).sum())\n",
    "        tp = int(((subset[\"true_is_bot\"] == 1) & (subset[\"pred_is_bot\"] == 1)).sum())\n",
    "        tn = int(((subset[\"true_is_bot\"] == 0) & (subset[\"pred_is_bot\"] == 0)).sum())\n",
    "        n = int(len(subset))\n",
    "\n",
    "        topic_rows.append(\n",
    "            {\n",
    "                \"topic\": topic_col.replace(\"topic_\", \"\"),\n",
    "                \"n_posts\": n,\n",
    "                \"fp_posts\": fp,\n",
    "                \"fn_posts\": fn,\n",
    "                \"tp_posts\": tp,\n",
    "                \"tn_posts\": tn,\n",
    "                \"error_rate\": (fp + fn) / n if n else 0.0,\n",
    "                \"bot_rate\": subset[\"true_is_bot\"].mean(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    topic_post_error_report_en = pd.DataFrame(topic_rows)\n",
    "    if not topic_post_error_report_en.empty:\n",
    "        min_topic_posts = int(EXPERIMENT_CONFIG.get(\"error_analysis_min_topic_posts\", 20))\n",
    "        topic_post_error_report_en = topic_post_error_report_en.sort_values(\n",
    "            by=[\"error_rate\", \"n_posts\"], ascending=[False, False]\n",
    "        )\n",
    "\n",
    "        print(\"\\nTopic-level post errors (filtering tiny buckets):\")\n",
    "        print(\n",
    "            topic_post_error_report_en[topic_post_error_report_en[\"n_posts\"] >= min_topic_posts]\n",
    "            .head(15)\n",
    "            .to_string(index=False)\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nNo topic matches found in the test split.\")\n",
    "else:\n",
    "    print(\"\\nTopic features are disabled, so no topic-level error table was computed.\")\n",
    "\n",
    "if \"second_stage_account_predictions_en\" in globals() and isinstance(second_stage_account_predictions_en, pd.DataFrame):\n",
    "    account_eval = second_stage_account_predictions_en.copy()\n",
    "    if \"true_is_bot\" not in account_eval.columns and \"is_bot\" in account_eval.columns:\n",
    "        account_eval = account_eval.rename(columns={\"is_bot\": \"true_is_bot\"})\n",
    "else:\n",
    "    account_eval = (\n",
    "        posts_eval.groupby(\"author_id\", as_index=False)\n",
    "        .agg(\n",
    "            true_is_bot=(\"true_is_bot\", \"max\"),\n",
    "            mean_prob=(\"pred_prob\", \"mean\"),\n",
    "            any_pred=(\"pred_is_bot\", \"max\"),\n",
    "        )\n",
    "        .copy()\n",
    "    )\n",
    "\n",
    "    if ACCOUNT_DECISION_RULE == \"any\":\n",
    "        account_eval[\"pred_is_bot\"] = account_eval[\"any_pred\"].astype(np.int64)\n",
    "        account_eval[\"pred_prob\"] = account_eval[\"any_pred\"].astype(np.float32)\n",
    "    else:\n",
    "        account_eval[\"pred_is_bot\"] = (account_eval[\"mean_prob\"] >= analysis_threshold).astype(np.int64)\n",
    "        account_eval[\"pred_prob\"] = account_eval[\"mean_prob\"].astype(np.float32)\n",
    "\n",
    "if topic_feature_cols_en:\n",
    "    topic_strength = posts_eval.groupby(\"author_id\")[topic_feature_cols_en].mean()\n",
    "    dominant_topic = topic_strength.idxmax(axis=1).str.replace(\"topic_\", \"\", regex=False)\n",
    "    dominant_topic = dominant_topic.where(topic_strength.max(axis=1) > 0, \"no_topic_match\")\n",
    "else:\n",
    "    dominant_topic = pd.Series(\"no_topic_features\", index=account_eval[\"author_id\"])\n",
    "\n",
    "account_eval = account_eval.merge(\n",
    "    dominant_topic.rename(\"dominant_topic\"),\n",
    "    left_on=\"author_id\",\n",
    "    right_index=True,\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "account_eval[\"error_type\"] = np.where(\n",
    "    (account_eval[\"true_is_bot\"] == 1) & (account_eval[\"pred_is_bot\"] == 0),\n",
    "    \"FN\",\n",
    "    np.where(\n",
    "        (account_eval[\"true_is_bot\"] == 0) & (account_eval[\"pred_is_bot\"] == 1),\n",
    "        \"FP\",\n",
    "        np.where(account_eval[\"true_is_bot\"] == 1, \"TP\", \"TN\"),\n",
    "    ),\n",
    ")\n",
    "\n",
    "account_topic_report_en = (\n",
    "    account_eval.groupby(\"dominant_topic\", as_index=False)\n",
    "    .agg(\n",
    "        n_accounts=(\"author_id\", \"size\"),\n",
    "        bot_rate=(\"true_is_bot\", \"mean\"),\n",
    "        fp_accounts=(\"error_type\", lambda s: int((s == \"FP\").sum())),\n",
    "        fn_accounts=(\"error_type\", lambda s: int((s == \"FN\").sum())),\n",
    "        tp_accounts=(\"error_type\", lambda s: int((s == \"TP\").sum())),\n",
    "        tn_accounts=(\"error_type\", lambda s: int((s == \"TN\").sum())),\n",
    "    )\n",
    ")\n",
    "\n",
    "account_topic_report_en[\"error_rate\"] = (\n",
    "    account_topic_report_en[\"fp_accounts\"] + account_topic_report_en[\"fn_accounts\"]\n",
    ") / account_topic_report_en[\"n_accounts\"]\n",
    "account_topic_report_en = account_topic_report_en.sort_values(\n",
    "    by=[\"error_rate\", \"n_accounts\"], ascending=[False, False]\n",
    ")\n",
    "\n",
    "min_topic_accounts = int(EXPERIMENT_CONFIG.get(\"error_analysis_min_topic_accounts\", 5))\n",
    "print(\"\\nAccount-level errors by dominant topic:\")\n",
    "print(\n",
    "    account_topic_report_en[account_topic_report_en[\"n_accounts\"] >= min_topic_accounts]\n",
    "    .head(15)\n",
    "    .to_string(index=False)\n",
    ")\n",
    "\n",
    "fp_examples = posts_eval[\n",
    "    (posts_eval[\"true_is_bot\"] == 0) & (posts_eval[\"pred_is_bot\"] == 1)\n",
    "][[\"author_id\", \"text_clean\", \"pred_prob\"]].sort_values(\"pred_prob\", ascending=False).head(10)\n",
    "fn_examples = posts_eval[\n",
    "    (posts_eval[\"true_is_bot\"] == 1) & (posts_eval[\"pred_is_bot\"] == 0)\n",
    "][[\"author_id\", \"text_clean\", \"pred_prob\"]].sort_values(\"pred_prob\", ascending=True).head(10)\n",
    "\n",
    "hard_case_examples_en = pd.concat(\n",
    "    [\n",
    "        fp_examples.assign(error_type=\"FP\"),\n",
    "        fn_examples.assign(error_type=\"FN\"),\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "if not hard_case_examples_en.empty:\n",
    "    print(\"\\nSample hard cases (for pattern inspection, not rule memorization):\")\n",
    "    print(hard_case_examples_en.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "348e1a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "competition_final_data/dataset.posts&users.34.tweets.json competition_final_data/dataset.posts&users.34.users.json\n",
      "competition_final_data/dataset.posts&users.35.tweets.json competition_final_data/dataset.posts&users.35.users.json\n"
     ]
    }
   ],
   "source": [
    "def split_combined(path_str):\n",
    "    p = Path(path_str)\n",
    "    data = json.loads(p.read_text())\n",
    "    tweets_path = p.with_name(p.stem + \".tweets.json\")\n",
    "    users_path = p.with_name(p.stem + \".users.json\")\n",
    "    tweets_path.write_text(json.dumps(data[\"posts\"], ensure_ascii=False))\n",
    "    users_path.write_text(json.dumps(data[\"users\"], ensure_ascii=False))\n",
    "    return tweets_path, users_path\n",
    "\n",
    "en_tweets, en_users = split_combined(\"competition_final_data/dataset.posts&users.34.json\")\n",
    "fr_tweets, fr_users = split_combined(\"competition_final_data/dataset.posts&users.35.json\")\n",
    "\n",
    "print(en_tweets, en_users)\n",
    "print(fr_tweets, fr_users)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66c1f78d",
   "metadata": {},
   "source": [
    "## 10. Competition Inference and Rule-Compliant Export\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e2aa7ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Language filter applied: lang=en | tweets kept=14920/14920\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-14 12:15:54.343472: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-14 12:16:01.094221: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-14 12:16:06.284160: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-14 12:16:13.740631: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-14 12:16:21.250839: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Competition tweets file: competition_final_data/dataset.posts&users.34.tweets.json\n",
      "Competition users file:  competition_final_data/dataset.posts&users.34.users.json\n",
      "Submission file:        /Users/adrienbelanger/Library/GitHub/bot_or_not_McHacks_2026/competition_final_export/kickingbots.detections.en.txt\n",
      "Team name:              kickingbots\n",
      "Language:               en\n",
      "Accounts flagged:       78\n",
      "Format check: one author_id per line, filename matches rules [team_name].detections.[lang].txt\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "required_globals = [\n",
    "    \"tokenizer\",\n",
    "    \"MAX_LENGTH\",\n",
    "    \"feature_cols_en\",\n",
    "    \"add_text_features\",\n",
    "    \"train_en\",\n",
    "]\n",
    "missing = [name for name in required_globals if name not in globals()]\n",
    "if missing:\n",
    "    raise ValueError(f\"Run the training/booster cells first. Missing: {missing}\")\n",
    "\n",
    "TEAM_NAME = \"kickingbots\"\n",
    "SUBMISSION_LANG = globals().get(\"SUBMISSION_LANG\", \"en\")\n",
    "\n",
    "base_root = Path(globals().get(\"PROJECT_ROOT\", Path(\".\")))\n",
    "COMPETITION_DATA_DIR = Path(globals().get(\"COMPETITION_DATA_DIR\", base_root / \"competition_final_data\")).resolve()\n",
    "COMPETITION_TWEETS_FILE = en_tweets\n",
    "COMPETITION_USERS_FILE = en_users\n",
    "SUBMISSION_OUTPUT_DIR = Path(globals().get(\"SUBMISSION_OUTPUT_DIR\", base_root / \"competition_final_export\")).resolve()\n",
    "SUBMISSION_OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "def _load_json_or_jsonl(path):\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Missing file: {path}\")\n",
    "\n",
    "    with path.open() as f:\n",
    "        try:\n",
    "            payload = json.load(f)\n",
    "            if isinstance(payload, list):\n",
    "                return payload\n",
    "            if isinstance(payload, dict):\n",
    "                return [payload]\n",
    "            raise ValueError(f\"Unsupported JSON payload in {path}: {type(payload)}\")\n",
    "        except json.JSONDecodeError:\n",
    "            pass\n",
    "\n",
    "    rows = []\n",
    "    with path.open() as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if not line:\n",
    "                continue\n",
    "            rows.append(json.loads(line))\n",
    "    return rows\n",
    "\n",
    "\n",
    "if not COMPETITION_TWEETS_FILE.exists() or not COMPETITION_USERS_FILE.exists():\n",
    "    tweets_candidates = sorted(COMPETITION_DATA_DIR.glob(\"*tweets*.json\"))\n",
    "    users_candidates = sorted(COMPETITION_DATA_DIR.glob(\"*users*.json\"))\n",
    "    raise FileNotFoundError(\n",
    "        \"Competition files not found. Set COMPETITION_TWEETS_FILE and COMPETITION_USERS_FILE. \"\n",
    "        f\"Tweets candidates: {[p.name for p in tweets_candidates]} | Users candidates: {[p.name for p in users_candidates]}\"\n",
    "    )\n",
    "\n",
    "competition_posts = pd.DataFrame(_load_json_or_jsonl(COMPETITION_TWEETS_FILE))\n",
    "competition_users = pd.DataFrame(_load_json_or_jsonl(COMPETITION_USERS_FILE))\n",
    "\n",
    "if competition_posts.empty:\n",
    "    raise ValueError(f\"No rows loaded from {COMPETITION_TWEETS_FILE}\")\n",
    "if competition_users.empty:\n",
    "    raise ValueError(f\"No rows loaded from {COMPETITION_USERS_FILE}\")\n",
    "\n",
    "required_post_cols = {\"author_id\", \"text\"}\n",
    "missing_post_cols = sorted(required_post_cols - set(competition_posts.columns))\n",
    "if missing_post_cols:\n",
    "    raise ValueError(f\"Competition tweets file is missing required columns: {missing_post_cols}\")\n",
    "\n",
    "STRICT_LANG_CHECK = bool(globals().get(\"STRICT_LANG_CHECK\", True))\n",
    "\n",
    "if \"lang\" not in competition_posts.columns:\n",
    "    if STRICT_LANG_CHECK:\n",
    "        raise ValueError(\n",
    "            \"Competition tweets file has no 'lang' column. \"\n",
    "            \"Refusing to run with STRICT_LANG_CHECK=True to avoid cross-language leakage.\"\n",
    "        )\n",
    "    print(\"Warning: no 'lang' column found; proceeding without language filtering.\")\n",
    "else:\n",
    "    competition_posts[\"_lang_norm\"] = competition_posts[\"lang\"].astype(str).str.lower().str.strip()\n",
    "    before_lang_filter = len(competition_posts)\n",
    "    competition_posts = competition_posts[competition_posts[\"_lang_norm\"] == str(SUBMISSION_LANG).lower()].copy()\n",
    "    after_lang_filter = len(competition_posts)\n",
    "    print(\n",
    "        f\"Language filter applied: lang={SUBMISSION_LANG} | \"\n",
    "        f\"tweets kept={after_lang_filter}/{before_lang_filter}\"\n",
    "    )\n",
    "\n",
    "if competition_posts.empty:\n",
    "    raise ValueError(f\"No tweets left after lang={SUBMISSION_LANG} filtering.\")\n",
    "\n",
    "competition_posts = add_text_features(competition_posts.copy())\n",
    "\n",
    "# Topic features (if enabled in this notebook)\n",
    "topic_cols_expected = list(globals().get(\"topic_feature_cols_en\", []))\n",
    "if bool(globals().get(\"USE_TOPIC_FEATURES\", False)) and topic_cols_expected and \"add_topic_features\" in globals():\n",
    "    if \"load_target_topic_keywords\" in globals():\n",
    "        topic_keywords = load_target_topic_keywords()\n",
    "    elif \"load_english_topic_keywords\" in globals():\n",
    "        topic_keywords = load_english_topic_keywords()\n",
    "    else:\n",
    "        topic_keywords = globals().get(\"topic_keywords_en\", {})\n",
    "\n",
    "    topic_mode = str(globals().get(\"TOPIC_MATCH_MODE\", \"word\"))\n",
    "    competition_posts, _ = add_topic_features(competition_posts, topic_keywords, topic_mode)\n",
    "\n",
    "for col in topic_cols_expected:\n",
    "    if col not in competition_posts.columns:\n",
    "        competition_posts[col] = 0\n",
    "\n",
    "for col in feature_cols_en:\n",
    "    if col not in competition_posts.columns:\n",
    "        competition_posts[col] = 0\n",
    "\n",
    "X_meta_comp = competition_posts[feature_cols_en].to_numpy(dtype=np.float32)\n",
    "if bool(globals().get(\"SCALE_META_FEATURES\", True)) and globals().get(\"scaler_en\", None) is not None:\n",
    "    X_meta_comp = scaler_en.transform(X_meta_comp).astype(np.float32)\n",
    "\n",
    "if topic_cols_expected:\n",
    "    X_topic_comp = competition_posts[topic_cols_expected].to_numpy(dtype=np.float32)\n",
    "else:\n",
    "    X_topic_comp = np.zeros((len(competition_posts), 0), dtype=np.float32)\n",
    "\n",
    "X_aux_comp = np.concatenate([X_meta_comp, X_topic_comp], axis=1)\n",
    "\n",
    "enc_comp = tokenizer(\n",
    "    competition_posts[\"text_clean\"].tolist(),\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=MAX_LENGTH,\n",
    "    return_attention_mask=True,\n",
    ")\n",
    "input_ids_comp = np.asarray(enc_comp[\"input_ids\"], dtype=np.int32)\n",
    "attention_mask_comp = np.asarray(enc_comp[\"attention_mask\"], dtype=np.float32)\n",
    "\n",
    "first_stage_models = globals().get(\"ensemble_models_en\", None)\n",
    "if isinstance(first_stage_models, list) and len(first_stage_models) > 0:\n",
    "    prob_list = []\n",
    "    for model in first_stage_models:\n",
    "        p = model.predict(\n",
    "            {\n",
    "                \"input_ids\": input_ids_comp,\n",
    "                \"attention_mask\": attention_mask_comp,\n",
    "                \"aux_features\": X_aux_comp,\n",
    "            },\n",
    "            verbose=0,\n",
    "        ).ravel()\n",
    "        prob_list.append(p)\n",
    "\n",
    "    agg_mode = str(globals().get(\"ENSEMBLE_AGGREGATION\", EXPERIMENT_CONFIG.get(\"ensemble_aggregation\", \"mean\"))).lower()\n",
    "    stack = np.vstack(prob_list)\n",
    "    if agg_mode == \"median\":\n",
    "        post_prob_comp = np.median(stack, axis=0).astype(np.float32)\n",
    "    else:\n",
    "        post_prob_comp = np.mean(stack, axis=0).astype(np.float32)\n",
    "else:\n",
    "    fallback_model = globals().get(\"model_seed\", globals().get(\"model\", None))\n",
    "    if fallback_model is None:\n",
    "        raise ValueError(\"No first-stage model available. Re-run booster/training cell first.\")\n",
    "    post_prob_comp = fallback_model.predict(\n",
    "        {\n",
    "            \"input_ids\": input_ids_comp,\n",
    "            \"attention_mask\": attention_mask_comp,\n",
    "            \"aux_features\": X_aux_comp,\n",
    "        },\n",
    "        verbose=0,\n",
    "    ).ravel().astype(np.float32)\n",
    "\n",
    "post_bot_threshold = float(\n",
    "    globals().get(\"selected_threshold_ensemble\", globals().get(\"SELECTED_THRESHOLD\", globals().get(\"PREDICTION_THRESHOLD\", 0.5)))\n",
    ")\n",
    "\n",
    "comp_posts_for_account = competition_posts.copy()\n",
    "comp_posts_for_account[\"post_prob\"] = post_prob_comp\n",
    "comp_posts_for_account[\"pred_bot_post\"] = (comp_posts_for_account[\"post_prob\"] >= post_bot_threshold).astype(np.float32)\n",
    "\n",
    "# Build account-level table\n",
    "comp_posts_for_account[\"has_url_post\"] = (comp_posts_for_account.get(\"url_count\", 0) > 0).astype(np.float32)\n",
    "comp_posts_for_account[\"has_mention_post\"] = (comp_posts_for_account.get(\"mention_count\", 0) > 0).astype(np.float32)\n",
    "comp_posts_for_account[\"has_hashtag_post\"] = (comp_posts_for_account.get(\"hashtag_count\", 0) > 0).astype(np.float32)\n",
    "\n",
    "agg_spec = {\n",
    "    \"text_clean\": [\"size\"],\n",
    "    \"post_prob\": [\"mean\", \"std\", \"max\", \"min\"],\n",
    "    \"char_count\": [\"mean\", \"std\", \"max\"],\n",
    "    \"word_count\": [\"mean\", \"std\", \"max\"],\n",
    "    \"url_count\": [\"mean\", \"max\"],\n",
    "    \"mention_count\": [\"mean\", \"max\"],\n",
    "    \"hashtag_count\": [\"mean\", \"max\"],\n",
    "    \"exclamation_count\": [\"mean\", \"max\"],\n",
    "    \"question_count\": [\"mean\", \"max\"],\n",
    "    \"has_url_post\": [\"mean\"],\n",
    "    \"has_mention_post\": [\"mean\"],\n",
    "    \"has_hashtag_post\": [\"mean\"],\n",
    "    \"pred_bot_post\": [\"mean\", \"sum\"],\n",
    "}\n",
    "for col in topic_cols_expected:\n",
    "    agg_spec[col] = [\"mean\"]\n",
    "\n",
    "account_features = comp_posts_for_account.groupby(\"author_id\", as_index=False).agg(agg_spec)\n",
    "account_features.columns = [\n",
    "    \"author_id\" if col == (\"author_id\", \"\") else f\"{col[0]}_{col[1]}\"\n",
    "    for col in account_features.columns.to_flat_index()\n",
    "]\n",
    "account_features = account_features.rename(columns={\"text_clean_size\": \"n_posts\"})\n",
    "for col in [\"post_prob_std\", \"char_count_std\", \"word_count_std\"]:\n",
    "    if col in account_features.columns:\n",
    "        account_features[col] = account_features[col].fillna(0.0)\n",
    "account_features[\"n_posts_log1p\"] = np.log1p(account_features[\"n_posts\"].astype(np.float32))\n",
    "account_features[\"pred_bot_post_count\"] = account_features.get(\"pred_bot_post_sum\", 0).astype(np.float32)\n",
    "account_features[\"pred_bot_post_rate\"] = account_features.get(\"pred_bot_post_mean\", 0).astype(np.float32)\n",
    "\n",
    "# Merge user metadata\n",
    "u = competition_users.copy()\n",
    "if \"id\" not in u.columns:\n",
    "    raise ValueError(\"Competition users file must contain 'id'.\")\n",
    "for col in [\"tweet_count\", \"z_score\", \"username\", \"name\", \"description\", \"location\"]:\n",
    "    if col not in u.columns:\n",
    "        u[col] = \"\" if col in {\"username\", \"name\", \"description\", \"location\"} else 0\n",
    "u[\"username_len\"] = u[\"username\"].fillna(\"\").astype(str).str.len()\n",
    "u[\"name_len\"] = u[\"name\"].fillna(\"\").astype(str).str.len()\n",
    "u[\"description_len\"] = u[\"description\"].fillna(\"\").astype(str).str.len()\n",
    "u[\"has_location\"] = u[\"location\"].fillna(\"\").astype(str).str.strip().ne(\"\").astype(np.float32)\n",
    "\n",
    "user_features = u[[\"id\", \"tweet_count\", \"z_score\", \"username_len\", \"name_len\", \"description_len\", \"has_location\"]].copy()\n",
    "account_features = account_features.merge(user_features, left_on=\"author_id\", right_on=\"id\", how=\"left\").drop(columns=[\"id\"])\n",
    "\n",
    "numeric_cols = [col for col in account_features.columns if col != \"author_id\"]\n",
    "account_features[numeric_cols] = account_features[numeric_cols].apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "# Account-level prediction\n",
    "use_second_stage = bool(globals().get(\"USE_SECOND_STAGE_ACCOUNT_MODEL\", EXPERIMENT_CONFIG.get(\"use_second_stage_account_model\", False)))\n",
    "second_stage_model = globals().get(\"second_stage_model_en\", None)\n",
    "\n",
    "if use_second_stage and second_stage_model is not None and \"feature_cols_account\" in globals():\n",
    "    for col in feature_cols_account:\n",
    "        if col not in account_features.columns:\n",
    "            account_features[col] = 0.0\n",
    "    X_acc = account_features[feature_cols_account].to_numpy(dtype=np.float32)\n",
    "\n",
    "    raw_acc_prob = second_stage_model.predict_proba(X_acc)[:, 1].astype(np.float32)\n",
    "    base_acc_prob = account_features[\"post_prob_mean\"].to_numpy(dtype=np.float32)\n",
    "    alpha = float(globals().get(\"second_stage_alpha_en\", 1.0))\n",
    "    pred_prob = (alpha * raw_acc_prob) + ((1.0 - alpha) * base_acc_prob)\n",
    "\n",
    "    threshold = float(globals().get(\"second_stage_selected_threshold_en\", post_bot_threshold))\n",
    "else:\n",
    "    threshold = float(globals().get(\"selected_threshold_ensemble\", globals().get(\"SELECTED_THRESHOLD\", post_bot_threshold)))\n",
    "    if str(globals().get(\"ACCOUNT_DECISION_RULE\", EXPERIMENT_CONFIG.get(\"account_decision_rule\", \"mean\"))) == \"any\":\n",
    "        pred_prob = (account_features[\"pred_bot_post_count\"] > 0).astype(np.float32).to_numpy()\n",
    "    else:\n",
    "        pred_prob = account_features[\"post_prob_mean\"].to_numpy(dtype=np.float32)\n",
    "\n",
    "pred_is_bot = (pred_prob >= threshold).astype(np.int64)\n",
    "\n",
    "# Optional support rules (used by French notebook; harmless defaults for English)\n",
    "min_bot_posts = int(globals().get(\"second_stage_selected_min_bot_posts_en\", EXPERIMENT_CONFIG.get(\"account_min_bot_posts\", 1)))\n",
    "min_bot_post_rate = float(globals().get(\"second_stage_selected_min_bot_post_rate_en\", EXPERIMENT_CONFIG.get(\"account_min_bot_post_rate\", 0.0)))\n",
    "if min_bot_posts > 1:\n",
    "    pred_is_bot = pred_is_bot & (account_features[\"pred_bot_post_count\"].to_numpy(dtype=np.float32) >= float(min_bot_posts))\n",
    "if min_bot_post_rate > 0:\n",
    "    denom = np.maximum(account_features[\"n_posts\"].to_numpy(dtype=np.float32), 1.0)\n",
    "    post_rate = account_features[\"pred_bot_post_count\"].to_numpy(dtype=np.float32) / denom\n",
    "    pred_is_bot = pred_is_bot & (post_rate >= float(min_bot_post_rate))\n",
    "\n",
    "account_predictions_comp = account_features[[\"author_id\"]].copy()\n",
    "account_predictions_comp[\"pred_prob\"] = pred_prob.astype(np.float32)\n",
    "account_predictions_comp[\"pred_is_bot\"] = pred_is_bot.astype(np.int64)\n",
    "\n",
    "team_name_for_file = str(TEAM_NAME).strip().replace(\" \", \"_\")\n",
    "if not team_name_for_file:\n",
    "    raise ValueError(\"TEAM_NAME cannot be empty.\")\n",
    "\n",
    "submission_txt = SUBMISSION_OUTPUT_DIR / f\"{team_name_for_file}.detections.{SUBMISSION_LANG}.txt\"\n",
    "bot_ids = account_predictions_comp.loc[account_predictions_comp[\"pred_is_bot\"] == 1, \"author_id\"].astype(str).drop_duplicates()\n",
    "submission_txt.write_text(\"\\n\".join(bot_ids) + (\"\\n\" if len(bot_ids) else \"\"))\n",
    "\n",
    "print(f\"Competition tweets file: {COMPETITION_TWEETS_FILE}\")\n",
    "print(f\"Competition users file:  {COMPETITION_USERS_FILE}\")\n",
    "print(f\"Submission file:        {submission_txt}\")\n",
    "print(f\"Team name:              {team_name_for_file}\")\n",
    "print(f\"Language:               {SUBMISSION_LANG}\")\n",
    "print(f\"Accounts flagged:       {len(bot_ids)}\")\n",
    "print(\"Format check: one author_id per line, filename matches rules [team_name].detections.[lang].txt\")\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_botsornot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
