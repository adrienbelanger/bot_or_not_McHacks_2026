{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "4fa69427",
      "metadata": {},
      "source": [
        "# McHacks 26 - Bot or Not\n",
        "\n",
        "### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "25e59269",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "from pathlib import Path\n",
        "import json\n",
        "import pandas as pd\n",
        "import re\n",
        "from transformers import AutoTokenizer\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3c78ef1e",
      "metadata": {},
      "source": [
        "### Data processing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "071ab039",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "EN sources: ['dataset.posts&users.30.json', 'dataset.posts&users.32.json']\n",
            "EN posts: 15,765 users: 546 bot_ids: 129\n",
            "FR sources: ['dataset.posts&users.31.json', 'dataset.posts&users.33.json']\n",
            "FR posts: 9,004 users: 343 bot_ids: 55\n"
          ]
        }
      ],
      "source": [
        "DATA_DIR = Path(\"data\")\n",
        "\n",
        "def get_version(path):\n",
        "    try:\n",
        "        return int(path.stem.split(\".\")[-1])\n",
        "    except ValueError:\n",
        "        return None\n",
        "\n",
        "posts_users_files = sorted(DATA_DIR.glob(\"dataset.posts&users.*.json\"), key=get_version)\n",
        "if not posts_users_files:\n",
        "    raise FileNotFoundError(\"No dataset.posts&users.*.json files found in data/\")\n",
        "\n",
        "combined = {}\n",
        "bots_by_lang = {}\n",
        "\n",
        "for path in posts_users_files:\n",
        "    with path.open() as f:\n",
        "        data = json.load(f)\n",
        "    lang = data.get(\"lang\")\n",
        "\n",
        "    combined.setdefault(lang, {\"posts\": [], \"users\": [], \"sources\": []})\n",
        "    combined[lang][\"posts\"].extend(data.get(\"posts\", []))\n",
        "    combined[lang][\"users\"].extend(data.get(\"users\", []))\n",
        "    combined[lang][\"sources\"].append(path.name)\n",
        "\n",
        "    version = get_version(path)\n",
        "    if version is not None:\n",
        "        bots_path = DATA_DIR / f\"dataset.bots.{version}.txt\"\n",
        "        if bots_path.exists():\n",
        "            bots_by_lang.setdefault(lang, set()).update(bots_path.read_text().splitlines())\n",
        "\n",
        "posts_en = pd.DataFrame(combined.get(\"en\", {}).get(\"posts\", []))\n",
        "users_en = pd.DataFrame(combined.get(\"en\", {}).get(\"users\", []))\n",
        "bot_ids_en = bots_by_lang.get(\"en\", set())\n",
        "if not users_en.empty:\n",
        "    users_en[\"is_bot\"] = users_en[\"id\"].isin(bot_ids_en)\n",
        "\n",
        "posts_fr = pd.DataFrame(combined.get(\"fr\", {}).get(\"posts\", []))\n",
        "users_fr = pd.DataFrame(combined.get(\"fr\", {}).get(\"users\", []))\n",
        "bot_ids_fr = bots_by_lang.get(\"fr\", set())\n",
        "if not users_fr.empty:\n",
        "    users_fr[\"is_bot\"] = users_fr[\"id\"].isin(bot_ids_fr)\n",
        "\n",
        "print(\"EN sources:\", combined.get(\"en\", {}).get(\"sources\", []))\n",
        "print(f\"EN posts: {len(posts_en):,} users: {len(users_en):,} bot_ids: {len(bot_ids_en):,}\")\n",
        "print(\"FR sources:\", combined.get(\"fr\", {}).get(\"sources\", []))\n",
        "print(f\"FR posts: {len(posts_fr):,} users: {len(users_fr):,} bot_ids: {len(bot_ids_fr):,}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e34ec8c",
      "metadata": {},
      "source": [
        "### Experiment parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "926640c2",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Experiment config loaded:\n",
            "- tokenizer_name: vinai/bertweet-base\n",
            "- max_length: 96\n",
            "- dedupe_users: True\n",
            "- dedupe_posts: True\n",
            "- scale_meta_features: True\n",
            "- use_topic_features: True\n",
            "- topic_match_mode: word\n",
            "- test_size: 0.2\n",
            "- random_seed: 42\n",
            "- validation_split: 0.15\n",
            "- epochs: 8\n",
            "- batch_size: 128\n",
            "- learning_rate: 0.001\n",
            "- prediction_threshold: 0.62\n",
            "- use_threshold_search: True\n",
            "- threshold_search_min: 0.1\n",
            "- threshold_search_max: 0.9\n",
            "- threshold_search_steps: 161\n",
            "- account_decision_rule: mean\n",
            "- split_by_author: True\n",
            "- ensemble_seeds: [13, 29, 42, 73, 101]\n",
            "- ensemble_aggregation: mean\n",
            "- use_external_pretrain: False\n",
            "- external_pretrain_source: fox8\n",
            "- external_pretrain_max_users: 20000\n",
            "- external_pretrain_max_tweets_per_user: 5\n",
            "- external_pretrain_max_posts: 120000\n",
            "- external_pretrain_lang: en\n",
            "- external_pretrain_sample_seed: 42\n",
            "- external_pretrain_epochs: 1\n",
            "- external_pretrain_batch_size: 128\n",
            "- external_pretrain_use_balanced_weights: True\n",
            "- use_second_stage_account_model: True\n",
            "- second_stage_profile: auto\n",
            "- second_stage_use_blend: True\n",
            "- second_stage_blend_alphas: [1.0, 0.9, 0.8, 0.7, 0.55]\n",
            "- second_stage_min_gain_vs_ensemble: 1\n",
            "- second_stage_use_oof_features: True\n",
            "- second_stage_oof_folds: 4\n",
            "- second_stage_oof_epochs: 4\n",
            "- second_stage_oof_batch_size: 128\n",
            "- second_stage_oof_seeds: [13, 42]\n",
            "- second_stage_oof_use_external_pretrain: False\n",
            "- second_stage_learning_rate: 0.05\n",
            "- second_stage_max_iter: 300\n",
            "- second_stage_max_depth: 4\n",
            "- second_stage_l2: 0.2\n",
            "- second_stage_min_data_in_leaf: 20\n",
            "- second_stage_subsample: 0.8\n",
            "- second_stage_rsm: 0.8\n",
            "- second_stage_od_wait: 50\n",
            "- second_stage_use_balanced_weights: True\n",
            "- use_class_weights: False\n",
            "- embedding_dim: 96\n",
            "- gru_units: 48\n",
            "- aux_dense_units: 32\n",
            "- head_dense_units: 48\n",
            "- dropout_text: 0.4\n",
            "- dropout_aux: 0.3\n",
            "- dropout_head: 0.4\n",
            "- early_stopping_patience: 1\n",
            "- reduce_lr_patience: 1\n",
            "- reduce_lr_factor: 0.5\n",
            "- reduce_lr_min_lr: 1e-05\n",
            "- error_analysis_min_topic_posts: 20\n",
            "- error_analysis_min_topic_accounts: 5\n"
          ]
        }
      ],
      "source": [
        "EXPERIMENT_CONFIG = {\n",
        "    \"tokenizer_name\": \"vinai/bertweet-base\",\n",
        "    \"max_length\": 96,\n",
        "    \"dedupe_users\": True,\n",
        "    \"dedupe_posts\": True,\n",
        "    \"scale_meta_features\": True,\n",
        "    \"use_topic_features\": True,\n",
        "    \"topic_match_mode\": \"word\",  # options: \"contains\", \"word\"\n",
        "    \"test_size\": 0.20,\n",
        "    \"random_seed\": 42,\n",
        "    \"validation_split\": 0.15,\n",
        "    \"epochs\": 8,\n",
        "    \"batch_size\": 128,\n",
        "    \"learning_rate\": 0.001,\n",
        "    \"prediction_threshold\": 0.62,\n",
        "    \"use_threshold_search\": True,\n",
        "    \"threshold_search_min\": 0.10,\n",
        "    \"threshold_search_max\": 0.90,\n",
        "    \"threshold_search_steps\": 161,\n",
        "    \"account_decision_rule\": \"mean\",  # options: \"mean\", \"any\"\n",
        "    \"split_by_author\": True,\n",
        "    \"ensemble_seeds\": [13, 29, 42, 73, 101],\n",
        "    \"ensemble_aggregation\": \"mean\",  # options: \"mean\", \"median\"\n",
        "    \"use_external_pretrain\": False,\n",
        "    \"external_pretrain_source\": \"fox8\",\n",
        "    \"external_pretrain_max_users\": 20000,\n",
        "    \"external_pretrain_max_tweets_per_user\": 5,\n",
        "    \"external_pretrain_max_posts\": 120000,\n",
        "    \"external_pretrain_lang\": \"en\",\n",
        "    \"external_pretrain_sample_seed\": 42,\n",
        "    \"external_pretrain_epochs\": 1,\n",
        "    \"external_pretrain_batch_size\": 128,\n",
        "    \"external_pretrain_use_balanced_weights\": True,\n",
        "    \"use_second_stage_account_model\": True,\n",
        "    \"second_stage_profile\": \"auto\",  # options: \"auto\", \"legacy\", \"regularized\", \"custom\"\n",
        "    \"second_stage_use_blend\": True,\n",
        "    \"second_stage_blend_alphas\": [1.0, 0.90, 0.80, 0.70, 0.55],\n",
        "    \"second_stage_min_gain_vs_ensemble\": 1,\n",
        "    \"second_stage_use_oof_features\": True,\n",
        "    \"second_stage_oof_folds\": 4,\n",
        "    \"second_stage_oof_epochs\": 4,\n",
        "    \"second_stage_oof_batch_size\": 128,\n",
        "    \"second_stage_oof_seeds\": [13, 42],\n",
        "    \"second_stage_oof_use_external_pretrain\": False,\n",
        "    \"second_stage_learning_rate\": 0.05,\n",
        "    \"second_stage_max_iter\": 300,\n",
        "    \"second_stage_max_depth\": 4,\n",
        "    \"second_stage_l2\": 0.2,\n",
        "    \"second_stage_min_data_in_leaf\": 20,\n",
        "    \"second_stage_subsample\": 0.8,\n",
        "    \"second_stage_rsm\": 0.8,\n",
        "    \"second_stage_od_wait\": 50,\n",
        "    \"second_stage_use_balanced_weights\": True,\n",
        "    \"use_class_weights\": False,\n",
        "    \"embedding_dim\": 96,\n",
        "    \"gru_units\": 48,\n",
        "    \"aux_dense_units\": 32,\n",
        "    \"head_dense_units\": 48,\n",
        "    \"dropout_text\": 0.40,\n",
        "    \"dropout_aux\": 0.30,\n",
        "    \"dropout_head\": 0.40,\n",
        "    \"early_stopping_patience\": 1,\n",
        "    \"reduce_lr_patience\": 1,\n",
        "    \"reduce_lr_factor\": 0.50,\n",
        "    \"reduce_lr_min_lr\": 1e-5,\n",
        "    \"error_analysis_min_topic_posts\": 20,\n",
        "    \"error_analysis_min_topic_accounts\": 5,\n",
        "}\n",
        "\n",
        "print(\"Experiment config loaded:\")\n",
        "for key, value in EXPERIMENT_CONFIG.items():\n",
        "    print(f\"- {key}: {value}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "37411cfe",
      "metadata": {},
      "source": [
        "### Tokenizing the text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "a1288615",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenizer: vinai/bertweet-base\n",
            "English labeled posts: 15,765\n",
            "Token tensor shape: (15765, 96)\n",
            "Meta feature shape: (15765, 7), label shape: (15765,)\n",
            "Dedupe users/posts: True/True\n",
            "Scale metadata features: True\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "try:\n",
        "    from transformers import AutoTokenizer\n",
        "except ImportError as exc:\n",
        "    raise ImportError(\"Install transformers first: pip install transformers\") from exc\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "if \"EXPERIMENT_CONFIG\" not in globals():\n",
        "    raise ValueError(\"Run the Experiment parameters cell first.\")\n",
        "\n",
        "TOKENIZER_NAME = str(EXPERIMENT_CONFIG[\"tokenizer_name\"])\n",
        "MAX_LENGTH = int(EXPERIMENT_CONFIG[\"max_length\"])\n",
        "DEDUPE_USERS = bool(EXPERIMENT_CONFIG[\"dedupe_users\"])\n",
        "DEDUPE_POSTS = bool(EXPERIMENT_CONFIG[\"dedupe_posts\"])\n",
        "SCALE_META_FEATURES = bool(EXPERIMENT_CONFIG[\"scale_meta_features\"])\n",
        "\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.replace(\"\\n\", \" \").strip()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    return text\n",
        "\n",
        "def add_text_features(df):\n",
        "    out = df.copy()\n",
        "    out[\"text_clean\"] = out[\"text\"].fillna(\"\").map(clean_text)\n",
        "    out[\"char_count\"] = out[\"text_clean\"].str.len()\n",
        "    out[\"word_count\"] = out[\"text_clean\"].str.split().str.len()\n",
        "    out[\"url_count\"] = out[\"text_clean\"].str.count(r\"https?://\\S+|www\\.\\S+\")\n",
        "    out[\"mention_count\"] = out[\"text_clean\"].str.count(r\"@\\w+\")\n",
        "    out[\"hashtag_count\"] = out[\"text_clean\"].str.count(r\"#\\w+\")\n",
        "    out[\"exclamation_count\"] = out[\"text_clean\"].str.count(r\"!\")\n",
        "    out[\"question_count\"] = out[\"text_clean\"].str.count(r\"\\?\")\n",
        "    return out\n",
        "\n",
        "if posts_en.empty or users_en.empty:\n",
        "    raise ValueError(\"Run the data processing cell first to load English data.\")\n",
        "\n",
        "users_en_labeled = (\n",
        "    users_en.drop_duplicates(subset=\"id\", keep=\"last\").copy()\n",
        "    if DEDUPE_USERS\n",
        "    else users_en.copy()\n",
        ")\n",
        "posts_en_unique = (\n",
        "    posts_en.drop_duplicates(subset=\"id\", keep=\"last\").copy()\n",
        "    if DEDUPE_POSTS\n",
        "    else posts_en.copy()\n",
        ")\n",
        "\n",
        "label_map_en = users_en_labeled.set_index(\"id\")[\"is_bot\"]\n",
        "train_en = posts_en_unique.copy()\n",
        "train_en[\"is_bot\"] = train_en[\"author_id\"].map(label_map_en)\n",
        "train_en = train_en.dropna(subset=[\"is_bot\"]).copy()\n",
        "train_en[\"is_bot\"] = train_en[\"is_bot\"].astype(\"int64\")\n",
        "\n",
        "train_en = add_text_features(train_en)\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, use_fast=True)\n",
        "encodings_en = tokenizer(\n",
        "    train_en[\"text_clean\"].tolist(),\n",
        "    truncation=True,\n",
        "    padding=\"max_length\",\n",
        "    max_length=MAX_LENGTH,\n",
        "    return_attention_mask=True,\n",
        ")\n",
        "\n",
        "feature_cols_en = [\n",
        "    \"char_count\",\n",
        "    \"word_count\",\n",
        "    \"url_count\",\n",
        "    \"mention_count\",\n",
        "    \"hashtag_count\",\n",
        "    \"exclamation_count\",\n",
        "    \"question_count\",\n",
        "]\n",
        "X_meta_en = train_en[feature_cols_en].to_numpy(dtype=np.float32)\n",
        "y_en = train_en[\"is_bot\"].to_numpy(dtype=np.int64)\n",
        "\n",
        "if SCALE_META_FEATURES:\n",
        "    scaler_en = StandardScaler()\n",
        "    X_meta_en_scaled = scaler_en.fit_transform(X_meta_en).astype(np.float32)\n",
        "else:\n",
        "    scaler_en = None\n",
        "    X_meta_en_scaled = X_meta_en.copy()\n",
        "\n",
        "print(f\"Tokenizer: {TOKENIZER_NAME}\")\n",
        "print(f\"English labeled posts: {len(train_en):,}\")\n",
        "print(f\"Token tensor shape: {np.asarray(encodings_en['input_ids']).shape}\")\n",
        "print(f\"Meta feature shape: {X_meta_en_scaled.shape}, label shape: {y_en.shape}\")\n",
        "print(f\"Dedupe users/posts: {DEDUPE_USERS}/{DEDUPE_POSTS}\")\n",
        "print(f\"Scale metadata features: {SCALE_META_FEATURES}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b9f8c2a",
      "metadata": {},
      "source": [
        "## External data pretraining"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "4c8c2c6a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "External pretraining disabled. Set use_external_pretrain=True to enable.\n"
          ]
        }
      ],
      "source": [
        "from pathlib import Path\n",
        "import json\n",
        "import random\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "if \"EXPERIMENT_CONFIG\" not in globals():\n",
        "    raise ValueError(\"Run the Experiment parameters cell first.\")\n",
        "if \"tokenizer\" not in globals() or \"add_text_features\" not in globals():\n",
        "    raise ValueError(\"Run the Tokenizing the text cell first.\")\n",
        "\n",
        "USE_EXTERNAL_PRETRAIN = bool(EXPERIMENT_CONFIG.get(\"use_external_pretrain\", False))\n",
        "if not USE_EXTERNAL_PRETRAIN:\n",
        "    print(\"External pretraining disabled. Set use_external_pretrain=True to enable.\")\n",
        "else:\n",
        "    external_source = str(EXPERIMENT_CONFIG.get(\"external_pretrain_source\", \"fox8\")).lower()\n",
        "    max_users = int(EXPERIMENT_CONFIG.get(\"external_pretrain_max_users\", 20000))\n",
        "    max_posts = int(EXPERIMENT_CONFIG.get(\"external_pretrain_max_posts\", 120000))\n",
        "    max_tweets_per_user = int(EXPERIMENT_CONFIG.get(\"external_pretrain_max_tweets_per_user\", 5))\n",
        "    lang_filter = str(EXPERIMENT_CONFIG.get(\"external_pretrain_lang\", \"en\")).lower().strip()\n",
        "    sample_seed = int(EXPERIMENT_CONFIG.get(\"external_pretrain_sample_seed\", 42))\n",
        "\n",
        "    random.seed(sample_seed)\n",
        "\n",
        "    data_dir = Path(\"external_data\")\n",
        "    if external_source != \"fox8\":\n",
        "        raise ValueError(\"Only fox8_23_dataset.ndjson is supported for pretraining right now.\")\n",
        "\n",
        "    fox8_path = data_dir / \"fox8_23_dataset.ndjson\"\n",
        "    if not fox8_path.exists():\n",
        "        raise FileNotFoundError(f\"Missing {fox8_path}\")\n",
        "\n",
        "    # Reservoir sample users to avoid loading the full file.\n",
        "    sampled_users = []\n",
        "    with fox8_path.open() as f:\n",
        "        for i, line in enumerate(f):\n",
        "            try:\n",
        "                obj = json.loads(line)\n",
        "            except json.JSONDecodeError:\n",
        "                continue\n",
        "            if len(sampled_users) < max_users:\n",
        "                sampled_users.append(obj)\n",
        "            else:\n",
        "                j = random.randint(0, i)\n",
        "                if j < max_users:\n",
        "                    sampled_users[j] = obj\n",
        "\n",
        "    rows = []\n",
        "    for user in sampled_users:\n",
        "        label = str(user.get(\"label\", \"\")).strip().lower()\n",
        "        if not label:\n",
        "            continue\n",
        "        if label in {\"human\", \"normal\"}:\n",
        "            is_bot = 0\n",
        "        elif label in {\"bot\", \"spambot\", \"fake\", \"automated\"}:\n",
        "            is_bot = 1\n",
        "        else:\n",
        "            # Unknown label; skip\n",
        "            continue\n",
        "\n",
        "        user_id = user.get(\"user_id\")\n",
        "        tweets = user.get(\"user_tweets\", []) or []\n",
        "\n",
        "        per_user_count = 0\n",
        "        for tweet in tweets:\n",
        "            if len(rows) >= max_posts or per_user_count >= max_tweets_per_user:\n",
        "                break\n",
        "            text = tweet.get(\"text\") if isinstance(tweet, dict) else None\n",
        "            if not text:\n",
        "                continue\n",
        "            if lang_filter:\n",
        "                tweet_lang = str(tweet.get(\"lang\", \"\")).lower().strip() if isinstance(tweet, dict) else \"\"\n",
        "                if tweet_lang and tweet_lang != lang_filter:\n",
        "                    continue\n",
        "            rows.append(\n",
        "                {\n",
        "                    \"author_id\": f\"fox8_{user_id}\",\n",
        "                    \"text\": text,\n",
        "                    \"is_bot\": is_bot,\n",
        "                }\n",
        "            )\n",
        "            per_user_count += 1\n",
        "\n",
        "    external_pretrain_df = pd.DataFrame(rows)\n",
        "    if external_pretrain_df.empty:\n",
        "        raise ValueError(\"No external pretrain rows built. Check labels/lang filter.\")\n",
        "\n",
        "    external_pretrain_df = external_pretrain_df.dropna(subset=[\"text\"]).reset_index(drop=True)\n",
        "    external_pretrain_df = add_text_features(external_pretrain_df)\n",
        "\n",
        "    external_pretrain_encodings = tokenizer(\n",
        "        external_pretrain_df[\"text_clean\"].tolist(),\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_LENGTH,\n",
        "        return_attention_mask=True,\n",
        "    )\n",
        "\n",
        "    external_pretrain_input_ids = np.asarray(external_pretrain_encodings[\"input_ids\"], dtype=np.int32)\n",
        "    external_pretrain_attention_mask = np.asarray(external_pretrain_encodings[\"attention_mask\"], dtype=np.float32)\n",
        "\n",
        "    external_pretrain_meta = external_pretrain_df[feature_cols_en].to_numpy(dtype=np.float32)\n",
        "    if \"scaler_en\" in globals() and SCALE_META_FEATURES:\n",
        "        external_pretrain_meta_scaled = scaler_en.transform(external_pretrain_meta).astype(np.float32)\n",
        "    else:\n",
        "        external_pretrain_meta_scaled = external_pretrain_meta\n",
        "\n",
        "    external_pretrain_labels = external_pretrain_df[\"is_bot\"].to_numpy(dtype=np.int64)\n",
        "\n",
        "    print(\"External pretrain dataset prepared:\")\n",
        "    print(f\"- source: {external_source}\")\n",
        "    print(f\"- rows: {len(external_pretrain_df):,}\")\n",
        "    print(\n",
        "        f\"- bots: {int((external_pretrain_labels==1).sum())}, humans: {int((external_pretrain_labels==0).sum())}\"\n",
        "    )\n",
        "    print(f\"- max_length: {MAX_LENGTH}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "251cded3",
      "metadata": {},
      "source": [
        "## Train-Test split for model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "c8c62216",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/8\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-08 17:40:36.990595: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 78ms/step - accuracy: 0.6519 - auc: 0.5759 - loss: 0.6292"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-08 17:40:46.159449: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 87ms/step - accuracy: 0.7262 - auc: 0.7286 - loss: 0.5578 - val_accuracy: 0.8564 - val_auc: 0.8224 - val_loss: 0.4052 - learning_rate: 0.0010\n",
            "Epoch 2/8\n",
            "\u001b[1m81/81\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 114ms/step - accuracy: 0.8954 - auc: 0.9120 - loss: 0.2983 - val_accuracy: 0.8540 - val_auc: 0.8043 - val_loss: 0.4422 - learning_rate: 0.0010\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-08 17:40:56.092418: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Split mode: author\n",
            "Topic features enabled: True\n",
            "Topic match mode: word\n",
            "Account decision rule: mean\n",
            "Topic columns: ['topic_pop', 'topic_nba', 'topic_movies', 'topic_nhl']\n",
            "Split sizes (fit/val/test posts): 10318/2041/3406\n",
            "Split sizes (fit/val/test accounts): 370/66/110\n",
            "Default threshold from config: 0.6200\n",
            "Selected threshold used on test: 0.3750\n",
            "Best validation score from threshold search: 45\n",
            "Test Accuracy (post-level): 0.8388\n",
            "Test ROC-AUC (post-level): 0.8676\n",
            "Test account score @ selected threshold -> score=65, TP=19, FN=7, FP=2, accounts=110\n",
            "Test account score @ config threshold -> score=39, TP=13, FN=13, FP=0, accounts=110\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     0.9025    0.8739    0.8880      2490\n",
            "           1     0.6844    0.7434    0.7127       916\n",
            "\n",
            "    accuracy                         0.8388      3406\n",
            "   macro avg     0.7935    0.8087    0.8003      3406\n",
            "weighted avg     0.8439    0.8388    0.8408      3406\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "try:\n",
        "    import tensorflow as tf\n",
        "except ImportError as exc:\n",
        "    raise ImportError(\"Install tensorflow first: pip install tensorflow\") from exc\n",
        "\n",
        "if \"EXPERIMENT_CONFIG\" not in globals():\n",
        "    raise ValueError(\"Run the Experiment parameters cell first.\")\n",
        "\n",
        "TEST_SIZE = float(EXPERIMENT_CONFIG[\"test_size\"])\n",
        "RANDOM_SEED = int(EXPERIMENT_CONFIG[\"random_seed\"])\n",
        "VALIDATION_SPLIT = float(EXPERIMENT_CONFIG[\"validation_split\"])\n",
        "EPOCHS = int(EXPERIMENT_CONFIG[\"epochs\"])\n",
        "BATCH_SIZE = int(EXPERIMENT_CONFIG[\"batch_size\"])\n",
        "LEARNING_RATE = float(EXPERIMENT_CONFIG[\"learning_rate\"])\n",
        "PREDICTION_THRESHOLD = float(EXPERIMENT_CONFIG[\"prediction_threshold\"])\n",
        "USE_CLASS_WEIGHTS = bool(EXPERIMENT_CONFIG[\"use_class_weights\"])\n",
        "USE_TOPIC_FEATURES = bool(EXPERIMENT_CONFIG[\"use_topic_features\"])\n",
        "TOPIC_MATCH_MODE = str(EXPERIMENT_CONFIG[\"topic_match_mode\"])\n",
        "SPLIT_BY_AUTHOR = bool(EXPERIMENT_CONFIG.get(\"split_by_author\", True))\n",
        "\n",
        "USE_THRESHOLD_SEARCH = bool(EXPERIMENT_CONFIG.get(\"use_threshold_search\", False))\n",
        "THRESHOLD_SEARCH_MIN = float(EXPERIMENT_CONFIG.get(\"threshold_search_min\", 0.10))\n",
        "THRESHOLD_SEARCH_MAX = float(EXPERIMENT_CONFIG.get(\"threshold_search_max\", 0.90))\n",
        "THRESHOLD_SEARCH_STEPS = int(EXPERIMENT_CONFIG.get(\"threshold_search_steps\", 81))\n",
        "ACCOUNT_DECISION_RULE = str(EXPERIMENT_CONFIG.get(\"account_decision_rule\", \"mean\"))\n",
        "\n",
        "EMBEDDING_DIM = int(EXPERIMENT_CONFIG[\"embedding_dim\"])\n",
        "GRU_UNITS = int(EXPERIMENT_CONFIG[\"gru_units\"])\n",
        "AUX_DENSE_UNITS = int(EXPERIMENT_CONFIG[\"aux_dense_units\"])\n",
        "HEAD_DENSE_UNITS = int(EXPERIMENT_CONFIG[\"head_dense_units\"])\n",
        "DROPOUT_TEXT = float(EXPERIMENT_CONFIG[\"dropout_text\"])\n",
        "DROPOUT_AUX = float(EXPERIMENT_CONFIG[\"dropout_aux\"])\n",
        "DROPOUT_HEAD = float(EXPERIMENT_CONFIG[\"dropout_head\"])\n",
        "\n",
        "EARLY_STOPPING_PATIENCE = int(EXPERIMENT_CONFIG[\"early_stopping_patience\"])\n",
        "REDUCE_LR_PATIENCE = int(EXPERIMENT_CONFIG[\"reduce_lr_patience\"])\n",
        "REDUCE_LR_FACTOR = float(EXPERIMENT_CONFIG[\"reduce_lr_factor\"])\n",
        "REDUCE_LR_MIN_LR = float(EXPERIMENT_CONFIG[\"reduce_lr_min_lr\"])\n",
        "\n",
        "if not (0.0 < TEST_SIZE < 1.0):\n",
        "    raise ValueError(\"test_size must be between 0 and 1.\")\n",
        "if not (0.0 <= VALIDATION_SPLIT < 1.0):\n",
        "    raise ValueError(\"validation_split must be in [0, 1).\")\n",
        "if TOPIC_MATCH_MODE not in {\"contains\", \"word\"}:\n",
        "    raise ValueError('topic_match_mode must be \"contains\" or \"word\".')\n",
        "if ACCOUNT_DECISION_RULE not in {\"mean\", \"any\"}:\n",
        "    raise ValueError('account_decision_rule must be \"mean\" or \"any\".')\n",
        "if USE_THRESHOLD_SEARCH:\n",
        "    if not (0.0 < THRESHOLD_SEARCH_MIN < 1.0 and 0.0 < THRESHOLD_SEARCH_MAX < 1.0):\n",
        "        raise ValueError(\"threshold_search_min and threshold_search_max must be in (0, 1).\")\n",
        "    if THRESHOLD_SEARCH_MIN >= THRESHOLD_SEARCH_MAX:\n",
        "        raise ValueError(\"threshold_search_min must be smaller than threshold_search_max.\")\n",
        "    if THRESHOLD_SEARCH_STEPS < 2:\n",
        "        raise ValueError(\"threshold_search_steps must be >= 2.\")\n",
        "\n",
        "\n",
        "def load_english_topic_keywords():\n",
        "    topic_keywords = {}\n",
        "    for source_name in combined.get(\"en\", {}).get(\"sources\", []):\n",
        "        source_path = DATA_DIR / source_name\n",
        "        with source_path.open() as f:\n",
        "            payload = json.load(f)\n",
        "        for topic_item in payload.get(\"metadata\", {}).get(\"topics\", []):\n",
        "            topic = str(topic_item.get(\"topic\", \"\")).strip().lower()\n",
        "            if not topic:\n",
        "                continue\n",
        "            keywords = {\n",
        "                str(keyword).strip().lower()\n",
        "                for keyword in topic_item.get(\"keywords\", [])\n",
        "                if str(keyword).strip()\n",
        "            }\n",
        "            keywords.add(topic)\n",
        "            topic_keywords.setdefault(topic, set()).update(keywords)\n",
        "    return {topic: sorted(values, key=len, reverse=True) for topic, values in topic_keywords.items()}\n",
        "\n",
        "\n",
        "def add_topic_features(df, topic_keywords, match_mode):\n",
        "    out = df.copy()\n",
        "    text_lower = out[\"text_clean\"].str.lower()\n",
        "    topic_cols = []\n",
        "    for topic, keywords in topic_keywords.items():\n",
        "        col = f\"topic_{topic}\"\n",
        "        topic_cols.append(col)\n",
        "        if not keywords:\n",
        "            out[col] = 0\n",
        "            continue\n",
        "        if match_mode == \"word\":\n",
        "            pattern = \"|\".join(rf\"\\\\b{re.escape(keyword)}\\\\b\" for keyword in keywords)\n",
        "        else:\n",
        "            pattern = \"|\".join(re.escape(keyword) for keyword in keywords)\n",
        "        out[col] = text_lower.str.contains(pattern, regex=True).astype(np.int8)\n",
        "    return out, topic_cols\n",
        "\n",
        "\n",
        "def compute_account_score(author_ids, true_labels, pred_probs, threshold, decision_rule):\n",
        "    tmp = pd.DataFrame(\n",
        "        {\n",
        "            \"author_id\": author_ids,\n",
        "            \"true_is_bot\": np.asarray(true_labels, dtype=np.int64),\n",
        "            \"pred_prob\": np.asarray(pred_probs, dtype=np.float32),\n",
        "        }\n",
        "    )\n",
        "    tmp[\"pred_post\"] = (tmp[\"pred_prob\"] >= threshold).astype(np.int64)\n",
        "\n",
        "    account = (\n",
        "        tmp.groupby(\"author_id\", as_index=False)\n",
        "        .agg(\n",
        "            true_is_bot=(\"true_is_bot\", \"max\"),\n",
        "            mean_prob=(\"pred_prob\", \"mean\"),\n",
        "            any_pred=(\"pred_post\", \"max\"),\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if decision_rule == \"any\":\n",
        "        account[\"pred_is_bot\"] = account[\"any_pred\"].astype(np.int64)\n",
        "    else:\n",
        "        account[\"pred_is_bot\"] = (account[\"mean_prob\"] >= threshold).astype(np.int64)\n",
        "\n",
        "    tp_accounts = int(((account[\"true_is_bot\"] == 1) & (account[\"pred_is_bot\"] == 1)).sum())\n",
        "    fn_accounts = int(((account[\"true_is_bot\"] == 1) & (account[\"pred_is_bot\"] == 0)).sum())\n",
        "    fp_accounts = int(((account[\"true_is_bot\"] == 0) & (account[\"pred_is_bot\"] == 1)).sum())\n",
        "\n",
        "    score = (4 * tp_accounts) - (1 * fn_accounts) - (2 * fp_accounts)\n",
        "    return score, tp_accounts, fn_accounts, fp_accounts, len(account)\n",
        "\n",
        "\n",
        "def predict_for_indices(model, post_indices):\n",
        "    if len(post_indices) == 0:\n",
        "        return np.array([], dtype=np.float32)\n",
        "    return model.predict(\n",
        "        {\n",
        "            \"input_ids\": input_ids_en[post_indices],\n",
        "            \"attention_mask\": attention_mask_en[post_indices],\n",
        "            \"aux_features\": X_aux_en[post_indices],\n",
        "        },\n",
        "        verbose=0,\n",
        "    ).ravel()\n",
        "\n",
        "\n",
        "if USE_TOPIC_FEATURES:\n",
        "    topic_keywords_en = load_english_topic_keywords()\n",
        "    train_en_model, topic_feature_cols_en = add_topic_features(train_en, topic_keywords_en, TOPIC_MATCH_MODE)\n",
        "else:\n",
        "    train_en_model = train_en.copy()\n",
        "    topic_feature_cols_en = []\n",
        "\n",
        "input_ids_en = np.asarray(encodings_en[\"input_ids\"], dtype=np.int32)\n",
        "attention_mask_en = np.asarray(encodings_en[\"attention_mask\"], dtype=np.float32)\n",
        "X_topic_en = (\n",
        "    train_en_model[topic_feature_cols_en].to_numpy(dtype=np.float32)\n",
        "    if topic_feature_cols_en\n",
        "    else np.zeros((len(train_en_model), 0), dtype=np.float32)\n",
        ")\n",
        "X_aux_en = np.concatenate([X_meta_en_scaled, X_topic_en], axis=1)\n",
        "\n",
        "all_post_idx = np.arange(len(y_en))\n",
        "\n",
        "if SPLIT_BY_AUTHOR:\n",
        "    author_labels_df = (\n",
        "        train_en_model.groupby(\"author_id\", as_index=False)[\"is_bot\"]\n",
        "        .max()\n",
        "        .rename(columns={\"is_bot\": \"account_is_bot\"})\n",
        "    )\n",
        "    all_authors = author_labels_df[\"author_id\"].to_numpy()\n",
        "    all_author_labels = author_labels_df[\"account_is_bot\"].to_numpy(dtype=np.int64)\n",
        "\n",
        "    train_authors, test_authors = train_test_split(\n",
        "        all_authors,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=RANDOM_SEED,\n",
        "        stratify=all_author_labels,\n",
        "    )\n",
        "\n",
        "    if VALIDATION_SPLIT > 0:\n",
        "        train_author_labels = (\n",
        "            author_labels_df.set_index(\"author_id\").loc[train_authors, \"account_is_bot\"].to_numpy(dtype=np.int64)\n",
        "        )\n",
        "        fit_authors, val_authors = train_test_split(\n",
        "            train_authors,\n",
        "            test_size=VALIDATION_SPLIT,\n",
        "            random_state=RANDOM_SEED,\n",
        "            stratify=train_author_labels,\n",
        "        )\n",
        "    else:\n",
        "        fit_authors = train_authors\n",
        "        val_authors = np.array([], dtype=all_authors.dtype)\n",
        "\n",
        "    fit_author_set = set(fit_authors.tolist())\n",
        "    val_author_set = set(val_authors.tolist())\n",
        "    test_author_set = set(test_authors.tolist())\n",
        "\n",
        "    fit_idx = np.flatnonzero(train_en_model[\"author_id\"].isin(fit_author_set).to_numpy())\n",
        "    val_idx = np.flatnonzero(train_en_model[\"author_id\"].isin(val_author_set).to_numpy())\n",
        "    test_idx = np.flatnonzero(train_en_model[\"author_id\"].isin(test_author_set).to_numpy())\n",
        "    split_mode = \"author\"\n",
        "else:\n",
        "    train_idx, test_idx = train_test_split(\n",
        "        all_post_idx,\n",
        "        test_size=TEST_SIZE,\n",
        "        random_state=RANDOM_SEED,\n",
        "        stratify=y_en,\n",
        "    )\n",
        "\n",
        "    if VALIDATION_SPLIT > 0:\n",
        "        fit_idx, val_idx = train_test_split(\n",
        "            train_idx,\n",
        "            test_size=VALIDATION_SPLIT,\n",
        "            random_state=RANDOM_SEED,\n",
        "            stratify=y_en[train_idx],\n",
        "        )\n",
        "    else:\n",
        "        fit_idx = train_idx\n",
        "        val_idx = np.array([], dtype=np.int64)\n",
        "\n",
        "    split_mode = \"post\"\n",
        "\n",
        "X_fit_ids, X_test_ids = input_ids_en[fit_idx], input_ids_en[test_idx]\n",
        "X_fit_mask, X_test_mask = attention_mask_en[fit_idx], attention_mask_en[test_idx]\n",
        "X_fit_aux, X_test_aux = X_aux_en[fit_idx], X_aux_en[test_idx]\n",
        "y_fit, y_test = y_en[fit_idx], y_en[test_idx]\n",
        "\n",
        "X_val_ids = input_ids_en[val_idx] if len(val_idx) else None\n",
        "X_val_mask = attention_mask_en[val_idx] if len(val_idx) else None\n",
        "X_val_aux = X_aux_en[val_idx] if len(val_idx) else None\n",
        "y_val = y_en[val_idx] if len(val_idx) else None\n",
        "\n",
        "fit_author_ids = np.unique(train_en_model.iloc[fit_idx][\"author_id\"].to_numpy())\n",
        "val_author_ids = np.unique(train_en_model.iloc[val_idx][\"author_id\"].to_numpy()) if len(val_idx) else np.array([])\n",
        "test_author_ids = np.unique(train_en_model.iloc[test_idx][\"author_id\"].to_numpy())\n",
        "\n",
        "class_weight_dict = None\n",
        "if USE_CLASS_WEIGHTS:\n",
        "    classes = np.unique(y_fit)\n",
        "    class_weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_fit)\n",
        "    class_weight_dict = {int(c): float(w) for c, w in zip(classes, class_weights)}\n",
        "\n",
        "\n",
        "def build_multifeature_model(\n",
        "    vocab_size,\n",
        "    seq_len,\n",
        "    aux_dim,\n",
        "    embedding_dim,\n",
        "    gru_units,\n",
        "    aux_dense_units,\n",
        "    head_dense_units,\n",
        "    dropout_text,\n",
        "    dropout_aux,\n",
        "    dropout_head,\n",
        "    learning_rate,\n",
        "):\n",
        "    ids_input = tf.keras.layers.Input(shape=(seq_len,), dtype=\"int32\", name=\"input_ids\")\n",
        "    mask_input = tf.keras.layers.Input(shape=(seq_len,), dtype=\"float32\", name=\"attention_mask\")\n",
        "    aux_input = tf.keras.layers.Input(shape=(aux_dim,), dtype=\"float32\", name=\"aux_features\")\n",
        "\n",
        "    x = tf.keras.layers.Embedding(vocab_size, embedding_dim, name=\"token_embedding\")(ids_input)\n",
        "    mask = tf.keras.layers.Reshape((seq_len, 1))(mask_input)\n",
        "    x = tf.keras.layers.Multiply()([x, mask])\n",
        "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(gru_units))(x)\n",
        "    x = tf.keras.layers.Dropout(dropout_text)(x)\n",
        "\n",
        "    aux = tf.keras.layers.Dense(aux_dense_units, activation=\"relu\")(aux_input)\n",
        "    aux = tf.keras.layers.Dropout(dropout_aux)(aux)\n",
        "\n",
        "    merged = tf.keras.layers.Concatenate()([x, aux])\n",
        "    merged = tf.keras.layers.Dense(head_dense_units, activation=\"relu\")(merged)\n",
        "    merged = tf.keras.layers.Dropout(dropout_head)(merged)\n",
        "    output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(merged)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[ids_input, mask_input, aux_input], outputs=output)\n",
        "    model.compile(\n",
        "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
        "        loss=\"binary_crossentropy\",\n",
        "        metrics=[tf.keras.metrics.BinaryAccuracy(name=\"accuracy\"), tf.keras.metrics.AUC(name=\"auc\")],\n",
        "    )\n",
        "    return model\n",
        "\n",
        "\n",
        "model_en = build_multifeature_model(\n",
        "    vocab_size=tokenizer.vocab_size,\n",
        "    seq_len=MAX_LENGTH,\n",
        "    aux_dim=X_fit_aux.shape[1],\n",
        "    embedding_dim=EMBEDDING_DIM,\n",
        "    gru_units=GRU_UNITS,\n",
        "    aux_dense_units=AUX_DENSE_UNITS,\n",
        "    head_dense_units=HEAD_DENSE_UNITS,\n",
        "    dropout_text=DROPOUT_TEXT,\n",
        "    dropout_aux=DROPOUT_AUX,\n",
        "    dropout_head=DROPOUT_HEAD,\n",
        "    learning_rate=LEARNING_RATE,\n",
        ")\n",
        "\n",
        "has_validation = len(val_idx) > 0\n",
        "callbacks = []\n",
        "if has_validation and EARLY_STOPPING_PATIENCE > 0:\n",
        "    callbacks.append(\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor=\"val_auc\",\n",
        "            mode=\"max\",\n",
        "            patience=EARLY_STOPPING_PATIENCE,\n",
        "            restore_best_weights=True,\n",
        "        )\n",
        "    )\n",
        "if has_validation and REDUCE_LR_PATIENCE > 0:\n",
        "    callbacks.append(\n",
        "        tf.keras.callbacks.ReduceLROnPlateau(\n",
        "            monitor=\"val_auc\",\n",
        "            mode=\"max\",\n",
        "            factor=REDUCE_LR_FACTOR,\n",
        "            patience=REDUCE_LR_PATIENCE,\n",
        "            min_lr=REDUCE_LR_MIN_LR,\n",
        "        )\n",
        "    )\n",
        "\n",
        "train_inputs = {\n",
        "    \"input_ids\": X_fit_ids,\n",
        "    \"attention_mask\": X_fit_mask,\n",
        "    \"aux_features\": X_fit_aux,\n",
        "}\n",
        "\n",
        "fit_kwargs = {\n",
        "    \"epochs\": EPOCHS,\n",
        "    \"batch_size\": BATCH_SIZE,\n",
        "    \"class_weight\": class_weight_dict,\n",
        "    \"callbacks\": callbacks,\n",
        "    \"verbose\": 1,\n",
        "}\n",
        "if has_validation:\n",
        "    fit_kwargs[\"validation_data\"] = (\n",
        "        {\n",
        "            \"input_ids\": X_val_ids,\n",
        "            \"attention_mask\": X_val_mask,\n",
        "            \"aux_features\": X_val_aux,\n",
        "        },\n",
        "        y_val,\n",
        "    )\n",
        "\n",
        "history_en = model_en.fit(train_inputs, y_fit, **fit_kwargs)\n",
        "\n",
        "post_prob_fit = predict_for_indices(model_en, fit_idx)\n",
        "post_prob_val = predict_for_indices(model_en, val_idx)\n",
        "post_prob_test = predict_for_indices(model_en, test_idx)\n",
        "\n",
        "y_prob = post_prob_test.copy()\n",
        "threshold_search_results_en = pd.DataFrame()\n",
        "SELECTED_THRESHOLD = PREDICTION_THRESHOLD\n",
        "best_threshold_val_score = None\n",
        "\n",
        "if has_validation and USE_THRESHOLD_SEARCH:\n",
        "    val_author_ids_for_score = train_en_model.iloc[val_idx][\"author_id\"].to_numpy()\n",
        "    search_rows = []\n",
        "    for threshold in np.linspace(THRESHOLD_SEARCH_MIN, THRESHOLD_SEARCH_MAX, THRESHOLD_SEARCH_STEPS):\n",
        "        score, tp_acc, fn_acc, fp_acc, n_accounts = compute_account_score(\n",
        "            author_ids=val_author_ids_for_score,\n",
        "            true_labels=y_val,\n",
        "            pred_probs=post_prob_val,\n",
        "            threshold=float(threshold),\n",
        "            decision_rule=ACCOUNT_DECISION_RULE,\n",
        "        )\n",
        "        search_rows.append(\n",
        "            {\n",
        "                \"threshold\": float(threshold),\n",
        "                \"score\": int(score),\n",
        "                \"tp_accounts\": int(tp_acc),\n",
        "                \"fn_accounts\": int(fn_acc),\n",
        "                \"fp_accounts\": int(fp_acc),\n",
        "                \"n_accounts\": int(n_accounts),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    threshold_search_results_en = pd.DataFrame(search_rows)\n",
        "    best_row = threshold_search_results_en.sort_values(\n",
        "        by=[\"score\", \"fp_accounts\", \"tp_accounts\", \"threshold\"],\n",
        "        ascending=[False, True, False, False],\n",
        "    ).iloc[0]\n",
        "\n",
        "    SELECTED_THRESHOLD = float(best_row[\"threshold\"])\n",
        "    best_threshold_val_score = int(best_row[\"score\"])\n",
        "\n",
        "y_pred = (y_prob >= SELECTED_THRESHOLD).astype(np.int64)\n",
        "\n",
        "test_author_ids_for_score = train_en_model.iloc[test_idx][\"author_id\"].to_numpy()\n",
        "(\n",
        "    test_score,\n",
        "    test_tp_accounts,\n",
        "    test_fn_accounts,\n",
        "    test_fp_accounts,\n",
        "    test_n_accounts,\n",
        ") = compute_account_score(\n",
        "    author_ids=test_author_ids_for_score,\n",
        "    true_labels=y_test,\n",
        "    pred_probs=y_prob,\n",
        "    threshold=SELECTED_THRESHOLD,\n",
        "    decision_rule=ACCOUNT_DECISION_RULE,\n",
        ")\n",
        "\n",
        "(\n",
        "    baseline_test_score,\n",
        "    baseline_tp_accounts,\n",
        "    baseline_fn_accounts,\n",
        "    baseline_fp_accounts,\n",
        "    baseline_n_accounts,\n",
        ") = compute_account_score(\n",
        "    author_ids=test_author_ids_for_score,\n",
        "    true_labels=y_test,\n",
        "    pred_probs=y_prob,\n",
        "    threshold=PREDICTION_THRESHOLD,\n",
        "    decision_rule=ACCOUNT_DECISION_RULE,\n",
        ")\n",
        "\n",
        "print(f\"Split mode: {split_mode}\")\n",
        "print(f\"Topic features enabled: {USE_TOPIC_FEATURES}\")\n",
        "print(f\"Topic match mode: {TOPIC_MATCH_MODE}\")\n",
        "print(f\"Account decision rule: {ACCOUNT_DECISION_RULE}\")\n",
        "print(\"Topic columns:\", topic_feature_cols_en)\n",
        "print(f\"Split sizes (fit/val/test posts): {len(fit_idx)}/{len(val_idx)}/{len(test_idx)}\")\n",
        "print(\n",
        "    f\"Split sizes (fit/val/test accounts): {len(fit_author_ids)}/{len(val_author_ids)}/{len(test_author_ids)}\"\n",
        ")\n",
        "print(f\"Default threshold from config: {PREDICTION_THRESHOLD:.4f}\")\n",
        "print(f\"Selected threshold used on test: {SELECTED_THRESHOLD:.4f}\")\n",
        "if best_threshold_val_score is not None:\n",
        "    print(f\"Best validation score from threshold search: {best_threshold_val_score}\")\n",
        "print(f\"Test Accuracy (post-level): {accuracy_score(y_test, y_pred):.4f}\")\n",
        "print(f\"Test ROC-AUC (post-level): {roc_auc_score(y_test, y_prob):.4f}\")\n",
        "print(\n",
        "    f\"Test account score @ selected threshold -> score={test_score}, TP={test_tp_accounts}, FN={test_fn_accounts}, FP={test_fp_accounts}, accounts={test_n_accounts}\"\n",
        ")\n",
        "print(\n",
        "    f\"Test account score @ config threshold -> score={baseline_test_score}, TP={baseline_tp_accounts}, FN={baseline_fn_accounts}, FP={baseline_fp_accounts}, accounts={baseline_n_accounts}\"\n",
        ")\n",
        "print(classification_report(y_test, y_pred, digits=4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b0a6a2a8",
      "metadata": {},
      "source": [
        "## Bot-detector score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "e84ff0e0",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Threshold used: 0.3750\n",
            "Account decision rule: mean\n",
            "Accounts scored (test split): 110\n",
            "TP: 19  FN: 7  FP: 2  TN: 82\n",
            "Bot detector score: 65\n",
            "Score out of max possible: 65/104 (62.50%)\n",
            "Score range on this split: [-194, 104]\n",
            "Range-normalized score: 86.91%\n",
            "Second-stage account-model score (recommended): 89 at threshold 0.7900\n"
          ]
        }
      ],
      "source": [
        "if any(name not in globals() for name in [\"train_en_model\", \"test_idx\", \"y_prob\", \"PREDICTION_THRESHOLD\"]):\n",
        "    raise ValueError(\"Run the Train-Test split for model cell first.\")\n",
        "\n",
        "threshold_for_score = float(globals().get(\"SELECTED_THRESHOLD\", PREDICTION_THRESHOLD))\n",
        "if \"ACCOUNT_DECISION_RULE\" in globals():\n",
        "    account_decision_rule = str(ACCOUNT_DECISION_RULE)\n",
        "elif \"EXPERIMENT_CONFIG\" in globals():\n",
        "    account_decision_rule = str(EXPERIMENT_CONFIG.get(\"account_decision_rule\", \"mean\"))\n",
        "else:\n",
        "    account_decision_rule = \"mean\"\n",
        "\n",
        "if account_decision_rule not in {\"mean\", \"any\"}:\n",
        "    raise ValueError('account_decision_rule must be \"mean\" or \"any\".')\n",
        "\n",
        "# Build test-set account labels/predictions from post-level outputs.\n",
        "score_df = train_en_model.iloc[test_idx][[\"author_id\", \"is_bot\"]].copy()\n",
        "score_df[\"pred_prob\"] = y_prob\n",
        "score_df[\"pred_post\"] = (score_df[\"pred_prob\"] >= threshold_for_score).astype(np.int64)\n",
        "\n",
        "account_df = (\n",
        "    score_df.groupby(\"author_id\", as_index=False)\n",
        "    .agg(\n",
        "        true_is_bot=(\"is_bot\", \"max\"),\n",
        "        mean_prob=(\"pred_prob\", \"mean\"),\n",
        "        any_pred=(\"pred_post\", \"max\"),\n",
        "        n_posts=(\"pred_post\", \"size\"),\n",
        "    )\n",
        ")\n",
        "\n",
        "if account_decision_rule == \"any\":\n",
        "    account_df[\"pred_is_bot\"] = account_df[\"any_pred\"].astype(np.int64)\n",
        "else:\n",
        "    account_df[\"pred_is_bot\"] = (account_df[\"mean_prob\"] >= threshold_for_score).astype(np.int64)\n",
        "\n",
        "tp_accounts = int(((account_df[\"true_is_bot\"] == 1) & (account_df[\"pred_is_bot\"] == 1)).sum())\n",
        "fn_accounts = int(((account_df[\"true_is_bot\"] == 1) & (account_df[\"pred_is_bot\"] == 0)).sum())\n",
        "fp_accounts = int(((account_df[\"true_is_bot\"] == 0) & (account_df[\"pred_is_bot\"] == 1)).sum())\n",
        "tn_accounts = int(((account_df[\"true_is_bot\"] == 0) & (account_df[\"pred_is_bot\"] == 0)).sum())\n",
        "\n",
        "score = (4 * tp_accounts) - (1 * fn_accounts) - (2 * fp_accounts)\n",
        "max_possible_score = 4 * int((account_df[\"true_is_bot\"] == 1).sum())\n",
        "min_possible_score = (\n",
        "    -1 * int((account_df[\"true_is_bot\"] == 1).sum())\n",
        "    -2 * int((account_df[\"true_is_bot\"] == 0).sum())\n",
        ")\n",
        "\n",
        "score_ratio = score / max_possible_score if max_possible_score > 0 else np.nan\n",
        "score_normalized = (\n",
        "    (score - min_possible_score) / (max_possible_score - min_possible_score)\n",
        "    if max_possible_score != min_possible_score\n",
        "    else np.nan\n",
        ")\n",
        "\n",
        "print(f\"Threshold used: {threshold_for_score:.4f}\")\n",
        "print(f\"Account decision rule: {account_decision_rule}\")\n",
        "print(f\"Accounts scored (test split): {len(account_df)}\")\n",
        "print(f\"TP: {tp_accounts}  FN: {fn_accounts}  FP: {fp_accounts}  TN: {tn_accounts}\")\n",
        "print(f\"Bot detector score: {score}\")\n",
        "print(f\"Score out of max possible: {score}/{max_possible_score} ({score_ratio:.2%})\")\n",
        "print(f\"Score range on this split: [{min_possible_score}, {max_possible_score}]\")\n",
        "print(f\"Range-normalized score: {score_normalized:.2%}\")\n",
        "\n",
        "if \"second_stage_test_score_en\" in globals():\n",
        "    print(\n",
        "        f\"Second-stage account-model score (recommended): {second_stage_test_score_en} at threshold {second_stage_selected_threshold_en:.4f}\"\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9a2db3f5",
      "metadata": {},
      "source": [
        "## Strongest booster (account-level second stage)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "a1c7e2d9",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-08 17:41:01.448172: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
            "2026-02-08 17:41:09.872902: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
            "2026-02-08 17:41:19.174813: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
            "2026-02-08 17:41:24.318368: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
            "2026-02-08 17:41:33.890745: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
            "2026-02-08 17:41:44.271394: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
            "2026-02-08 17:41:50.315189: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
            "2026-02-08 17:41:58.964379: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
            "2026-02-08 17:42:08.406994: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
            "2026-02-08 17:42:13.685961: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
            "2026-02-08 17:42:22.161380: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
            "2026-02-08 17:42:31.914517: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
            "2026-02-08 17:42:37.665636: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
            "2026-02-08 17:42:46.277329: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
            "2026-02-08 17:42:55.502602: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Building OOF first-stage features for second-stage training (seeds=[13, 42], folds=4, epochs=4)...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2026-02-08 17:43:01.135037: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
            "2026-02-08 17:43:07.876355: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
            "2026-02-08 17:43:21.441052: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
            "2026-02-08 17:43:30.886784: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
            "2026-02-08 17:43:45.264659: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
            "2026-02-08 17:43:54.321837: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
            "2026-02-08 17:44:09.688107: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
            "2026-02-08 17:44:18.056808: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
            "2026-02-08 17:44:25.159235: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
            "2026-02-08 17:44:33.857238: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
            "2026-02-08 17:44:48.756994: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
            "2026-02-08 17:44:56.867036: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
            "2026-02-08 17:45:10.606823: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
            "2026-02-08 17:45:18.730122: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
            "2026-02-08 17:45:25.402118: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
            "2026-02-08 17:45:34.008783: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
            "2026-02-08 17:45:48.019308: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Seed-level first-stage account scores (test):\n",
            " seed  threshold  test_score  tp_accounts  fn_accounts  fp_accounts\n",
            "   13      0.335          72           20            6            1\n",
            "   29      0.325          69           21            5            5\n",
            "   42      0.520          49           15           11            0\n",
            "   73      0.660          57           17            9            1\n",
            "  101      0.380          66           20            6            4\n",
            "Seed score mean/std: 62.60 / 9.45\n",
            "Ensemble aggregation: mean\n",
            "Ensemble selected threshold: 0.3700\n",
            "Ensemble test score: 77 (TP=21, FN=5, FP=1, accounts=110)\n",
            "Second-stage fit feature source: oof_first_stage\n",
            "OOF seed report for second-stage fit features:\n",
            " seed  fit_account_score_at_ensemble_threshold  tp_accounts  fn_accounts  fp_accounts  n_accounts\n",
            "   13                                      232           69           18           13         370\n",
            "   42                                      245           70           17            9         370\n",
            "Second-stage candidate report:\n",
            "    profile  alpha  threshold  val_score  val_tp_accounts  val_fn_accounts  val_fp_accounts  test_score  test_tp_accounts  test_fn_accounts  test_fp_accounts\n",
            "     legacy    1.0      0.250         58               16                0                3          96                26                 0                 4\n",
            "regularized    1.0      0.195         62               16                0                1          92                26                 0                 6\n",
            "Baseline account-level validation score (from first-stage means): 45 (TP=13, FN=3, FP=2, threshold=0.3700)\n",
            "Second-stage profile mode: auto\n",
            "Second-stage selected profile: regularized\n",
            "Second-stage blend alpha (CatBoost weight): 1.00\n",
            "Second-stage threshold: 0.1950\n",
            "Second-stage test score: 92/104 (88.46%)\n",
            "Second-stage confusion components -> TP=26, FN=0, FP=6\n",
            "Second-stage precision=0.8125, recall=1.0000\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0     1.0000    0.9286    0.9630        84\n",
            "           1     0.8125    1.0000    0.8966        26\n",
            "\n",
            "    accuracy                         0.9455       110\n",
            "   macro avg     0.9062    0.9643    0.9298       110\n",
            "weighted avg     0.9557    0.9455    0.9473       110\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import classification_report, precision_score, recall_score\n",
        "from sklearn.utils.class_weight import compute_sample_weight\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "try:\n",
        "    from catboost import CatBoostClassifier\n",
        "except ImportError as exc:\n",
        "    raise ImportError(\"Install catboost first: pip install catboost\") from exc\n",
        "\n",
        "required_globals = [\n",
        "    \"tf\",\n",
        "    \"EXPERIMENT_CONFIG\",\n",
        "    \"build_multifeature_model\",\n",
        "    \"compute_account_score\",\n",
        "    \"train_en_model\",\n",
        "    \"topic_feature_cols_en\",\n",
        "    \"fit_idx\",\n",
        "    \"val_idx\",\n",
        "    \"test_idx\",\n",
        "    \"input_ids_en\",\n",
        "    \"attention_mask_en\",\n",
        "    \"X_aux_en\",\n",
        "    \"y_en\",\n",
        "    \"MAX_LENGTH\",\n",
        "    \"tokenizer\",\n",
        "    \"users_en\",\n",
        "    \"EMBEDDING_DIM\",\n",
        "    \"GRU_UNITS\",\n",
        "    \"AUX_DENSE_UNITS\",\n",
        "    \"HEAD_DENSE_UNITS\",\n",
        "    \"DROPOUT_TEXT\",\n",
        "    \"DROPOUT_AUX\",\n",
        "    \"DROPOUT_HEAD\",\n",
        "    \"LEARNING_RATE\",\n",
        "    \"EPOCHS\",\n",
        "    \"BATCH_SIZE\",\n",
        "    \"RANDOM_SEED\",\n",
        "    \"USE_CLASS_WEIGHTS\",\n",
        "    \"class_weight_dict\",\n",
        "    \"USE_THRESHOLD_SEARCH\",\n",
        "    \"THRESHOLD_SEARCH_MIN\",\n",
        "    \"THRESHOLD_SEARCH_MAX\",\n",
        "    \"THRESHOLD_SEARCH_STEPS\",\n",
        "    \"PREDICTION_THRESHOLD\",\n",
        "    \"ACCOUNT_DECISION_RULE\",\n",
        "    \"EARLY_STOPPING_PATIENCE\",\n",
        "    \"REDUCE_LR_PATIENCE\",\n",
        "    \"REDUCE_LR_FACTOR\",\n",
        "    \"REDUCE_LR_MIN_LR\",\n",
        "]\n",
        "missing = [name for name in required_globals if name not in globals()]\n",
        "if missing:\n",
        "    raise ValueError(f\"Run the Train-Test split for model cell first. Missing: {missing}\")\n",
        "\n",
        "if ACCOUNT_DECISION_RULE not in {\"mean\", \"any\"}:\n",
        "    raise ValueError('account_decision_rule must be \"mean\" or \"any\".')\n",
        "\n",
        "ensemble_seeds_cfg = EXPERIMENT_CONFIG.get(\"ensemble_seeds\", [RANDOM_SEED])\n",
        "if isinstance(ensemble_seeds_cfg, (int, np.integer)):\n",
        "    ensemble_seeds = [int(ensemble_seeds_cfg)]\n",
        "elif isinstance(ensemble_seeds_cfg, (list, tuple)):\n",
        "    ensemble_seeds = [int(seed) for seed in ensemble_seeds_cfg]\n",
        "else:\n",
        "    raise ValueError(\"ensemble_seeds must be an int or a list of ints.\")\n",
        "ensemble_seeds = list(dict.fromkeys(ensemble_seeds))\n",
        "if not ensemble_seeds:\n",
        "    raise ValueError(\"ensemble_seeds cannot be empty.\")\n",
        "\n",
        "ENSEMBLE_AGGREGATION = str(EXPERIMENT_CONFIG.get(\"ensemble_aggregation\", \"mean\")).lower()\n",
        "if ENSEMBLE_AGGREGATION not in {\"mean\", \"median\"}:\n",
        "    raise ValueError('ensemble_aggregation must be \"mean\" or \"median\".')\n",
        "\n",
        "USE_EXTERNAL_PRETRAIN = bool(EXPERIMENT_CONFIG.get(\"use_external_pretrain\", False))\n",
        "EXTERNAL_PRETRAIN_EPOCHS = int(EXPERIMENT_CONFIG.get(\"external_pretrain_epochs\", 1))\n",
        "EXTERNAL_PRETRAIN_BATCH_SIZE = int(EXPERIMENT_CONFIG.get(\"external_pretrain_batch_size\", 128))\n",
        "EXTERNAL_PRETRAIN_USE_BALANCED_WEIGHTS = bool(\n",
        "    EXPERIMENT_CONFIG.get(\"external_pretrain_use_balanced_weights\", True)\n",
        ")\n",
        "\n",
        "USE_SECOND_STAGE_ACCOUNT_MODEL = bool(EXPERIMENT_CONFIG.get(\"use_second_stage_account_model\", True))\n",
        "SECOND_STAGE_PROFILE = str(EXPERIMENT_CONFIG.get(\"second_stage_profile\", \"auto\")).lower()\n",
        "if SECOND_STAGE_PROFILE not in {\"auto\", \"legacy\", \"regularized\", \"custom\"}:\n",
        "    raise ValueError('second_stage_profile must be one of: \"auto\", \"legacy\", \"regularized\", \"custom\".')\n",
        "\n",
        "SECOND_STAGE_USE_BLEND = bool(EXPERIMENT_CONFIG.get(\"second_stage_use_blend\", True))\n",
        "blend_alphas_cfg = EXPERIMENT_CONFIG.get(\"second_stage_blend_alphas\", [1.0, 0.85, 0.70, 0.55])\n",
        "if isinstance(blend_alphas_cfg, (int, float, np.floating, np.integer)):\n",
        "    SECOND_STAGE_BLEND_ALPHAS = [float(blend_alphas_cfg)]\n",
        "elif isinstance(blend_alphas_cfg, (list, tuple)):\n",
        "    SECOND_STAGE_BLEND_ALPHAS = [float(alpha) for alpha in blend_alphas_cfg]\n",
        "else:\n",
        "    raise ValueError(\"second_stage_blend_alphas must be a number or a list of numbers.\")\n",
        "SECOND_STAGE_BLEND_ALPHAS = [a for a in SECOND_STAGE_BLEND_ALPHAS if 0.0 <= a <= 1.0]\n",
        "if not SECOND_STAGE_BLEND_ALPHAS:\n",
        "    SECOND_STAGE_BLEND_ALPHAS = [1.0]\n",
        "if 1.0 not in SECOND_STAGE_BLEND_ALPHAS:\n",
        "    SECOND_STAGE_BLEND_ALPHAS.append(1.0)\n",
        "SECOND_STAGE_BLEND_ALPHAS = sorted(set(SECOND_STAGE_BLEND_ALPHAS), reverse=True)\n",
        "SECOND_STAGE_MIN_GAIN_VS_ENSEMBLE = float(EXPERIMENT_CONFIG.get(\"second_stage_min_gain_vs_ensemble\", 0.0))\n",
        "\n",
        "SECOND_STAGE_USE_OOF_FEATURES = bool(EXPERIMENT_CONFIG.get(\"second_stage_use_oof_features\", True))\n",
        "SECOND_STAGE_OOF_FOLDS = int(EXPERIMENT_CONFIG.get(\"second_stage_oof_folds\", 4))\n",
        "SECOND_STAGE_OOF_EPOCHS = int(EXPERIMENT_CONFIG.get(\"second_stage_oof_epochs\", max(2, min(EPOCHS, 4))))\n",
        "SECOND_STAGE_OOF_BATCH_SIZE = int(EXPERIMENT_CONFIG.get(\"second_stage_oof_batch_size\", BATCH_SIZE))\n",
        "SECOND_STAGE_OOF_USE_EXTERNAL_PRETRAIN = bool(\n",
        "    EXPERIMENT_CONFIG.get(\"second_stage_oof_use_external_pretrain\", False)\n",
        ")\n",
        "\n",
        "SECOND_STAGE_LEARNING_RATE = float(EXPERIMENT_CONFIG.get(\"second_stage_learning_rate\", 0.05))\n",
        "SECOND_STAGE_MAX_ITER = int(EXPERIMENT_CONFIG.get(\"second_stage_max_iter\", 300))\n",
        "SECOND_STAGE_MAX_DEPTH = int(EXPERIMENT_CONFIG.get(\"second_stage_max_depth\", 4))\n",
        "SECOND_STAGE_L2 = float(EXPERIMENT_CONFIG.get(\"second_stage_l2\", 0.2))\n",
        "SECOND_STAGE_MIN_DATA_IN_LEAF = int(EXPERIMENT_CONFIG.get(\"second_stage_min_data_in_leaf\", 20))\n",
        "SECOND_STAGE_SUBSAMPLE = float(EXPERIMENT_CONFIG.get(\"second_stage_subsample\", 0.8))\n",
        "SECOND_STAGE_RSM = float(EXPERIMENT_CONFIG.get(\"second_stage_rsm\", 0.8))\n",
        "SECOND_STAGE_OD_WAIT = int(EXPERIMENT_CONFIG.get(\"second_stage_od_wait\", 50))\n",
        "SECOND_STAGE_USE_BALANCED_WEIGHTS = bool(EXPERIMENT_CONFIG.get(\"second_stage_use_balanced_weights\", True))\n",
        "\n",
        "oof_seeds_cfg = EXPERIMENT_CONFIG.get(\n",
        "    \"second_stage_oof_seeds\",\n",
        "    ensemble_seeds[: min(3, len(ensemble_seeds))],\n",
        ")\n",
        "if isinstance(oof_seeds_cfg, (int, np.integer)):\n",
        "    second_stage_oof_seeds = [int(oof_seeds_cfg)]\n",
        "elif isinstance(oof_seeds_cfg, (list, tuple)):\n",
        "    second_stage_oof_seeds = [int(seed) for seed in oof_seeds_cfg]\n",
        "else:\n",
        "    raise ValueError(\"second_stage_oof_seeds must be an int or a list of ints.\")\n",
        "second_stage_oof_seeds = list(dict.fromkeys(second_stage_oof_seeds))\n",
        "if SECOND_STAGE_USE_OOF_FEATURES and not second_stage_oof_seeds:\n",
        "    raise ValueError(\"second_stage_oof_seeds cannot be empty when second_stage_use_oof_features=True.\")\n",
        "\n",
        "has_validation = len(val_idx) > 0\n",
        "fit_author_ids_for_score = train_en_model.iloc[fit_idx][\"author_id\"].to_numpy()\n",
        "val_author_ids_for_score = train_en_model.iloc[val_idx][\"author_id\"].to_numpy() if has_validation else np.array([])\n",
        "test_author_ids_for_score = train_en_model.iloc[test_idx][\"author_id\"].to_numpy()\n",
        "\n",
        "def _predict_post_probs(model, indices):\n",
        "    if len(indices) == 0:\n",
        "        return np.array([], dtype=np.float32)\n",
        "    return model.predict(\n",
        "        {\n",
        "            \"input_ids\": input_ids_en[indices],\n",
        "            \"attention_mask\": attention_mask_en[indices],\n",
        "            \"aux_features\": X_aux_en[indices],\n",
        "        },\n",
        "        verbose=0,\n",
        "    ).ravel()\n",
        "\n",
        "def _build_callbacks():\n",
        "    callbacks = []\n",
        "    if has_validation and EARLY_STOPPING_PATIENCE > 0:\n",
        "        callbacks.append(\n",
        "            tf.keras.callbacks.EarlyStopping(\n",
        "                monitor=\"val_auc\",\n",
        "                mode=\"max\",\n",
        "                patience=EARLY_STOPPING_PATIENCE,\n",
        "                restore_best_weights=True,\n",
        "            )\n",
        "        )\n",
        "    if has_validation and REDUCE_LR_PATIENCE > 0:\n",
        "        callbacks.append(\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor=\"val_auc\",\n",
        "                mode=\"max\",\n",
        "                factor=REDUCE_LR_FACTOR,\n",
        "                patience=REDUCE_LR_PATIENCE,\n",
        "                min_lr=REDUCE_LR_MIN_LR,\n",
        "            )\n",
        "        )\n",
        "    return callbacks\n",
        "\n",
        "def _build_fold_callbacks():\n",
        "    callbacks = []\n",
        "    if EARLY_STOPPING_PATIENCE > 0:\n",
        "        callbacks.append(\n",
        "            tf.keras.callbacks.EarlyStopping(\n",
        "                monitor=\"val_auc\",\n",
        "                mode=\"max\",\n",
        "                patience=EARLY_STOPPING_PATIENCE,\n",
        "                restore_best_weights=True,\n",
        "            )\n",
        "        )\n",
        "    if REDUCE_LR_PATIENCE > 0:\n",
        "        callbacks.append(\n",
        "            tf.keras.callbacks.ReduceLROnPlateau(\n",
        "                monitor=\"val_auc\",\n",
        "                mode=\"max\",\n",
        "                factor=REDUCE_LR_FACTOR,\n",
        "                patience=REDUCE_LR_PATIENCE,\n",
        "                min_lr=REDUCE_LR_MIN_LR,\n",
        "            )\n",
        "        )\n",
        "    return callbacks\n",
        "\n",
        "def _search_best_threshold(author_ids, labels, probs):\n",
        "    if not (has_validation and USE_THRESHOLD_SEARCH):\n",
        "        return float(PREDICTION_THRESHOLD), None, pd.DataFrame()\n",
        "\n",
        "    rows = []\n",
        "    for threshold in np.linspace(THRESHOLD_SEARCH_MIN, THRESHOLD_SEARCH_MAX, THRESHOLD_SEARCH_STEPS):\n",
        "        score, tp, fn, fp, n_accounts = compute_account_score(\n",
        "            author_ids=author_ids,\n",
        "            true_labels=labels,\n",
        "            pred_probs=probs,\n",
        "            threshold=float(threshold),\n",
        "            decision_rule=ACCOUNT_DECISION_RULE,\n",
        "        )\n",
        "        rows.append(\n",
        "            {\n",
        "                \"threshold\": float(threshold),\n",
        "                \"score\": int(score),\n",
        "                \"tp_accounts\": int(tp),\n",
        "                \"fn_accounts\": int(fn),\n",
        "                \"fp_accounts\": int(fp),\n",
        "                \"n_accounts\": int(n_accounts),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    best_row = df.sort_values(\n",
        "        by=[\"score\", \"fp_accounts\", \"tp_accounts\", \"threshold\"],\n",
        "        ascending=[False, True, False, False],\n",
        "    ).iloc[0]\n",
        "    return float(best_row[\"threshold\"]), int(best_row[\"score\"]), df\n",
        "\n",
        "def _aggregate_probabilities(prob_list, mode):\n",
        "    if not prob_list:\n",
        "        return np.array([], dtype=np.float32)\n",
        "    stack = np.vstack(prob_list)\n",
        "    if mode == \"median\":\n",
        "        return np.median(stack, axis=0).astype(np.float32)\n",
        "    return np.mean(stack, axis=0).astype(np.float32)\n",
        "\n",
        "def _prepare_external_pretrain_inputs():\n",
        "    if not USE_EXTERNAL_PRETRAIN:\n",
        "        return None, None, None\n",
        "\n",
        "    required_external = [\n",
        "        \"external_pretrain_input_ids\",\n",
        "        \"external_pretrain_attention_mask\",\n",
        "        \"external_pretrain_meta_scaled\",\n",
        "        \"external_pretrain_labels\",\n",
        "    ]\n",
        "    missing_external = [name for name in required_external if name not in globals()]\n",
        "    if missing_external:\n",
        "        raise ValueError(\n",
        "            f\"External pretrain enabled, but missing variables: {missing_external}. Run the External data pretraining cell.\"\n",
        "        )\n",
        "\n",
        "    external_meta = external_pretrain_meta_scaled\n",
        "    aux_dim = X_aux_en.shape[1]\n",
        "    meta_dim = external_meta.shape[1]\n",
        "    if aux_dim > meta_dim:\n",
        "        padding = np.zeros((external_meta.shape[0], aux_dim - meta_dim), dtype=np.float32)\n",
        "        external_aux = np.concatenate([external_meta, padding], axis=1)\n",
        "    else:\n",
        "        external_aux = external_meta[:, :aux_dim]\n",
        "\n",
        "    external_inputs = {\n",
        "        \"input_ids\": external_pretrain_input_ids,\n",
        "        \"attention_mask\": external_pretrain_attention_mask,\n",
        "        \"aux_features\": external_aux,\n",
        "    }\n",
        "    external_labels = np.asarray(globals()[\"external_pretrain_labels\"], dtype=np.int64)\n",
        "\n",
        "    external_sample_weight = None\n",
        "    if EXTERNAL_PRETRAIN_USE_BALANCED_WEIGHTS:\n",
        "        external_sample_weight = compute_sample_weight(class_weight=\"balanced\", y=external_labels)\n",
        "\n",
        "    return external_inputs, external_labels, external_sample_weight\n",
        "\n",
        "def _build_author_oof_folds(post_indices, n_splits, seed):\n",
        "    fit_posts = (\n",
        "        train_en_model.iloc[post_indices][[\"author_id\", \"is_bot\"]]\n",
        "        .reset_index()\n",
        "        .rename(columns={\"index\": \"post_index\"})\n",
        "    )\n",
        "    author_df = fit_posts.groupby(\"author_id\", as_index=False).agg(\n",
        "        account_label=(\"is_bot\", \"max\"),\n",
        "        post_index=(\"post_index\", list),\n",
        "    )\n",
        "\n",
        "    if len(author_df) < 2:\n",
        "        return [np.asarray(post_indices, dtype=np.int64)]\n",
        "\n",
        "    label_counts = author_df[\"account_label\"].value_counts()\n",
        "    min_class_count = int(label_counts.min()) if not label_counts.empty else 0\n",
        "    n_splits_eff = min(int(n_splits), len(author_df), min_class_count)\n",
        "\n",
        "    if n_splits_eff < 2:\n",
        "        return [np.asarray(post_indices, dtype=np.int64)]\n",
        "\n",
        "    splitter = StratifiedKFold(n_splits=n_splits_eff, shuffle=True, random_state=int(seed))\n",
        "\n",
        "    folds = []\n",
        "    author_ids = author_df[\"author_id\"].to_numpy()\n",
        "    author_labels = author_df[\"account_label\"].to_numpy(dtype=np.int64)\n",
        "\n",
        "    for _, hold_author_pos in splitter.split(author_ids, author_labels):\n",
        "        hold_lists = author_df.iloc[hold_author_pos][\"post_index\"].tolist()\n",
        "        hold_posts = np.asarray([idx for lst in hold_lists for idx in lst], dtype=np.int64)\n",
        "        folds.append(hold_posts)\n",
        "\n",
        "    return folds\n",
        "\n",
        "def _compute_oof_post_probs_for_seed(seed, post_indices, use_external_pretrain_for_oof):\n",
        "    post_indices = np.asarray(post_indices, dtype=np.int64)\n",
        "    folds = _build_author_oof_folds(post_indices, SECOND_STAGE_OOF_FOLDS, RANDOM_SEED + int(seed))\n",
        "\n",
        "    if len(folds) == 1 and len(folds[0]) == len(post_indices):\n",
        "        print(\n",
        "            \"[OOF] Not enough account diversity for stratified folds; using in-sample fallback for second-stage training features.\"\n",
        "        )\n",
        "        tf.keras.backend.clear_session()\n",
        "        tf.keras.utils.set_random_seed(int(seed))\n",
        "        np.random.seed(int(seed))\n",
        "\n",
        "        fallback_model = build_multifeature_model(\n",
        "            vocab_size=tokenizer.vocab_size,\n",
        "            seq_len=MAX_LENGTH,\n",
        "            aux_dim=X_aux_en.shape[1],\n",
        "            embedding_dim=EMBEDDING_DIM,\n",
        "            gru_units=GRU_UNITS,\n",
        "            aux_dense_units=AUX_DENSE_UNITS,\n",
        "            head_dense_units=HEAD_DENSE_UNITS,\n",
        "            dropout_text=DROPOUT_TEXT,\n",
        "            dropout_aux=DROPOUT_AUX,\n",
        "            dropout_head=DROPOUT_HEAD,\n",
        "            learning_rate=LEARNING_RATE,\n",
        "        )\n",
        "\n",
        "        fit_inputs_local = {\n",
        "            \"input_ids\": input_ids_en[post_indices],\n",
        "            \"attention_mask\": attention_mask_en[post_indices],\n",
        "            \"aux_features\": X_aux_en[post_indices],\n",
        "        }\n",
        "\n",
        "        fit_kwargs_local = {\n",
        "            \"epochs\": SECOND_STAGE_OOF_EPOCHS,\n",
        "            \"batch_size\": SECOND_STAGE_OOF_BATCH_SIZE,\n",
        "            \"class_weight\": class_weight_dict if USE_CLASS_WEIGHTS else None,\n",
        "            \"verbose\": 0,\n",
        "        }\n",
        "\n",
        "        fallback_model.fit(fit_inputs_local, y_en[post_indices], **fit_kwargs_local)\n",
        "        return _predict_post_probs(fallback_model, post_indices)\n",
        "\n",
        "    index_map = np.full(len(y_en), -1, dtype=np.int64)\n",
        "    index_map[post_indices] = np.arange(len(post_indices), dtype=np.int64)\n",
        "    oof_local = np.full(len(post_indices), np.nan, dtype=np.float32)\n",
        "\n",
        "    for fold_id, hold_posts in enumerate(folds, start=1):\n",
        "        train_posts = post_indices[~np.isin(post_indices, hold_posts)]\n",
        "        if len(train_posts) == 0 or len(hold_posts) == 0:\n",
        "            continue\n",
        "\n",
        "        tf.keras.backend.clear_session()\n",
        "        tf.keras.utils.set_random_seed(int(seed) * 100 + fold_id)\n",
        "        np.random.seed(int(seed) * 100 + fold_id)\n",
        "\n",
        "        fold_model = build_multifeature_model(\n",
        "            vocab_size=tokenizer.vocab_size,\n",
        "            seq_len=MAX_LENGTH,\n",
        "            aux_dim=X_aux_en.shape[1],\n",
        "            embedding_dim=EMBEDDING_DIM,\n",
        "            gru_units=GRU_UNITS,\n",
        "            aux_dense_units=AUX_DENSE_UNITS,\n",
        "            head_dense_units=HEAD_DENSE_UNITS,\n",
        "            dropout_text=DROPOUT_TEXT,\n",
        "            dropout_aux=DROPOUT_AUX,\n",
        "            dropout_head=DROPOUT_HEAD,\n",
        "            learning_rate=LEARNING_RATE,\n",
        "        )\n",
        "\n",
        "        if use_external_pretrain_for_oof and external_pretrain_inputs is not None:\n",
        "            pretrain_kwargs = {\n",
        "                \"epochs\": EXTERNAL_PRETRAIN_EPOCHS,\n",
        "                \"batch_size\": EXTERNAL_PRETRAIN_BATCH_SIZE,\n",
        "                \"shuffle\": True,\n",
        "                \"verbose\": 0,\n",
        "            }\n",
        "            if external_pretrain_sample_weight is not None:\n",
        "                pretrain_kwargs[\"sample_weight\"] = external_pretrain_sample_weight\n",
        "            fold_model.fit(external_pretrain_inputs, external_pretrain_labels_arr, **pretrain_kwargs)\n",
        "\n",
        "        train_inputs_fold = {\n",
        "            \"input_ids\": input_ids_en[train_posts],\n",
        "            \"attention_mask\": attention_mask_en[train_posts],\n",
        "            \"aux_features\": X_aux_en[train_posts],\n",
        "        }\n",
        "        hold_inputs_fold = {\n",
        "            \"input_ids\": input_ids_en[hold_posts],\n",
        "            \"attention_mask\": attention_mask_en[hold_posts],\n",
        "            \"aux_features\": X_aux_en[hold_posts],\n",
        "        }\n",
        "\n",
        "        fit_kwargs_fold = {\n",
        "            \"epochs\": SECOND_STAGE_OOF_EPOCHS,\n",
        "            \"batch_size\": SECOND_STAGE_OOF_BATCH_SIZE,\n",
        "            \"class_weight\": class_weight_dict if USE_CLASS_WEIGHTS else None,\n",
        "            \"callbacks\": _build_fold_callbacks(),\n",
        "            \"validation_data\": (hold_inputs_fold, y_en[hold_posts]),\n",
        "            \"verbose\": 0,\n",
        "        }\n",
        "\n",
        "        fold_model.fit(train_inputs_fold, y_en[train_posts], **fit_kwargs_fold)\n",
        "        hold_probs = fold_model.predict(hold_inputs_fold, verbose=0).ravel()\n",
        "\n",
        "        hold_local = index_map[hold_posts]\n",
        "        valid_mask = hold_local >= 0\n",
        "        oof_local[hold_local[valid_mask]] = hold_probs[valid_mask]\n",
        "\n",
        "    missing_mask = np.isnan(oof_local)\n",
        "    if missing_mask.any():\n",
        "        print(f\"[OOF] Filling {int(missing_mask.sum())} missing predictions with in-sample fallback for seed {seed}.\")\n",
        "        tf.keras.backend.clear_session()\n",
        "        tf.keras.utils.set_random_seed(int(seed) + 999)\n",
        "        np.random.seed(int(seed) + 999)\n",
        "\n",
        "        fallback_model = build_multifeature_model(\n",
        "            vocab_size=tokenizer.vocab_size,\n",
        "            seq_len=MAX_LENGTH,\n",
        "            aux_dim=X_aux_en.shape[1],\n",
        "            embedding_dim=EMBEDDING_DIM,\n",
        "            gru_units=GRU_UNITS,\n",
        "            aux_dense_units=AUX_DENSE_UNITS,\n",
        "            head_dense_units=HEAD_DENSE_UNITS,\n",
        "            dropout_text=DROPOUT_TEXT,\n",
        "            dropout_aux=DROPOUT_AUX,\n",
        "            dropout_head=DROPOUT_HEAD,\n",
        "            learning_rate=LEARNING_RATE,\n",
        "        )\n",
        "\n",
        "        fit_inputs_local = {\n",
        "            \"input_ids\": input_ids_en[post_indices],\n",
        "            \"attention_mask\": attention_mask_en[post_indices],\n",
        "            \"aux_features\": X_aux_en[post_indices],\n",
        "        }\n",
        "\n",
        "        fit_kwargs_local = {\n",
        "            \"epochs\": SECOND_STAGE_OOF_EPOCHS,\n",
        "            \"batch_size\": SECOND_STAGE_OOF_BATCH_SIZE,\n",
        "            \"class_weight\": class_weight_dict if USE_CLASS_WEIGHTS else None,\n",
        "            \"verbose\": 0,\n",
        "        }\n",
        "        fallback_model.fit(fit_inputs_local, y_en[post_indices], **fit_kwargs_local)\n",
        "        fallback_probs = fallback_model.predict(fit_inputs_local, verbose=0).ravel()\n",
        "        oof_local[missing_mask] = fallback_probs[missing_mask]\n",
        "\n",
        "    return oof_local.astype(np.float32)\n",
        "\n",
        "fit_inputs = {\n",
        "    \"input_ids\": input_ids_en[fit_idx],\n",
        "    \"attention_mask\": attention_mask_en[fit_idx],\n",
        "    \"aux_features\": X_aux_en[fit_idx],\n",
        "}\n",
        "val_inputs = (\n",
        "    {\n",
        "        \"input_ids\": input_ids_en[val_idx],\n",
        "        \"attention_mask\": attention_mask_en[val_idx],\n",
        "        \"aux_features\": X_aux_en[val_idx],\n",
        "    }\n",
        "    if has_validation\n",
        "    else None\n",
        ")\n",
        "\n",
        "post_prob_fit_list = []\n",
        "post_prob_val_list = []\n",
        "post_prob_test_list = []\n",
        "seed_rows = []\n",
        "\n",
        "if hasattr(tf.config.experimental, \"enable_op_determinism\"):\n",
        "    try:\n",
        "        tf.config.experimental.enable_op_determinism()\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "external_pretrain_inputs = None\n",
        "external_pretrain_labels_arr = None\n",
        "external_pretrain_sample_weight = None\n",
        "if USE_EXTERNAL_PRETRAIN:\n",
        "    external_pretrain_inputs, external_pretrain_labels_arr, external_pretrain_sample_weight = _prepare_external_pretrain_inputs()\n",
        "\n",
        "for seed in ensemble_seeds:\n",
        "    tf.keras.backend.clear_session()\n",
        "    tf.keras.utils.set_random_seed(int(seed))\n",
        "    np.random.seed(int(seed))\n",
        "\n",
        "    model_seed = build_multifeature_model(\n",
        "        vocab_size=tokenizer.vocab_size,\n",
        "        seq_len=MAX_LENGTH,\n",
        "        aux_dim=X_aux_en.shape[1],\n",
        "        embedding_dim=EMBEDDING_DIM,\n",
        "        gru_units=GRU_UNITS,\n",
        "        aux_dense_units=AUX_DENSE_UNITS,\n",
        "        head_dense_units=HEAD_DENSE_UNITS,\n",
        "        dropout_text=DROPOUT_TEXT,\n",
        "        dropout_aux=DROPOUT_AUX,\n",
        "        dropout_head=DROPOUT_HEAD,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "    )\n",
        "\n",
        "    if USE_EXTERNAL_PRETRAIN:\n",
        "        pretrain_kwargs = {\n",
        "            \"epochs\": EXTERNAL_PRETRAIN_EPOCHS,\n",
        "            \"batch_size\": EXTERNAL_PRETRAIN_BATCH_SIZE,\n",
        "            \"shuffle\": True,\n",
        "            \"verbose\": 0,\n",
        "        }\n",
        "        if external_pretrain_sample_weight is not None:\n",
        "            pretrain_kwargs[\"sample_weight\"] = external_pretrain_sample_weight\n",
        "        model_seed.fit(external_pretrain_inputs, external_pretrain_labels_arr, **pretrain_kwargs)\n",
        "\n",
        "    fit_kwargs = {\n",
        "        \"epochs\": EPOCHS,\n",
        "        \"batch_size\": BATCH_SIZE,\n",
        "        \"class_weight\": class_weight_dict if USE_CLASS_WEIGHTS else None,\n",
        "        \"callbacks\": _build_callbacks(),\n",
        "        \"verbose\": 0,\n",
        "    }\n",
        "    if has_validation:\n",
        "        fit_kwargs[\"validation_data\"] = (val_inputs, y_en[val_idx])\n",
        "\n",
        "    model_seed.fit(fit_inputs, y_en[fit_idx], **fit_kwargs)\n",
        "\n",
        "    fit_probs = _predict_post_probs(model_seed, fit_idx)\n",
        "    val_probs = _predict_post_probs(model_seed, val_idx)\n",
        "    test_probs = _predict_post_probs(model_seed, test_idx)\n",
        "\n",
        "    threshold_seed, val_score_seed, _ = _search_best_threshold(val_author_ids_for_score, y_en[val_idx], val_probs)\n",
        "\n",
        "    test_score_seed, tp_seed, fn_seed, fp_seed, n_accounts_seed = compute_account_score(\n",
        "        author_ids=test_author_ids_for_score,\n",
        "        true_labels=y_en[test_idx],\n",
        "        pred_probs=test_probs,\n",
        "        threshold=threshold_seed,\n",
        "        decision_rule=ACCOUNT_DECISION_RULE,\n",
        "    )\n",
        "\n",
        "    seed_rows.append(\n",
        "        {\n",
        "            \"seed\": int(seed),\n",
        "            \"threshold\": float(threshold_seed),\n",
        "            \"val_best_score\": val_score_seed,\n",
        "            \"test_score\": int(test_score_seed),\n",
        "            \"tp_accounts\": int(tp_seed),\n",
        "            \"fn_accounts\": int(fn_seed),\n",
        "            \"fp_accounts\": int(fp_seed),\n",
        "            \"n_accounts\": int(n_accounts_seed),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    post_prob_fit_list.append(fit_probs)\n",
        "    post_prob_val_list.append(val_probs)\n",
        "    post_prob_test_list.append(test_probs)\n",
        "\n",
        "seed_report_df = pd.DataFrame(seed_rows)\n",
        "ensemble_seed_report_en = seed_report_df.copy()\n",
        "\n",
        "post_prob_fit_ensemble_en = _aggregate_probabilities(post_prob_fit_list, ENSEMBLE_AGGREGATION)\n",
        "post_prob_val_ensemble_en = _aggregate_probabilities(post_prob_val_list, ENSEMBLE_AGGREGATION)\n",
        "post_prob_test_ensemble_en = _aggregate_probabilities(post_prob_test_list, ENSEMBLE_AGGREGATION)\n",
        "\n",
        "selected_threshold_ensemble, best_ensemble_val_score, threshold_search_results_ensemble_en = _search_best_threshold(\n",
        "    val_author_ids_for_score,\n",
        "    y_en[val_idx],\n",
        "    post_prob_val_ensemble_en,\n",
        ")\n",
        "\n",
        "(\n",
        "    test_score_ensemble_en,\n",
        "    ensemble_tp_accounts_en,\n",
        "    ensemble_fn_accounts_en,\n",
        "    ensemble_fp_accounts_en,\n",
        "    ensemble_n_accounts_en,\n",
        ") = compute_account_score(\n",
        "    author_ids=test_author_ids_for_score,\n",
        "    true_labels=y_en[test_idx],\n",
        "    pred_probs=post_prob_test_ensemble_en,\n",
        "    threshold=selected_threshold_ensemble,\n",
        "    decision_rule=ACCOUNT_DECISION_RULE,\n",
        ")\n",
        "\n",
        "# Make the ensemble outputs the default baseline for downstream score/plot cells.\n",
        "post_prob_fit = post_prob_fit_ensemble_en\n",
        "post_prob_val = post_prob_val_ensemble_en\n",
        "post_prob_test = post_prob_test_ensemble_en\n",
        "y_prob = post_prob_test_ensemble_en\n",
        "SELECTED_THRESHOLD = float(selected_threshold_ensemble)\n",
        "test_score = int(test_score_ensemble_en)\n",
        "test_tp_accounts = int(ensemble_tp_accounts_en)\n",
        "test_fn_accounts = int(ensemble_fn_accounts_en)\n",
        "test_fp_accounts = int(ensemble_fp_accounts_en)\n",
        "test_n_accounts = int(ensemble_n_accounts_en)\n",
        "\n",
        "seed_mean_score_en = float(seed_report_df[\"test_score\"].mean())\n",
        "seed_std_score_en = float(seed_report_df[\"test_score\"].std(ddof=1)) if len(seed_report_df) > 1 else 0.0\n",
        "\n",
        "def build_account_feature_table(post_indices, post_probs, bot_threshold):\n",
        "    posts = train_en_model.iloc[post_indices].copy()\n",
        "    posts[\"post_prob\"] = np.asarray(post_probs, dtype=np.float32)\n",
        "\n",
        "    posts[\"has_url_post\"] = (posts[\"url_count\"] > 0).astype(np.float32)\n",
        "    posts[\"has_mention_post\"] = (posts[\"mention_count\"] > 0).astype(np.float32)\n",
        "    posts[\"has_hashtag_post\"] = (posts[\"hashtag_count\"] > 0).astype(np.float32)\n",
        "    posts[\"pred_bot_post\"] = (posts[\"post_prob\"] >= bot_threshold).astype(np.float32)\n",
        "\n",
        "    agg_spec = {\n",
        "        \"is_bot\": [\"max\"],\n",
        "        \"text_clean\": [\"size\"],\n",
        "        \"post_prob\": [\"mean\", \"std\", \"max\", \"min\"],\n",
        "        \"char_count\": [\"mean\", \"std\", \"max\"],\n",
        "        \"word_count\": [\"mean\", \"std\", \"max\"],\n",
        "        \"url_count\": [\"mean\", \"max\"],\n",
        "        \"mention_count\": [\"mean\", \"max\"],\n",
        "        \"hashtag_count\": [\"mean\", \"max\"],\n",
        "        \"exclamation_count\": [\"mean\", \"max\"],\n",
        "        \"question_count\": [\"mean\", \"max\"],\n",
        "        \"has_url_post\": [\"mean\"],\n",
        "        \"has_mention_post\": [\"mean\"],\n",
        "        \"has_hashtag_post\": [\"mean\"],\n",
        "        \"pred_bot_post\": [\"mean\"],\n",
        "    }\n",
        "\n",
        "    for topic_col in topic_feature_cols_en:\n",
        "        agg_spec[topic_col] = [\"mean\"]\n",
        "\n",
        "    account = posts.groupby(\"author_id\", as_index=False).agg(agg_spec)\n",
        "    account.columns = [\n",
        "        \"author_id\" if col == (\"author_id\", \"\") else f\"{col[0]}_{col[1]}\"\n",
        "        for col in account.columns.to_flat_index()\n",
        "    ]\n",
        "\n",
        "    account = account.rename(columns={\"is_bot_max\": \"true_is_bot\", \"text_clean_size\": \"n_posts\"})\n",
        "    for col in [\"post_prob_std\", \"char_count_std\", \"word_count_std\"]:\n",
        "        if col in account.columns:\n",
        "            account[col] = account[col].fillna(0.0)\n",
        "    account[\"n_posts_log1p\"] = np.log1p(account[\"n_posts\"].astype(np.float32))\n",
        "\n",
        "    user_source = (\n",
        "        users_en_labeled.copy()\n",
        "        if \"users_en_labeled\" in globals()\n",
        "        else users_en.drop_duplicates(subset=\"id\", keep=\"last\").copy()\n",
        "    )\n",
        "    user_source[\"username_len\"] = user_source[\"username\"].fillna(\"\").str.len()\n",
        "    user_source[\"name_len\"] = user_source[\"name\"].fillna(\"\").str.len()\n",
        "    user_source[\"description_len\"] = user_source[\"description\"].fillna(\"\").str.len()\n",
        "    user_source[\"has_location\"] = user_source[\"location\"].fillna(\"\").str.strip().ne(\"\").astype(np.float32)\n",
        "\n",
        "    user_features = user_source[\n",
        "        [\n",
        "            \"id\",\n",
        "            \"tweet_count\",\n",
        "            \"z_score\",\n",
        "            \"username_len\",\n",
        "            \"name_len\",\n",
        "            \"description_len\",\n",
        "            \"has_location\",\n",
        "        ]\n",
        "    ].copy()\n",
        "\n",
        "    account = account.merge(user_features, left_on=\"author_id\", right_on=\"id\", how=\"left\").drop(columns=[\"id\"])\n",
        "\n",
        "    numeric_cols = [col for col in account.columns if col != \"author_id\"]\n",
        "    account[numeric_cols] = account[numeric_cols].apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
        "    return account\n",
        "\n",
        "def score_from_account_probs(true_labels, pred_probs, threshold):\n",
        "    pred_labels = (pred_probs >= threshold).astype(np.int64)\n",
        "    tp = int(((true_labels == 1) & (pred_labels == 1)).sum())\n",
        "    fn = int(((true_labels == 1) & (pred_labels == 0)).sum())\n",
        "    fp = int(((true_labels == 0) & (pred_labels == 1)).sum())\n",
        "    score = (4 * tp) - (1 * fn) - (2 * fp)\n",
        "    return score, tp, fn, fp, pred_labels\n",
        "\n",
        "def _search_threshold_for_account_probs(y_true, y_prob):\n",
        "    if len(y_true) == 0:\n",
        "        return float(PREDICTION_THRESHOLD), None, 0, 0, 0, pd.DataFrame()\n",
        "\n",
        "    if not USE_THRESHOLD_SEARCH:\n",
        "        score, tp, fn, fp, _ = score_from_account_probs(y_true, y_prob, float(PREDICTION_THRESHOLD))\n",
        "        return float(PREDICTION_THRESHOLD), int(score), int(tp), int(fn), int(fp), pd.DataFrame()\n",
        "\n",
        "    rows = []\n",
        "    for threshold in np.linspace(THRESHOLD_SEARCH_MIN, THRESHOLD_SEARCH_MAX, THRESHOLD_SEARCH_STEPS):\n",
        "        score, tp, fn, fp, _ = score_from_account_probs(y_true, y_prob, float(threshold))\n",
        "        rows.append(\n",
        "            {\n",
        "                \"threshold\": float(threshold),\n",
        "                \"score\": int(score),\n",
        "                \"tp_accounts\": int(tp),\n",
        "                \"fn_accounts\": int(fn),\n",
        "                \"fp_accounts\": int(fp),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    df = pd.DataFrame(rows)\n",
        "    best_row = df.sort_values(\n",
        "        by=[\"score\", \"fp_accounts\", \"tp_accounts\", \"threshold\"],\n",
        "        ascending=[False, True, False, False],\n",
        "    ).iloc[0]\n",
        "\n",
        "    return (\n",
        "        float(best_row[\"threshold\"]),\n",
        "        int(best_row[\"score\"]),\n",
        "        int(best_row[\"tp_accounts\"]),\n",
        "        int(best_row[\"fn_accounts\"]),\n",
        "        int(best_row[\"fp_accounts\"]),\n",
        "        df,\n",
        "    )\n",
        "\n",
        "def _make_second_stage_candidates():\n",
        "    candidates = {}\n",
        "\n",
        "    legacy_params = {\n",
        "        \"iterations\": 300,\n",
        "        \"learning_rate\": 0.05,\n",
        "        \"depth\": 4,\n",
        "        \"l2_leaf_reg\": 0.2,\n",
        "        \"loss_function\": \"Logloss\",\n",
        "        \"eval_metric\": \"AUC\",\n",
        "        \"random_seed\": RANDOM_SEED,\n",
        "        \"verbose\": False,\n",
        "    }\n",
        "\n",
        "    regularized_params = {\n",
        "        \"iterations\": SECOND_STAGE_MAX_ITER,\n",
        "        \"learning_rate\": SECOND_STAGE_LEARNING_RATE,\n",
        "        \"depth\": SECOND_STAGE_MAX_DEPTH,\n",
        "        \"l2_leaf_reg\": SECOND_STAGE_L2,\n",
        "        \"min_data_in_leaf\": SECOND_STAGE_MIN_DATA_IN_LEAF,\n",
        "        \"loss_function\": \"Logloss\",\n",
        "        \"eval_metric\": \"AUC\",\n",
        "        \"random_seed\": RANDOM_SEED,\n",
        "        \"verbose\": False,\n",
        "        \"od_type\": \"Iter\",\n",
        "        \"od_wait\": SECOND_STAGE_OD_WAIT,\n",
        "    }\n",
        "    if 0.0 < SECOND_STAGE_SUBSAMPLE < 1.0:\n",
        "        regularized_params[\"subsample\"] = SECOND_STAGE_SUBSAMPLE\n",
        "        regularized_params[\"bootstrap_type\"] = \"Bernoulli\"\n",
        "    if 0.0 < SECOND_STAGE_RSM <= 1.0:\n",
        "        regularized_params[\"rsm\"] = SECOND_STAGE_RSM\n",
        "\n",
        "    custom_params = {\n",
        "        \"iterations\": SECOND_STAGE_MAX_ITER,\n",
        "        \"learning_rate\": SECOND_STAGE_LEARNING_RATE,\n",
        "        \"depth\": SECOND_STAGE_MAX_DEPTH,\n",
        "        \"l2_leaf_reg\": SECOND_STAGE_L2,\n",
        "        \"min_data_in_leaf\": SECOND_STAGE_MIN_DATA_IN_LEAF,\n",
        "        \"loss_function\": \"Logloss\",\n",
        "        \"eval_metric\": \"AUC\",\n",
        "        \"random_seed\": RANDOM_SEED,\n",
        "        \"verbose\": False,\n",
        "        \"od_type\": \"Iter\",\n",
        "        \"od_wait\": SECOND_STAGE_OD_WAIT,\n",
        "    }\n",
        "    if 0.0 < SECOND_STAGE_SUBSAMPLE < 1.0:\n",
        "        custom_params[\"subsample\"] = SECOND_STAGE_SUBSAMPLE\n",
        "        custom_params[\"bootstrap_type\"] = \"Bernoulli\"\n",
        "    if 0.0 < SECOND_STAGE_RSM <= 1.0:\n",
        "        custom_params[\"rsm\"] = SECOND_STAGE_RSM\n",
        "\n",
        "    if SECOND_STAGE_PROFILE in {\"auto\", \"legacy\"}:\n",
        "        candidates[\"legacy\"] = legacy_params\n",
        "    if SECOND_STAGE_PROFILE in {\"auto\", \"regularized\"}:\n",
        "        candidates[\"regularized\"] = regularized_params\n",
        "    if SECOND_STAGE_PROFILE == \"custom\":\n",
        "        candidates[\"custom\"] = custom_params\n",
        "\n",
        "    if SECOND_STAGE_USE_BALANCED_WEIGHTS:\n",
        "        for params in candidates.values():\n",
        "            params[\"auto_class_weights\"] = \"Balanced\"\n",
        "\n",
        "    return candidates\n",
        "\n",
        "if USE_SECOND_STAGE_ACCOUNT_MODEL:\n",
        "    post_prob_fit_for_second_stage_en = post_prob_fit_ensemble_en.copy()\n",
        "    second_stage_fit_feature_source_en = \"in_sample_ensemble\"\n",
        "    second_stage_oof_report_en = pd.DataFrame()\n",
        "\n",
        "    if SECOND_STAGE_USE_OOF_FEATURES:\n",
        "        print(\n",
        "            f\"Building OOF first-stage features for second-stage training (seeds={second_stage_oof_seeds}, folds={SECOND_STAGE_OOF_FOLDS}, epochs={SECOND_STAGE_OOF_EPOCHS})...\"\n",
        "        )\n",
        "        oof_fit_prob_list = []\n",
        "        oof_rows = []\n",
        "\n",
        "        for oof_seed in second_stage_oof_seeds:\n",
        "            oof_probs_seed = _compute_oof_post_probs_for_seed(\n",
        "                seed=int(oof_seed),\n",
        "                post_indices=fit_idx,\n",
        "                use_external_pretrain_for_oof=bool(USE_EXTERNAL_PRETRAIN and SECOND_STAGE_OOF_USE_EXTERNAL_PRETRAIN),\n",
        "            )\n",
        "\n",
        "            oof_score_seed, oof_tp, oof_fn, oof_fp, oof_accounts = compute_account_score(\n",
        "                author_ids=fit_author_ids_for_score,\n",
        "                true_labels=y_en[fit_idx],\n",
        "                pred_probs=oof_probs_seed,\n",
        "                threshold=selected_threshold_ensemble,\n",
        "                decision_rule=ACCOUNT_DECISION_RULE,\n",
        "            )\n",
        "\n",
        "            oof_rows.append(\n",
        "                {\n",
        "                    \"seed\": int(oof_seed),\n",
        "                    \"fit_account_score_at_ensemble_threshold\": int(oof_score_seed),\n",
        "                    \"tp_accounts\": int(oof_tp),\n",
        "                    \"fn_accounts\": int(oof_fn),\n",
        "                    \"fp_accounts\": int(oof_fp),\n",
        "                    \"n_accounts\": int(oof_accounts),\n",
        "                }\n",
        "            )\n",
        "\n",
        "            oof_fit_prob_list.append(oof_probs_seed)\n",
        "\n",
        "        post_prob_fit_for_second_stage_en = _aggregate_probabilities(oof_fit_prob_list, ENSEMBLE_AGGREGATION)\n",
        "        second_stage_fit_feature_source_en = \"oof_first_stage\"\n",
        "        second_stage_oof_report_en = pd.DataFrame(oof_rows)\n",
        "\n",
        "    fit_account_df = build_account_feature_table(fit_idx, post_prob_fit_for_second_stage_en, selected_threshold_ensemble)\n",
        "    val_account_df = (\n",
        "        build_account_feature_table(val_idx, post_prob_val_ensemble_en, selected_threshold_ensemble)\n",
        "        if has_validation\n",
        "        else pd.DataFrame()\n",
        "    )\n",
        "    test_account_df = build_account_feature_table(test_idx, post_prob_test_ensemble_en, selected_threshold_ensemble)\n",
        "\n",
        "    target_col = \"true_is_bot\"\n",
        "    feature_cols_account = [col for col in fit_account_df.columns if col not in {\"author_id\", target_col}]\n",
        "\n",
        "    for df in [fit_account_df, val_account_df, test_account_df]:\n",
        "        if df.empty:\n",
        "            continue\n",
        "        missing_cols = [col for col in feature_cols_account if col not in df.columns]\n",
        "        for col in missing_cols:\n",
        "            df[col] = 0.0\n",
        "\n",
        "    X_fit_acc = fit_account_df[feature_cols_account].to_numpy(dtype=np.float32)\n",
        "    y_fit_acc = fit_account_df[target_col].to_numpy(dtype=np.int64)\n",
        "    X_test_acc = test_account_df[feature_cols_account].to_numpy(dtype=np.float32)\n",
        "    y_test_acc = test_account_df[target_col].to_numpy(dtype=np.int64)\n",
        "\n",
        "    if len(val_account_df):\n",
        "        X_val_acc = val_account_df[feature_cols_account].to_numpy(dtype=np.float32)\n",
        "        y_val_acc = val_account_df[target_col].to_numpy(dtype=np.int64)\n",
        "    else:\n",
        "        X_val_acc = np.zeros((0, len(feature_cols_account)), dtype=np.float32)\n",
        "        y_val_acc = np.zeros((0,), dtype=np.int64)\n",
        "\n",
        "    baseline_account_val_score = None\n",
        "    baseline_account_val_threshold = float(selected_threshold_ensemble)\n",
        "    baseline_account_val_tp = 0\n",
        "    baseline_account_val_fn = 0\n",
        "    baseline_account_val_fp = 0\n",
        "\n",
        "    base_val_prob_acc = val_account_df[\"post_prob_mean\"].to_numpy(dtype=np.float32) if len(y_val_acc) else np.array([])\n",
        "    base_test_prob_acc = test_account_df[\"post_prob_mean\"].to_numpy(dtype=np.float32)\n",
        "\n",
        "    if len(y_val_acc):\n",
        "        (\n",
        "            baseline_account_val_threshold,\n",
        "            baseline_account_val_score,\n",
        "            baseline_account_val_tp,\n",
        "            baseline_account_val_fn,\n",
        "            baseline_account_val_fp,\n",
        "            baseline_account_threshold_search_en,\n",
        "        ) = _search_threshold_for_account_probs(y_val_acc, base_val_prob_acc)\n",
        "    else:\n",
        "        baseline_account_threshold_search_en = pd.DataFrame()\n",
        "\n",
        "    candidate_params = _make_second_stage_candidates()\n",
        "    candidate_rows = []\n",
        "    candidate_artifacts = []\n",
        "\n",
        "    for profile_name, params in candidate_params.items():\n",
        "        model = CatBoostClassifier(**params)\n",
        "\n",
        "        fit_kwargs = {}\n",
        "        if len(y_val_acc):\n",
        "            fit_kwargs[\"eval_set\"] = (X_val_acc, y_val_acc)\n",
        "            fit_kwargs[\"use_best_model\"] = True\n",
        "\n",
        "        model.fit(X_fit_acc, y_fit_acc, **fit_kwargs)\n",
        "\n",
        "        test_raw_prob = model.predict_proba(X_test_acc)[:, 1]\n",
        "        val_raw_prob = model.predict_proba(X_val_acc)[:, 1] if len(y_val_acc) else np.array([])\n",
        "\n",
        "        if len(y_val_acc):\n",
        "            best_candidate = None\n",
        "            for alpha in (SECOND_STAGE_BLEND_ALPHAS if SECOND_STAGE_USE_BLEND else [1.0]):\n",
        "                val_candidate_prob = (alpha * val_raw_prob) + ((1.0 - alpha) * base_val_prob_acc)\n",
        "                (\n",
        "                    cand_threshold,\n",
        "                    cand_val_score,\n",
        "                    cand_val_tp,\n",
        "                    cand_val_fn,\n",
        "                    cand_val_fp,\n",
        "                    cand_search_df,\n",
        "                ) = _search_threshold_for_account_probs(y_val_acc, val_candidate_prob)\n",
        "\n",
        "                if (\n",
        "                    best_candidate is None\n",
        "                    or cand_val_score > best_candidate[\"val_score\"]\n",
        "                    or (\n",
        "                        cand_val_score == best_candidate[\"val_score\"]\n",
        "                        and cand_val_fp < best_candidate[\"val_fp_accounts\"]\n",
        "                    )\n",
        "                    or (\n",
        "                        cand_val_score == best_candidate[\"val_score\"]\n",
        "                        and cand_val_fp == best_candidate[\"val_fp_accounts\"]\n",
        "                        and cand_val_tp > best_candidate[\"val_tp_accounts\"]\n",
        "                    )\n",
        "                ):\n",
        "                    best_candidate = {\n",
        "                        \"alpha\": float(alpha),\n",
        "                        \"threshold\": float(cand_threshold),\n",
        "                        \"val_score\": int(cand_val_score),\n",
        "                        \"val_tp_accounts\": int(cand_val_tp),\n",
        "                        \"val_fn_accounts\": int(cand_val_fn),\n",
        "                        \"val_fp_accounts\": int(cand_val_fp),\n",
        "                        \"search_df\": cand_search_df,\n",
        "                    }\n",
        "\n",
        "            chosen_alpha = best_candidate[\"alpha\"]\n",
        "            chosen_threshold = best_candidate[\"threshold\"]\n",
        "            chosen_val_score = best_candidate[\"val_score\"]\n",
        "            chosen_val_tp = best_candidate[\"val_tp_accounts\"]\n",
        "            chosen_val_fn = best_candidate[\"val_fn_accounts\"]\n",
        "            chosen_val_fp = best_candidate[\"val_fp_accounts\"]\n",
        "            threshold_df = best_candidate[\"search_df\"]\n",
        "        else:\n",
        "            chosen_alpha = 1.0\n",
        "            chosen_threshold = float(selected_threshold_ensemble)\n",
        "            chosen_val_score = np.nan\n",
        "            chosen_val_tp = np.nan\n",
        "            chosen_val_fn = np.nan\n",
        "            chosen_val_fp = np.nan\n",
        "            threshold_df = pd.DataFrame()\n",
        "\n",
        "        test_blend_prob = (chosen_alpha * test_raw_prob) + ((1.0 - chosen_alpha) * base_test_prob_acc)\n",
        "        (\n",
        "            test_score_profile,\n",
        "            test_tp_profile,\n",
        "            test_fn_profile,\n",
        "            test_fp_profile,\n",
        "            test_pred_profile,\n",
        "        ) = score_from_account_probs(y_test_acc, test_blend_prob, chosen_threshold)\n",
        "\n",
        "        candidate_rows.append(\n",
        "            {\n",
        "                \"profile\": profile_name,\n",
        "                \"alpha\": float(chosen_alpha),\n",
        "                \"threshold\": float(chosen_threshold),\n",
        "                \"val_score\": None if pd.isna(chosen_val_score) else int(chosen_val_score),\n",
        "                \"val_tp_accounts\": None if pd.isna(chosen_val_tp) else int(chosen_val_tp),\n",
        "                \"val_fn_accounts\": None if pd.isna(chosen_val_fn) else int(chosen_val_fn),\n",
        "                \"val_fp_accounts\": None if pd.isna(chosen_val_fp) else int(chosen_val_fp),\n",
        "                \"test_score\": int(test_score_profile),\n",
        "                \"test_tp_accounts\": int(test_tp_profile),\n",
        "                \"test_fn_accounts\": int(test_fn_profile),\n",
        "                \"test_fp_accounts\": int(test_fp_profile),\n",
        "            }\n",
        "        )\n",
        "\n",
        "        candidate_artifacts.append(\n",
        "            {\n",
        "                \"profile\": profile_name,\n",
        "                \"model\": model,\n",
        "                \"alpha\": float(chosen_alpha),\n",
        "                \"threshold\": float(chosen_threshold),\n",
        "                \"val_score\": chosen_val_score,\n",
        "                \"val_tp_accounts\": chosen_val_tp,\n",
        "                \"val_fn_accounts\": chosen_val_fn,\n",
        "                \"val_fp_accounts\": chosen_val_fp,\n",
        "                \"threshold_search_df\": threshold_df,\n",
        "                \"test_prob\": test_blend_prob,\n",
        "                \"test_pred\": test_pred_profile,\n",
        "                \"test_score\": int(test_score_profile),\n",
        "                \"test_tp_accounts\": int(test_tp_profile),\n",
        "                \"test_fn_accounts\": int(test_fn_profile),\n",
        "                \"test_fp_accounts\": int(test_fp_profile),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    second_stage_candidate_report_en = pd.DataFrame(candidate_rows)\n",
        "\n",
        "    if second_stage_candidate_report_en.empty:\n",
        "        raise ValueError(\"No second-stage candidate was trained. Check second_stage_profile.\")\n",
        "\n",
        "    if len(y_val_acc):\n",
        "        sorted_candidates = second_stage_candidate_report_en.sort_values(\n",
        "            by=[\"val_score\", \"val_fp_accounts\", \"val_tp_accounts\", \"threshold\", \"alpha\"],\n",
        "            ascending=[False, True, False, False, False],\n",
        "            na_position=\"last\",\n",
        "        )\n",
        "    else:\n",
        "        sorted_candidates = second_stage_candidate_report_en.sort_values(\n",
        "            by=[\"test_score\", \"test_fp_accounts\", \"test_tp_accounts\"],\n",
        "            ascending=[False, True, False],\n",
        "        )\n",
        "\n",
        "    best_profile_name = str(sorted_candidates.iloc[0][\"profile\"])\n",
        "    selected_artifact = next(item for item in candidate_artifacts if item[\"profile\"] == best_profile_name)\n",
        "\n",
        "    second_stage_selected_profile_en = best_profile_name\n",
        "    second_stage_used_fallback_en = False\n",
        "\n",
        "    if len(y_val_acc):\n",
        "        best_val_score = int(selected_artifact[\"val_score\"])\n",
        "        baseline_ref_score = int(baseline_account_val_score) if baseline_account_val_score is not None else int(best_ensemble_val_score or 0)\n",
        "\n",
        "        if best_val_score < (baseline_ref_score + SECOND_STAGE_MIN_GAIN_VS_ENSEMBLE):\n",
        "            second_stage_used_fallback_en = True\n",
        "            second_stage_selected_profile_en = \"fallback_ensemble\"\n",
        "\n",
        "    if second_stage_used_fallback_en:\n",
        "        second_stage_model_en = None\n",
        "        second_stage_selected_threshold_en = float(selected_threshold_ensemble)\n",
        "        second_stage_alpha_en = 0.0\n",
        "        second_stage_test_score_en = int(test_score_ensemble_en)\n",
        "        second_stage_tp_accounts_en = int(ensemble_tp_accounts_en)\n",
        "        second_stage_fn_accounts_en = int(ensemble_fn_accounts_en)\n",
        "        second_stage_fp_accounts_en = int(ensemble_fp_accounts_en)\n",
        "\n",
        "        baseline_account_eval = pd.DataFrame(\n",
        "            {\n",
        "                \"author_id\": test_author_ids_for_score,\n",
        "                \"true_is_bot\": y_en[test_idx].astype(np.int64),\n",
        "                \"post_prob\": post_prob_test_ensemble_en.astype(np.float32),\n",
        "            }\n",
        "        )\n",
        "        baseline_account_eval[\"pred_post\"] = (baseline_account_eval[\"post_prob\"] >= selected_threshold_ensemble).astype(np.int64)\n",
        "        baseline_account_eval = (\n",
        "            baseline_account_eval.groupby(\"author_id\", as_index=False)\n",
        "            .agg(\n",
        "                true_is_bot=(\"true_is_bot\", \"max\"),\n",
        "                mean_prob=(\"post_prob\", \"mean\"),\n",
        "                any_pred=(\"pred_post\", \"max\"),\n",
        "            )\n",
        "        )\n",
        "\n",
        "        if ACCOUNT_DECISION_RULE == \"any\":\n",
        "            baseline_account_eval[\"pred_is_bot\"] = baseline_account_eval[\"any_pred\"].astype(np.int64)\n",
        "            baseline_account_eval[\"pred_prob\"] = baseline_account_eval[\"any_pred\"].astype(np.float32)\n",
        "        else:\n",
        "            baseline_account_eval[\"pred_is_bot\"] = (\n",
        "                baseline_account_eval[\"mean_prob\"] >= selected_threshold_ensemble\n",
        "            ).astype(np.int64)\n",
        "            baseline_account_eval[\"pred_prob\"] = baseline_account_eval[\"mean_prob\"].astype(np.float32)\n",
        "\n",
        "        second_stage_test_prob_en = baseline_account_eval[\"pred_prob\"].to_numpy(dtype=np.float32)\n",
        "        second_stage_test_pred_en = baseline_account_eval[\"pred_is_bot\"].to_numpy(dtype=np.int64)\n",
        "\n",
        "        second_stage_account_predictions_en = baseline_account_eval[\n",
        "            [\"author_id\", \"true_is_bot\", \"pred_prob\", \"pred_is_bot\"]\n",
        "        ].copy()\n",
        "\n",
        "        second_stage_threshold_search_results_en = pd.DataFrame()\n",
        "    else:\n",
        "        second_stage_model_en = selected_artifact[\"model\"]\n",
        "        second_stage_selected_threshold_en = float(selected_artifact[\"threshold\"])\n",
        "        second_stage_alpha_en = float(selected_artifact[\"alpha\"])\n",
        "        second_stage_threshold_search_results_en = selected_artifact[\"threshold_search_df\"]\n",
        "        second_stage_test_prob_en = np.asarray(selected_artifact[\"test_prob\"], dtype=np.float32)\n",
        "        second_stage_test_pred_en = np.asarray(selected_artifact[\"test_pred\"], dtype=np.int64)\n",
        "        second_stage_test_score_en = int(selected_artifact[\"test_score\"])\n",
        "        second_stage_tp_accounts_en = int(selected_artifact[\"test_tp_accounts\"])\n",
        "        second_stage_fn_accounts_en = int(selected_artifact[\"test_fn_accounts\"])\n",
        "        second_stage_fp_accounts_en = int(selected_artifact[\"test_fp_accounts\"])\n",
        "\n",
        "        second_stage_account_predictions_en = test_account_df[[\"author_id\", target_col]].copy()\n",
        "        second_stage_account_predictions_en[\"pred_prob\"] = second_stage_test_prob_en\n",
        "        second_stage_account_predictions_en[\"pred_is_bot\"] = second_stage_test_pred_en\n",
        "\n",
        "    max_score_accounts = 4 * int((y_test_acc == 1).sum())\n",
        "    second_stage_score_ratio_en = (\n",
        "        second_stage_test_score_en / max_score_accounts if max_score_accounts > 0 else np.nan\n",
        "    )\n",
        "\n",
        "    max_possible_score = max_score_accounts\n",
        "\n",
        "print(\"Seed-level first-stage account scores (test):\")\n",
        "print(\n",
        "    seed_report_df[\n",
        "        [\n",
        "            \"seed\",\n",
        "            \"threshold\",\n",
        "            \"test_score\",\n",
        "            \"tp_accounts\",\n",
        "            \"fn_accounts\",\n",
        "            \"fp_accounts\",\n",
        "        ]\n",
        "    ].to_string(index=False)\n",
        ")\n",
        "print(f\"Seed score mean/std: {seed_mean_score_en:.2f} / {seed_std_score_en:.2f}\")\n",
        "print(f\"Ensemble aggregation: {ENSEMBLE_AGGREGATION}\")\n",
        "print(f\"Ensemble selected threshold: {selected_threshold_ensemble:.4f}\")\n",
        "print(\n",
        "    f\"Ensemble test score: {test_score_ensemble_en} (TP={ensemble_tp_accounts_en}, FN={ensemble_fn_accounts_en}, FP={ensemble_fp_accounts_en}, accounts={ensemble_n_accounts_en})\"\n",
        ")\n",
        "\n",
        "if USE_SECOND_STAGE_ACCOUNT_MODEL:\n",
        "    print(f\"Second-stage fit feature source: {second_stage_fit_feature_source_en}\")\n",
        "    if not second_stage_oof_report_en.empty:\n",
        "        print(\"OOF seed report for second-stage fit features:\")\n",
        "        print(second_stage_oof_report_en.to_string(index=False))\n",
        "\n",
        "    print(\"Second-stage candidate report:\")\n",
        "    print(second_stage_candidate_report_en.to_string(index=False))\n",
        "\n",
        "    if has_validation and baseline_account_val_score is not None:\n",
        "        print(\n",
        "            f\"Baseline account-level validation score (from first-stage means): {baseline_account_val_score} \"\n",
        "            f\"(TP={baseline_account_val_tp}, FN={baseline_account_val_fn}, FP={baseline_account_val_fp}, \"\n",
        "            f\"threshold={baseline_account_val_threshold:.4f})\"\n",
        "        )\n",
        "\n",
        "    print(f\"Second-stage profile mode: {SECOND_STAGE_PROFILE}\")\n",
        "    print(f\"Second-stage selected profile: {second_stage_selected_profile_en}\")\n",
        "    if second_stage_selected_profile_en != \"fallback_ensemble\":\n",
        "        print(f\"Second-stage blend alpha (CatBoost weight): {second_stage_alpha_en:.2f}\")\n",
        "        print(f\"Second-stage threshold: {second_stage_selected_threshold_en:.4f}\")\n",
        "    else:\n",
        "        print(\n",
        "            \"Second-stage fallback triggered: no candidate beat baseline validation score by the configured margin.\"\n",
        "        )\n",
        "\n",
        "    print(\n",
        "        f\"Second-stage test score: {second_stage_test_score_en}/{max_possible_score} ({second_stage_score_ratio_en:.2%})\"\n",
        "    )\n",
        "    print(\n",
        "        f\"Second-stage confusion components -> TP={second_stage_tp_accounts_en}, FN={second_stage_fn_accounts_en}, FP={second_stage_fp_accounts_en}\"\n",
        "    )\n",
        "    print(\n",
        "        f\"Second-stage precision={precision_score(y_test_acc, second_stage_test_pred_en, zero_division=0):.4f}, \"\n",
        "        f\"recall={recall_score(y_test_acc, second_stage_test_pred_en, zero_division=0):.4f}\"\n",
        "    )\n",
        "    print(classification_report(y_test_acc, second_stage_test_pred_en, digits=4))\n",
        "else:\n",
        "    print(\"Second-stage account model disabled in EXPERIMENT_CONFIG.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d26c4f9a",
      "metadata": {},
      "source": [
        "## Final score plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "c477e2ab",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 800x450 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Without booster: 77\n",
            "With booster: 92\n",
            "Competition top (max possible on this split): 104\n"
          ]
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "if \"train_en_model\" not in globals() or \"test_idx\" not in globals():\n",
        "    raise ValueError(\"Run the training cell first.\")\n",
        "\n",
        "score_without_booster = None\n",
        "if \"test_score\" in globals():\n",
        "    score_without_booster = float(test_score)\n",
        "elif \"score\" in globals():\n",
        "    score_without_booster = float(score)\n",
        "else:\n",
        "    raise ValueError(\"No baseline score found. Run the model scoring cells first.\")\n",
        "\n",
        "score_with_booster = float(second_stage_test_score_en) if \"second_stage_test_score_en\" in globals() else None\n",
        "\n",
        "if \"max_possible_score\" in globals():\n",
        "    competition_top = float(max_possible_score)\n",
        "else:\n",
        "    account_truth = (\n",
        "        train_en_model.iloc[test_idx][[\"author_id\", \"is_bot\"]]\n",
        "        .groupby(\"author_id\", as_index=False)[\"is_bot\"]\n",
        "        .max()\n",
        "    )\n",
        "    competition_top = float(4 * int(account_truth[\"is_bot\"].sum()))\n",
        "\n",
        "labels = [\"Without booster\"]\n",
        "scores = [score_without_booster]\n",
        "colors = [\"#4c78a8\"]\n",
        "\n",
        "if score_with_booster is not None:\n",
        "    labels.append(\"With booster\")\n",
        "    scores.append(score_with_booster)\n",
        "    colors.append(\"#f58518\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(8, 4.5))\n",
        "bars = ax.bar(labels, scores, color=colors, alpha=0.9)\n",
        "ax.axhline(competition_top, color=\"#54a24b\", linestyle=\"--\", linewidth=2, label=f\"Competition top: {competition_top:.0f}\")\n",
        "\n",
        "for bar, value in zip(bars, scores):\n",
        "    ax.text(\n",
        "        bar.get_x() + bar.get_width() / 2,\n",
        "        value,\n",
        "        f\"{value:.0f}\",\n",
        "        ha=\"center\",\n",
        "        va=\"bottom\",\n",
        "        fontsize=10,\n",
        "        fontweight=\"bold\",\n",
        "    )\n",
        "\n",
        "ax.set_title(\"Competition Score Comparison\")\n",
        "ax.set_ylabel(\"Score\")\n",
        "ax.set_ylim(0, max(max(scores), competition_top) * 1.15)\n",
        "ax.grid(axis=\"y\", linestyle=\":\", alpha=0.4)\n",
        "ax.legend(loc=\"upper left\")\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"Without booster: {score_without_booster:.0f}\")\n",
        "if score_with_booster is not None:\n",
        "    print(f\"With booster: {score_with_booster:.0f}\")\n",
        "print(f\"Competition top (max possible on this split): {competition_top:.0f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e9ef7ab6",
      "metadata": {},
      "source": [
        "## Error analysis (aggregate only)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "fc8171e5",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Post-level error summary (first-stage ensemble):\n",
            "error_type  n_posts\n",
            "        TN     2159\n",
            "        TP      706\n",
            "        FP      331\n",
            "        FN      210\n",
            "\n",
            "No topic matches found in the test split.\n",
            "\n",
            "Account-level errors by dominant topic:\n",
            "dominant_topic  n_accounts  bot_rate  fp_accounts  fn_accounts  tp_accounts  tn_accounts  error_rate\n",
            "no_topic_match         110  0.236364            6            0           26           78    0.054545\n",
            "\n",
            "Sample hard cases (for pattern inspection, not rule memorization):\n",
            "                           author_id                                                                                                                                                                                                                                                                                                                   text_clean  pred_prob error_type\n",
            "4d03e547-f849-8c68-bbbe-8dd7974855ca                                     volcanos, oceans, jungles, ice, sun, moon, food from the ground, trees, berries, papayas, fish, potatoes, cows, bread fruit, pigs, lettuce, and spices, and people paying rent. amazing how brains are fooled, like a spell casted dark magic over populations keeping us from maturing.   0.850806         FP\n",
            "1af28907-1303-bb1c-92ed-f57c6c8b8c2a                                                                                                                                                                                                                                     babe, look! the world is celebrating my everyday! my birthday! https://t.co/twitter_link   0.815605         FP\n",
            "4d03e547-f849-8c68-bbbe-8dd7974855ca                                            Prison music has genres too, rap, reggae, pop, rock, whatever is mainstream is practically prisoner music, even if its local, people in hawaii listen to the same songs for the past 25 years. I like anti prison music, but it too can make its way into the prisoner air waves.   0.796712         FP\n",
            "3dd13f5a-7d7f-923a-a7e4-22ac42b0fd90                                                                                                                                                                                                                     Why am I just hearing Taylor’s demo for This Is What You Came For for the first time today? Soooooo good   0.786214         FP\n",
            "cb3becd6-ecd9-a9ed-9f0a-f9e652c39596                                                                                                                                     GOOD MORNING???? I open X to find this???? That's the mysterious Foot-on -sofa IG pic revealed! It's a set!! And live performances??? And Cartier!!!! Omgggggg https://t.co/twitter_link   0.772712         FP\n",
            "d1bf4b96-fede-aa9b-9853-b7a347e2261f                                                                                                                                                                                        my second fave thai gl loveteam #UnlockYourLoveTeaserxBmineNear #UnlockYourLoveYuri #BmineNear #บีมายเนียร์ https://t.co/twitter_link   0.762969         FP\n",
            "82e2f5df-a405-adc8-93f5-e33f446f1f48                                                                                                                                                                                                                                                               Omo! What an interesting game! Still tipping Liverpool to win!   0.761754         FP\n",
            "5fef3628-ba50-881f-bffd-4c150bb1720d                                                                                                                                                                                                                           What are Saturdays for? Gain! Gain!! Gain!!!👍 Just drop your handles. I will follow everyone asap.   0.761109         FP\n",
            "293cef2c-e668-ad7a-8441-caeccb9d4faf                                                                                                                                                 Why is my heart still broken after an entire year? I’ve put in more self work on myself in this past 12 months than I have in over a decade. Yet I can’t move on. Why? Geez.   0.756942         FP\n",
            "0a93570e-e34d-a3d0-8d91-5c014adbb615                                                                                                                                                                                                             Speaking of my nieces, they are my gal pals. If any woman is desirous to marry me, she must gain their approval.   0.748222         FP\n",
            "8b02b826-fdda-400f-90ad-8ff07c57b89b                                                                                                                                                                                                                                        not sure what i was thinking here https://t.co/twitter_link https://t.co/twitter_link   0.014462         FN\n",
            "affbdacc-ecce-4e88-b671-2e1ff85f0034 TW: ED * * * * * * I was looking study &amp; book twt since oomf recommended But I tried searching for study &amp; book communities on here &amp; this is the very first result. No judgment. It’s a disease &amp; I’ve been there but it sad to see this pro-ED culture prevalent and poisonous.🥺 https://t.co/twitter_link   0.015132         FN\n",
            "c2af8c73-2bc2-415b-abaa-2fd5083b56e8                                                                                                                                                                                                                                                Good grab from Mo. 😉 #LGRW https://t.co/witter_link https://t.co/twitter_link   0.026252         FN\n",
            "8b02b826-fdda-400f-90ad-8ff07c57b89b                                        Your man a bird he heard you and your 2 sisters arguing over him in the bedroom drunk and you said BITCH TALK ABOUT MY MAN SHABLIM and she said BITCH YOU THE ONE THAT TOLD ME HE HAD THE OLIVE DICK THAT NIGHT and he walked in the bedroom quiet with jordans on dressed nice Lmfao   0.028767         FN\n",
            "affbdacc-ecce-4e88-b671-2e1ff85f0034                                                                                                                                                                                                               Manchester United vs Liverpool is constantly toxic. We will be there no depend what! https://t.co/twitter_link   0.031481         FN\n",
            "affbdacc-ecce-4e88-b671-2e1ff85f0034                                                                                                                                                                                                                                     they look so chill here acting like they don’t hate each other https://t.co/twitter_link   0.032584         FN\n",
            "affbdacc-ecce-4e88-b671-2e1ff85f0034                                                                                                                                                                                                                     ok yea he also saw that level 3 ninja low taper fade skibidi toilet rizz gyatt https://t.co/twitter_link   0.034333         FN\n",
            "8b02b826-fdda-400f-90ad-8ff07c57b89b                                                                                                                                                                                                                                   I’m a big fan of John David Washington https://t.co/twitter_link https://t.co/twitter_link   0.038261         FN\n",
            "8b02b826-fdda-400f-90ad-8ff07c57b89b                                                                                                                                                                                                           ✅ Badgers (+5.5) Those Badgers come through for us once again. WHAT A GAME. 🦡 We move. 🤌 https://t.co/twitter_link   0.038301         FN\n",
            "8b02b826-fdda-400f-90ad-8ff07c57b89b                                                                                                                                                                                                   i hope i didnt come off like a fool or whatever i should have said alot more but i was so caught off guard and unprepared.   0.038542         FN\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "required = [\"train_en_model\", \"test_idx\", \"post_prob_test\"]\n",
        "missing = [name for name in required if name not in globals()]\n",
        "if missing:\n",
        "    raise ValueError(f\"Run the training and booster cells first. Missing: {missing}\")\n",
        "\n",
        "analysis_threshold = float(SELECTED_THRESHOLD if \"SELECTED_THRESHOLD\" in globals() else PREDICTION_THRESHOLD)\n",
        "\n",
        "posts_eval = train_en_model.iloc[test_idx].copy().reset_index(drop=True)\n",
        "posts_eval[\"true_is_bot\"] = posts_eval[\"is_bot\"].astype(np.int64)\n",
        "posts_eval[\"pred_prob\"] = np.asarray(post_prob_test, dtype=np.float32)\n",
        "posts_eval[\"pred_is_bot\"] = (posts_eval[\"pred_prob\"] >= analysis_threshold).astype(np.int64)\n",
        "\n",
        "posts_eval[\"error_type\"] = np.where(\n",
        "    (posts_eval[\"true_is_bot\"] == 1) & (posts_eval[\"pred_is_bot\"] == 0),\n",
        "    \"FN\",\n",
        "    np.where(\n",
        "        (posts_eval[\"true_is_bot\"] == 0) & (posts_eval[\"pred_is_bot\"] == 1),\n",
        "        \"FP\",\n",
        "        np.where(posts_eval[\"true_is_bot\"] == 1, \"TP\", \"TN\"),\n",
        "    ),\n",
        ")\n",
        "\n",
        "post_error_summary_en = (\n",
        "    posts_eval[\"error_type\"]\n",
        "    .value_counts()\n",
        "    .rename_axis(\"error_type\")\n",
        "    .reset_index(name=\"n_posts\")\n",
        ")\n",
        "\n",
        "print(\"Post-level error summary (first-stage ensemble):\")\n",
        "print(post_error_summary_en.to_string(index=False))\n",
        "\n",
        "if topic_feature_cols_en:\n",
        "    topic_rows = []\n",
        "    for topic_col in topic_feature_cols_en:\n",
        "        subset = posts_eval[posts_eval[topic_col] > 0]\n",
        "        if subset.empty:\n",
        "            continue\n",
        "\n",
        "        fp = int(((subset[\"true_is_bot\"] == 0) & (subset[\"pred_is_bot\"] == 1)).sum())\n",
        "        fn = int(((subset[\"true_is_bot\"] == 1) & (subset[\"pred_is_bot\"] == 0)).sum())\n",
        "        tp = int(((subset[\"true_is_bot\"] == 1) & (subset[\"pred_is_bot\"] == 1)).sum())\n",
        "        tn = int(((subset[\"true_is_bot\"] == 0) & (subset[\"pred_is_bot\"] == 0)).sum())\n",
        "        n = int(len(subset))\n",
        "\n",
        "        topic_rows.append(\n",
        "            {\n",
        "                \"topic\": topic_col.replace(\"topic_\", \"\"),\n",
        "                \"n_posts\": n,\n",
        "                \"fp_posts\": fp,\n",
        "                \"fn_posts\": fn,\n",
        "                \"tp_posts\": tp,\n",
        "                \"tn_posts\": tn,\n",
        "                \"error_rate\": (fp + fn) / n if n else 0.0,\n",
        "                \"bot_rate\": subset[\"true_is_bot\"].mean(),\n",
        "            }\n",
        "        )\n",
        "\n",
        "    topic_post_error_report_en = pd.DataFrame(topic_rows)\n",
        "    if not topic_post_error_report_en.empty:\n",
        "        min_topic_posts = int(EXPERIMENT_CONFIG.get(\"error_analysis_min_topic_posts\", 20))\n",
        "        topic_post_error_report_en = topic_post_error_report_en.sort_values(\n",
        "            by=[\"error_rate\", \"n_posts\"], ascending=[False, False]\n",
        "        )\n",
        "\n",
        "        print(\"\\nTopic-level post errors (filtering tiny buckets):\")\n",
        "        print(\n",
        "            topic_post_error_report_en[topic_post_error_report_en[\"n_posts\"] >= min_topic_posts]\n",
        "            .head(15)\n",
        "            .to_string(index=False)\n",
        "        )\n",
        "    else:\n",
        "        print(\"\\nNo topic matches found in the test split.\")\n",
        "else:\n",
        "    print(\"\\nTopic features are disabled, so no topic-level error table was computed.\")\n",
        "\n",
        "if \"second_stage_account_predictions_en\" in globals() and isinstance(second_stage_account_predictions_en, pd.DataFrame):\n",
        "    account_eval = second_stage_account_predictions_en.copy()\n",
        "    if \"true_is_bot\" not in account_eval.columns and \"is_bot\" in account_eval.columns:\n",
        "        account_eval = account_eval.rename(columns={\"is_bot\": \"true_is_bot\"})\n",
        "else:\n",
        "    account_eval = (\n",
        "        posts_eval.groupby(\"author_id\", as_index=False)\n",
        "        .agg(\n",
        "            true_is_bot=(\"true_is_bot\", \"max\"),\n",
        "            mean_prob=(\"pred_prob\", \"mean\"),\n",
        "            any_pred=(\"pred_is_bot\", \"max\"),\n",
        "        )\n",
        "        .copy()\n",
        "    )\n",
        "\n",
        "    if ACCOUNT_DECISION_RULE == \"any\":\n",
        "        account_eval[\"pred_is_bot\"] = account_eval[\"any_pred\"].astype(np.int64)\n",
        "        account_eval[\"pred_prob\"] = account_eval[\"any_pred\"].astype(np.float32)\n",
        "    else:\n",
        "        account_eval[\"pred_is_bot\"] = (account_eval[\"mean_prob\"] >= analysis_threshold).astype(np.int64)\n",
        "        account_eval[\"pred_prob\"] = account_eval[\"mean_prob\"].astype(np.float32)\n",
        "\n",
        "if topic_feature_cols_en:\n",
        "    topic_strength = posts_eval.groupby(\"author_id\")[topic_feature_cols_en].mean()\n",
        "    dominant_topic = topic_strength.idxmax(axis=1).str.replace(\"topic_\", \"\", regex=False)\n",
        "    dominant_topic = dominant_topic.where(topic_strength.max(axis=1) > 0, \"no_topic_match\")\n",
        "else:\n",
        "    dominant_topic = pd.Series(\"no_topic_features\", index=account_eval[\"author_id\"])\n",
        "\n",
        "account_eval = account_eval.merge(\n",
        "    dominant_topic.rename(\"dominant_topic\"),\n",
        "    left_on=\"author_id\",\n",
        "    right_index=True,\n",
        "    how=\"left\",\n",
        ")\n",
        "\n",
        "account_eval[\"error_type\"] = np.where(\n",
        "    (account_eval[\"true_is_bot\"] == 1) & (account_eval[\"pred_is_bot\"] == 0),\n",
        "    \"FN\",\n",
        "    np.where(\n",
        "        (account_eval[\"true_is_bot\"] == 0) & (account_eval[\"pred_is_bot\"] == 1),\n",
        "        \"FP\",\n",
        "        np.where(account_eval[\"true_is_bot\"] == 1, \"TP\", \"TN\"),\n",
        "    ),\n",
        ")\n",
        "\n",
        "account_topic_report_en = (\n",
        "    account_eval.groupby(\"dominant_topic\", as_index=False)\n",
        "    .agg(\n",
        "        n_accounts=(\"author_id\", \"size\"),\n",
        "        bot_rate=(\"true_is_bot\", \"mean\"),\n",
        "        fp_accounts=(\"error_type\", lambda s: int((s == \"FP\").sum())),\n",
        "        fn_accounts=(\"error_type\", lambda s: int((s == \"FN\").sum())),\n",
        "        tp_accounts=(\"error_type\", lambda s: int((s == \"TP\").sum())),\n",
        "        tn_accounts=(\"error_type\", lambda s: int((s == \"TN\").sum())),\n",
        "    )\n",
        ")\n",
        "\n",
        "account_topic_report_en[\"error_rate\"] = (\n",
        "    account_topic_report_en[\"fp_accounts\"] + account_topic_report_en[\"fn_accounts\"]\n",
        ") / account_topic_report_en[\"n_accounts\"]\n",
        "account_topic_report_en = account_topic_report_en.sort_values(\n",
        "    by=[\"error_rate\", \"n_accounts\"], ascending=[False, False]\n",
        ")\n",
        "\n",
        "min_topic_accounts = int(EXPERIMENT_CONFIG.get(\"error_analysis_min_topic_accounts\", 5))\n",
        "print(\"\\nAccount-level errors by dominant topic:\")\n",
        "print(\n",
        "    account_topic_report_en[account_topic_report_en[\"n_accounts\"] >= min_topic_accounts]\n",
        "    .head(15)\n",
        "    .to_string(index=False)\n",
        ")\n",
        "\n",
        "fp_examples = posts_eval[\n",
        "    (posts_eval[\"true_is_bot\"] == 0) & (posts_eval[\"pred_is_bot\"] == 1)\n",
        "][[\"author_id\", \"text_clean\", \"pred_prob\"]].sort_values(\"pred_prob\", ascending=False).head(10)\n",
        "fn_examples = posts_eval[\n",
        "    (posts_eval[\"true_is_bot\"] == 1) & (posts_eval[\"pred_is_bot\"] == 0)\n",
        "][[\"author_id\", \"text_clean\", \"pred_prob\"]].sort_values(\"pred_prob\", ascending=True).head(10)\n",
        "\n",
        "hard_case_examples_en = pd.concat(\n",
        "    [\n",
        "        fp_examples.assign(error_type=\"FP\"),\n",
        "        fn_examples.assign(error_type=\"FN\"),\n",
        "    ],\n",
        "    ignore_index=True,\n",
        ")\n",
        "\n",
        "if not hard_case_examples_en.empty:\n",
        "    print(\"\\nSample hard cases (for pattern inspection, not rule memorization):\")\n",
        "    print(hard_case_examples_en.to_string(index=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "391775aa",
      "metadata": {},
      "source": [
        "## Booster search prep (do not run long sweep yet)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06d34ca8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "search_dir = Path(\"artifacts\")\n",
        "search_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "SEARCH_RESULTS_PATH = search_dir / \"booster_search_results.csv\"\n",
        "SEARCH_BEST_PATH = search_dir / \"booster_best_config.json\"\n",
        "\n",
        "BOOSTER_SEARCH_SPACE = {\n",
        "    \"second_stage_profile\": [\"auto\", \"legacy\", \"regularized\"],\n",
        "    \"second_stage_max_iter\": [200, 300, 450],\n",
        "    \"second_stage_max_depth\": [3, 4, 5],\n",
        "    \"second_stage_l2\": [0.1, 0.2, 0.5, 1.0],\n",
        "    \"second_stage_min_data_in_leaf\": [10, 20, 40],\n",
        "    \"second_stage_subsample\": [0.7, 0.8, 1.0],\n",
        "    \"second_stage_rsm\": [0.7, 0.8, 1.0],\n",
        "    \"second_stage_blend_alphas\": [[1.0], [1.0, 0.9], [1.0, 0.9, 0.8, 0.7]],\n",
        "    \"second_stage_min_gain_vs_ensemble\": [0, 1, 2],\n",
        "    \"second_stage_use_oof_features\": [True],\n",
        "    \"second_stage_oof_folds\": [3, 4],\n",
        "    \"second_stage_oof_epochs\": [3, 4],\n",
        "    \"second_stage_oof_seeds\": [[13], [13, 42]],\n",
        "}\n",
        "\n",
        "print(\"Search space prepared for later long run.\")\n",
        "print(f\"- dimensions: {len(BOOSTER_SEARCH_SPACE)}\")\n",
        "print(f\"- results file: {SEARCH_RESULTS_PATH}\")\n",
        "\n",
        "\n",
        "def _safe_float(value):\n",
        "    try:\n",
        "        return float(value)\n",
        "    except Exception:\n",
        "        return np.nan\n",
        "\n",
        "\n",
        "def log_current_booster_run(run_name=None, notes=\"\"):\n",
        "    timestamp = datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
        "    if run_name is None:\n",
        "        run_name = f\"run_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n",
        "\n",
        "    row = {\n",
        "        \"timestamp_utc\": timestamp,\n",
        "        \"run_name\": run_name,\n",
        "        \"notes\": notes,\n",
        "        \"ensemble_score\": _safe_float(globals().get(\"test_score_ensemble_en\", np.nan)),\n",
        "        \"booster_score\": _safe_float(globals().get(\"second_stage_test_score_en\", np.nan)),\n",
        "        \"score_gain\": _safe_float(globals().get(\"second_stage_test_score_en\", np.nan)) - _safe_float(globals().get(\"test_score_ensemble_en\", np.nan)),\n",
        "        \"seed_mean_score\": _safe_float(globals().get(\"seed_mean_score_en\", np.nan)),\n",
        "        \"seed_std_score\": _safe_float(globals().get(\"seed_std_score_en\", np.nan)),\n",
        "        \"selected_profile\": globals().get(\"second_stage_selected_profile_en\", \"\"),\n",
        "        \"selected_alpha\": _safe_float(globals().get(\"second_stage_alpha_en\", np.nan)),\n",
        "        \"selected_threshold\": _safe_float(globals().get(\"second_stage_selected_threshold_en\", np.nan)),\n",
        "        \"feature_source\": globals().get(\"second_stage_fit_feature_source_en\", \"\"),\n",
        "        \"config_json\": json.dumps(EXPERIMENT_CONFIG, sort_keys=True),\n",
        "    }\n",
        "\n",
        "    current = pd.DataFrame([row])\n",
        "    if SEARCH_RESULTS_PATH.exists():\n",
        "        history = pd.read_csv(SEARCH_RESULTS_PATH)\n",
        "        history = pd.concat([history, current], ignore_index=True)\n",
        "    else:\n",
        "        history = current\n",
        "\n",
        "    history = history.drop_duplicates(subset=[\"run_name\"], keep=\"last\")\n",
        "    history.to_csv(SEARCH_RESULTS_PATH, index=False)\n",
        "\n",
        "    leaderboard = history.sort_values(\n",
        "        by=[\"booster_score\", \"score_gain\", \"seed_std_score\"],\n",
        "        ascending=[False, False, True],\n",
        "    )\n",
        "\n",
        "    if len(leaderboard):\n",
        "        best_row = leaderboard.iloc[0].to_dict()\n",
        "        best_payload = {\n",
        "            \"saved_at_utc\": timestamp,\n",
        "            \"best_run\": best_row,\n",
        "            \"best_config\": json.loads(best_row.get(\"config_json\", \"{}\")),\n",
        "        }\n",
        "        SEARCH_BEST_PATH.write_text(json.dumps(best_payload, indent=2))\n",
        "\n",
        "    print(f\"Saved run to {SEARCH_RESULTS_PATH}\")\n",
        "    print(f\"Best config snapshot saved to {SEARCH_BEST_PATH}\")\n",
        "    print(\"\\nTop runs:\")\n",
        "    print(\n",
        "        leaderboard[\n",
        "            [\n",
        "                \"run_name\",\n",
        "                \"ensemble_score\",\n",
        "                \"booster_score\",\n",
        "                \"score_gain\",\n",
        "                \"seed_std_score\",\n",
        "                \"selected_profile\",\n",
        "                \"feature_source\",\n",
        "            ]\n",
        "        ].head(10).to_string(index=False)\n",
        "    )\n",
        "\n",
        "    return history\n",
        "\n",
        "\n",
        "def plot_search_history(top_n=20):\n",
        "    if not SEARCH_RESULTS_PATH.exists():\n",
        "        raise FileNotFoundError(f\"No results found at {SEARCH_RESULTS_PATH}\")\n",
        "\n",
        "    history = pd.read_csv(SEARCH_RESULTS_PATH)\n",
        "    if history.empty:\n",
        "        raise ValueError(\"Search history is empty.\")\n",
        "\n",
        "    leaderboard = history.sort_values(\n",
        "        by=[\"booster_score\", \"score_gain\", \"seed_std_score\"],\n",
        "        ascending=[False, False, True],\n",
        "    ).head(top_n)\n",
        "\n",
        "    plt.figure(figsize=(10, 5))\n",
        "    x = np.arange(len(leaderboard))\n",
        "    plt.plot(x, leaderboard[\"ensemble_score\"].to_numpy(), marker=\"o\", label=\"without booster\")\n",
        "    plt.plot(x, leaderboard[\"booster_score\"].to_numpy(), marker=\"o\", label=\"with booster\")\n",
        "    plt.xticks(x, leaderboard[\"run_name\"], rotation=70)\n",
        "    plt.ylabel(\"Competition score\")\n",
        "    plt.title(f\"Top {top_n} logged runs\")\n",
        "    plt.grid(axis=\"y\", linestyle=\":\", alpha=0.4)\n",
        "    plt.legend()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "venv_botsornot (3.13.0)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
