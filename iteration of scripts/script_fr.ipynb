{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fa69427",
   "metadata": {},
   "source": [
    "# McHacks 26 - Bot or Not\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25e59269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fa1b429",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN_COLAB=False\n",
      "USE_GOOGLE_DRIVE_DATA=True\n",
      "PROJECT_ROOT=.\n",
      "DATA_DIR=/Users/adrienbelanger/Library/GitHub/bot_or_not_McHacks_2026/data\n",
      "EXTERNAL_DATA_DIR=/Users/adrienbelanger/Library/GitHub/bot_or_not_McHacks_2026/external_data\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Toggle this in Colab when your datasets are stored on Google Drive.\n",
    "USE_GOOGLE_DRIVE_DATA = True\n",
    "\n",
    "# Adjust this only if your folder lives elsewhere in Drive.\n",
    "GOOGLE_DRIVE_PROJECT_ROOT = Path(\"/content/drive/MyDrive/bot_or_not_McHacks_2026\")\n",
    "LOCAL_PROJECT_ROOT = Path(\".\")\n",
    "\n",
    "IN_COLAB = \"google.colab\" in sys.modules\n",
    "PROJECT_ROOT = LOCAL_PROJECT_ROOT\n",
    "\n",
    "if IN_COLAB and USE_GOOGLE_DRIVE_DATA:\n",
    "    from google.colab import drive\n",
    "    !pip install emoji==0.6.0\n",
    "    !pip install catboost\n",
    "    !pip install transformers huggingface_hub datasets\n",
    "\n",
    "    drive.mount(\"/content/drive\", force_remount=False)\n",
    "    PROJECT_ROOT = GOOGLE_DRIVE_PROJECT_ROOT\n",
    "elif IN_COLAB:\n",
    "    PROJECT_ROOT = LOCAL_PROJECT_ROOT\n",
    "\n",
    "DATA_DIR = (PROJECT_ROOT / \"data\").resolve()\n",
    "EXTERNAL_DATA_DIR = (PROJECT_ROOT / \"external_data\").resolve()\n",
    "\n",
    "print(f\"IN_COLAB={IN_COLAB}\")\n",
    "print(f\"USE_GOOGLE_DRIVE_DATA={USE_GOOGLE_DRIVE_DATA}\")\n",
    "print(f\"PROJECT_ROOT={PROJECT_ROOT}\")\n",
    "print(f\"DATA_DIR={DATA_DIR}\")\n",
    "print(f\"EXTERNAL_DATA_DIR={EXTERNAL_DATA_DIR}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c78ef1e",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "071ab039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using DATA_DIR: /Users/adrienbelanger/Library/GitHub/bot_or_not_McHacks_2026/data\n",
      "Using EXTERNAL_DATA_DIR: /Users/adrienbelanger/Library/GitHub/bot_or_not_McHacks_2026/external_data\n",
      "EN sources: ['dataset.posts&users.30.json', 'dataset.posts&users.32.json']\n",
      "EN posts: 15,765 users: 546 bot_ids: 129\n",
      "FR sources: ['dataset.posts&users.31.json', 'dataset.posts&users.33.json']\n",
      "FR posts: 9,004 users: 343 bot_ids: 55\n"
     ]
    }
   ],
   "source": [
    "if \"DATA_DIR\" not in globals():\n",
    "    DATA_DIR = Path(\"data\").resolve()\n",
    "if \"EXTERNAL_DATA_DIR\" not in globals():\n",
    "    EXTERNAL_DATA_DIR = Path(\"external_data\").resolve()\n",
    "\n",
    "print(f\"Using DATA_DIR: {DATA_DIR}\")\n",
    "print(f\"Using EXTERNAL_DATA_DIR: {EXTERNAL_DATA_DIR}\")\n",
    "\n",
    "\n",
    "def get_version(path):\n",
    "    try:\n",
    "        return int(path.stem.split(\".\")[-1])\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "\n",
    "posts_users_files = sorted(DATA_DIR.glob(\"dataset.posts&users.*.json\"), key=get_version)\n",
    "if not posts_users_files:\n",
    "    raise FileNotFoundError(f\"No dataset.posts&users.*.json files found in {DATA_DIR}\")\n",
    "\n",
    "\n",
    "combined = {}\n",
    "bots_by_lang = {}\n",
    "\n",
    "\n",
    "for path in posts_users_files:\n",
    "    with path.open() as f:\n",
    "        data = json.load(f)\n",
    "    lang = data.get(\"lang\")\n",
    "\n",
    "    combined.setdefault(lang, {\"posts\": [], \"users\": [], \"sources\": []})\n",
    "    combined[lang][\"posts\"].extend(data.get(\"posts\", []))\n",
    "    combined[lang][\"users\"].extend(data.get(\"users\", []))\n",
    "    combined[lang][\"sources\"].append(path.name)\n",
    "\n",
    "    version = get_version(path)\n",
    "    if version is not None:\n",
    "        bots_path = DATA_DIR / f\"dataset.bots.{version}.txt\"\n",
    "        if bots_path.exists():\n",
    "            bots_by_lang.setdefault(lang, set()).update(bots_path.read_text().splitlines())\n",
    "\n",
    "\n",
    "posts_en = pd.DataFrame(combined.get(\"en\", {}).get(\"posts\", []))\n",
    "users_en = pd.DataFrame(combined.get(\"en\", {}).get(\"users\", []))\n",
    "bot_ids_en = bots_by_lang.get(\"en\", set())\n",
    "if not users_en.empty:\n",
    "    users_en[\"is_bot\"] = users_en[\"id\"].isin(bot_ids_en)\n",
    "\n",
    "posts_fr = pd.DataFrame(combined.get(\"fr\", {}).get(\"posts\", []))\n",
    "users_fr = pd.DataFrame(combined.get(\"fr\", {}).get(\"users\", []))\n",
    "bot_ids_fr = bots_by_lang.get(\"fr\", set())\n",
    "if not users_fr.empty:\n",
    "    users_fr[\"is_bot\"] = users_fr[\"id\"].isin(bot_ids_fr)\n",
    "\n",
    "print(\"EN sources:\", combined.get(\"en\", {}).get(\"sources\", []))\n",
    "print(f\"EN posts: {len(posts_en):,} users: {len(users_en):,} bot_ids: {len(bot_ids_en):,}\")\n",
    "print(\"FR sources:\", combined.get(\"fr\", {}).get(\"sources\", []))\n",
    "print(f\"FR posts: {len(posts_fr):,} users: {len(users_fr):,} bot_ids: {len(bot_ids_fr):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e34ec8c",
   "metadata": {},
   "source": [
    "### Experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "926640c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment config loaded:\n",
      "- tokenizer_name: cardiffnlp/twitter-xlm-roberta-base\n",
      "- target_lang: fr\n",
      "- max_length: 96\n",
      "- dedupe_users: True\n",
      "- dedupe_posts: True\n",
      "- scale_meta_features: True\n",
      "- normalize_social_tokens: True\n",
      "- strip_hashtag_symbol: True\n",
      "- lowercase_text: True\n",
      "- use_topic_features: False\n",
      "- topic_match_mode: word\n",
      "- topic_feature_scale: 0.5\n",
      "- topic_include_cross_language_keywords: False\n",
      "- topic_cross_langs: ['en']\n",
      "- test_size: 0.2\n",
      "- random_seed: 42\n",
      "- validation_split: 0.15\n",
      "- epochs: 10\n",
      "- batch_size: 96\n",
      "- learning_rate: 0.0008\n",
      "- prediction_threshold: 0.62\n",
      "- use_threshold_search: True\n",
      "- threshold_search_min: 0.1\n",
      "- threshold_search_max: 0.9\n",
      "- threshold_search_steps: 161\n",
      "- account_decision_rule: mean\n",
      "- account_min_bot_posts: 1\n",
      "- account_min_bot_post_rate: 0.0\n",
      "- split_by_author: True\n",
      "- ensemble_seeds: [13, 29, 42, 73, 101, 137, 173]\n",
      "- ensemble_aggregation: median\n",
      "- use_external_pretrain: True\n",
      "- external_pretrain_source: airt_ml\n",
      "- external_pretrain_download: True\n",
      "- external_pretrain_hf_repo: airt-ml/twitter-human-bots\n",
      "- external_pretrain_hf_filename: twitter_human_bots_dataset.csv\n",
      "- external_pretrain_max_users: 20000\n",
      "- external_pretrain_max_tweets_per_user: 5\n",
      "- external_pretrain_max_posts: 120000\n",
      "- external_pretrain_lang: fr\n",
      "- external_pretrain_sample_seed: 42\n",
      "- external_pretrain_epochs: 1\n",
      "- external_pretrain_batch_size: 128\n",
      "- external_pretrain_use_balanced_weights: True\n",
      "- use_second_stage_account_model: True\n",
      "- use_logreg_account_model: True\n",
      "- logreg_max_iter: 200\n",
      "- logreg_min_posts: 2\n",
      "- second_stage_profile: regularized\n",
      "- second_stage_use_blend: True\n",
      "- second_stage_blend_alphas: [1.0, 0.9, 0.8, 0.7, 0.55, 0.4, 0.25, 0.1]\n",
      "- second_stage_min_gain_vs_ensemble: 1\n",
      "- second_stage_max_extra_fp_vs_ensemble: 1\n",
      "- second_stage_post_calibrate: True\n",
      "- second_stage_account_min_bot_posts_grid: [1, 2, 3]\n",
      "- second_stage_account_min_bot_post_rate_grid: [0.0, 0.15, 0.25]\n",
      "- second_stage_min_val_accounts_for_booster: 25\n",
      "- second_stage_use_oof_features: True\n",
      "- second_stage_oof_folds: 4\n",
      "- second_stage_oof_epochs: 4\n",
      "- second_stage_oof_batch_size: 128\n",
      "- second_stage_oof_seeds: [13, 42]\n",
      "- second_stage_oof_use_external_pretrain: True\n",
      "- second_stage_learning_rate: 0.05\n",
      "- second_stage_max_iter: 300\n",
      "- second_stage_max_depth: 4\n",
      "- second_stage_l2: 0.2\n",
      "- second_stage_min_data_in_leaf: 20\n",
      "- second_stage_subsample: 0.8\n",
      "- second_stage_rsm: 0.8\n",
      "- second_stage_od_wait: 50\n",
      "- second_stage_use_balanced_weights: True\n",
      "- use_class_weights: True\n",
      "- embedding_dim: 96\n",
      "- gru_units: 48\n",
      "- aux_dense_units: 32\n",
      "- head_dense_units: 48\n",
      "- dropout_text: 0.4\n",
      "- dropout_aux: 0.3\n",
      "- dropout_head: 0.4\n",
      "- early_stopping_patience: 2\n",
      "- reduce_lr_patience: 2\n",
      "- reduce_lr_factor: 0.5\n",
      "- reduce_lr_min_lr: 1e-05\n",
      "- error_analysis_min_topic_posts: 20\n",
      "- error_analysis_min_topic_accounts: 5\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_CONFIG = {\n",
    "    \"tokenizer_name\": \"cardiffnlp/twitter-xlm-roberta-base\",\n",
    "\n",
    "    \"target_lang\": \"fr\",\n",
    "    \"max_length\": 96,\n",
    "    \"dedupe_users\": True,\n",
    "    \"dedupe_posts\": True,\n",
    "    \"scale_meta_features\": True,\n",
    "\n",
    "    \"normalize_social_tokens\": True,\n",
    "\n",
    "    \"strip_hashtag_symbol\": True,\n",
    "\n",
    "    \"lowercase_text\": True,\n",
    "    \"use_topic_features\": False,\n",
    "    \"topic_match_mode\": \"word\",  # options: \"contains\", \"word\"\n",
    "    \"topic_feature_scale\": 0.5,\n",
    "    \"topic_include_cross_language_keywords\": False,\n",
    "    \"topic_cross_langs\": [\"en\"],\n",
    "    \"test_size\": 0.20,\n",
    "    \"random_seed\": 42,\n",
    "    \"validation_split\": 0.15,\n",
    "    \"epochs\": 10,\n",
    "    \"batch_size\": 96,\n",
    "    \"learning_rate\": 0.0008,\n",
    "    \"prediction_threshold\": 0.62,\n",
    "    \"use_threshold_search\": True,\n",
    "    \"threshold_search_min\": 0.10,\n",
    "    \"threshold_search_max\": 0.90,\n",
    "    \"threshold_search_steps\": 161,\n",
    "    \"account_decision_rule\": \"mean\",  # options: \"mean\", \"any\"\n",
    "    \"account_min_bot_posts\": 1,\n",
    "    \"account_min_bot_post_rate\": 0.0,\n",
    "    \"split_by_author\": True,\n",
    "    \"ensemble_seeds\": [13, 29, 42, 73, 101, 137, 173],\n",
    "    \"ensemble_aggregation\": \"median\",  # options: \"mean\", \"median\"\n",
    "    \"use_external_pretrain\": True,\n",
    "    \"external_pretrain_source\": \"airt_ml\",\n",
    "    \"external_pretrain_download\": True,\n",
    "    \"external_pretrain_hf_repo\": \"airt-ml/twitter-human-bots\",\n",
    "    \"external_pretrain_hf_filename\": \"twitter_human_bots_dataset.csv\",\n",
    "    \"external_pretrain_max_users\": 20000,\n",
    "    \"external_pretrain_max_tweets_per_user\": 5,\n",
    "    \"external_pretrain_max_posts\": 120000,\n",
    "    \"external_pretrain_lang\": \"fr\",\n",
    "    \"external_pretrain_sample_seed\": 42,\n",
    "    \"external_pretrain_epochs\": 1,\n",
    "    \"external_pretrain_batch_size\": 128,\n",
    "    \"external_pretrain_use_balanced_weights\": True,\n",
    "    \"use_second_stage_account_model\": True,\n",
    "    \"use_logreg_account_model\": True,\n",
    "    \"logreg_max_iter\": 200,\n",
    "    \"logreg_min_posts\": 2,\n",
    "    \"second_stage_profile\": \"regularized\",  # options: \"auto\", \"legacy\", \"regularized\", \"custom\"\n",
    "    \"second_stage_use_blend\": True,\n",
    "    \"second_stage_blend_alphas\": [1.0, 0.90, 0.80, 0.70, 0.55, 0.40, 0.25, 0.10],\n",
    "    \"second_stage_min_gain_vs_ensemble\": 1,\n",
    "\n",
    "    \"second_stage_max_extra_fp_vs_ensemble\": 1,\n",
    "    \"second_stage_post_calibrate\": True,\n",
    "    \"second_stage_account_min_bot_posts_grid\": [1, 2, 3],\n",
    "    \"second_stage_account_min_bot_post_rate_grid\": [0.0, 0.15, 0.25],\n",
    "\n",
    "    \"second_stage_min_val_accounts_for_booster\": 25,\n",
    "    \"second_stage_use_oof_features\": True,\n",
    "    \"second_stage_oof_folds\": 4,\n",
    "    \"second_stage_oof_epochs\": 4,\n",
    "    \"second_stage_oof_batch_size\": 128,\n",
    "    \"second_stage_oof_seeds\": [13, 42],\n",
    "    \"second_stage_oof_use_external_pretrain\": True,\n",
    "    \"second_stage_learning_rate\": 0.05,\n",
    "    \"second_stage_max_iter\": 300,\n",
    "    \"second_stage_max_depth\": 4,\n",
    "    \"second_stage_l2\": 0.2,\n",
    "    \"second_stage_min_data_in_leaf\": 20,\n",
    "    \"second_stage_subsample\": 0.8,\n",
    "    \"second_stage_rsm\": 0.8,\n",
    "    \"second_stage_od_wait\": 50,\n",
    "    \"second_stage_use_balanced_weights\": True,\n",
    "    \"use_class_weights\": True,\n",
    "    \"embedding_dim\": 96,\n",
    "    \"gru_units\": 48,\n",
    "    \"aux_dense_units\": 32,\n",
    "    \"head_dense_units\": 48,\n",
    "    \"dropout_text\": 0.40,\n",
    "    \"dropout_aux\": 0.30,\n",
    "    \"dropout_head\": 0.40,\n",
    "    \"early_stopping_patience\": 2,\n",
    "    \"reduce_lr_patience\": 2,\n",
    "    \"reduce_lr_factor\": 0.50,\n",
    "    \"reduce_lr_min_lr\": 1e-5,\n",
    "    \"error_analysis_min_topic_posts\": 20,\n",
    "    \"error_analysis_min_topic_accounts\": 5,\n",
    "}\n",
    "\n",
    "print(\"Experiment config loaded:\")\n",
    "for key, value in EXPERIMENT_CONFIG.items():\n",
    "    print(f\"- {key}: {value}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37411cfe",
   "metadata": {},
   "source": [
    "### Tokenizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1288615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target language: FR\n",
      "Tokenizer: cardiffnlp/twitter-xlm-roberta-base\n",
      "Labeled posts used: 9,004\n",
      "Token tensor shape: (9004, 96)\n",
      "Meta feature shape: (9004, 7), label shape: (9004,)\n",
      "Dedupe users/posts: True/True\n",
      "Scale metadata features: True\n",
      "Normalize URLs/users: True\n",
      "Strip hashtag symbol: True\n",
      "Lowercase text: True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "except ImportError as exc:\n",
    "    raise ImportError(\"Install transformers first: pip install transformers\") from exc\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "if \"EXPERIMENT_CONFIG\" not in globals():\n",
    "    raise ValueError(\"Run the Experiment parameters cell first.\")\n",
    "\n",
    "TOKENIZER_NAME = str(EXPERIMENT_CONFIG[\"tokenizer_name\"])\n",
    "TARGET_LANG = str(EXPERIMENT_CONFIG.get(\"target_lang\", \"fr\")).strip().lower()\n",
    "MAX_LENGTH = int(EXPERIMENT_CONFIG[\"max_length\"])\n",
    "DEDUPE_USERS = bool(EXPERIMENT_CONFIG[\"dedupe_users\"])\n",
    "DEDUPE_POSTS = bool(EXPERIMENT_CONFIG[\"dedupe_posts\"])\n",
    "SCALE_META_FEATURES = bool(EXPERIMENT_CONFIG[\"scale_meta_features\"])\n",
    "NORMALIZE_SOCIAL_TOKENS = bool(EXPERIMENT_CONFIG.get(\"normalize_social_tokens\", True))\n",
    "STRIP_HASHTAG_SYMBOL = bool(EXPERIMENT_CONFIG.get(\"strip_hashtag_symbol\", True))\n",
    "LOWERCASE_TEXT = bool(EXPERIMENT_CONFIG.get(\"lowercase_text\", False))\n",
    "\n",
    "if TARGET_LANG not in {\"en\", \"fr\"}:\n",
    "    raise ValueError('target_lang must be \"en\" or \"fr\".')\n",
    "\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "\n",
    "    text = text.replace(\"\\n\", \" \").strip()\n",
    "\n",
    "    if NORMALIZE_SOCIAL_TOKENS:\n",
    "        text = re.sub(r\"https?://\\S+|www\\.\\S+\", \" <URL> \", text)\n",
    "        text = re.sub(r\"@\\w+\", \" <USER> \", text)\n",
    "\n",
    "    if STRIP_HASHTAG_SYMBOL:\n",
    "        text = re.sub(r\"#(\\w+)\", r\" \\1 \", text)\n",
    "\n",
    "    if LOWERCASE_TEXT:\n",
    "        text = text.lower()\n",
    "\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "def add_text_features(df):\n",
    "    out = df.copy()\n",
    "\n",
    "    raw_text = out[\"text\"].fillna(\"\").map(lambda x: x if isinstance(x, str) else \"\")\n",
    "    out[\"text_clean\"] = raw_text.map(clean_text)\n",
    "\n",
    "    # Counts from raw text retain signal even when text_clean is normalized.\n",
    "    out[\"url_count\"] = raw_text.str.count(r\"https?://\\S+|www\\.\\S+\")\n",
    "    out[\"mention_count\"] = raw_text.str.count(r\"@\\w+\")\n",
    "    out[\"hashtag_count\"] = raw_text.str.count(r\"#\\w+\")\n",
    "\n",
    "    out[\"char_count\"] = out[\"text_clean\"].str.len()\n",
    "    out[\"word_count\"] = out[\"text_clean\"].str.split().str.len()\n",
    "    out[\"exclamation_count\"] = out[\"text_clean\"].str.count(r\"!\")\n",
    "    out[\"question_count\"] = out[\"text_clean\"].str.count(r\"\\?\")\n",
    "    return out\n",
    "\n",
    "\n",
    "posts_by_lang = {\"en\": posts_en, \"fr\": posts_fr}\n",
    "users_by_lang = {\"en\": users_en, \"fr\": users_fr}\n",
    "\n",
    "posts_lang = posts_by_lang.get(TARGET_LANG, pd.DataFrame())\n",
    "users_lang = users_by_lang.get(TARGET_LANG, pd.DataFrame())\n",
    "\n",
    "if posts_lang.empty or users_lang.empty:\n",
    "    raise ValueError(\n",
    "        f\"Run the data processing cell first to load {TARGET_LANG.upper()} data. \"\n",
    "        f\"Posts={len(posts_lang)}, users={len(users_lang)}\"\n",
    "    )\n",
    "\n",
    "users_lang_labeled = (\n",
    "    users_lang.drop_duplicates(subset=\"id\", keep=\"last\").copy()\n",
    "    if DEDUPE_USERS\n",
    "    else users_lang.copy()\n",
    ")\n",
    "posts_lang_unique = (\n",
    "    posts_lang.drop_duplicates(subset=\"id\", keep=\"last\").copy()\n",
    "    if DEDUPE_POSTS\n",
    "    else posts_lang.copy()\n",
    ")\n",
    "\n",
    "# Keep downstream cell compatibility by reusing existing variable names.\n",
    "users_en = users_lang.copy()\n",
    "posts_en = posts_lang.copy()\n",
    "users_en_labeled = users_lang_labeled.copy()\n",
    "\n",
    "label_map_lang = users_lang_labeled.set_index(\"id\")[\"is_bot\"]\n",
    "train_en = posts_lang_unique.copy()\n",
    "train_en[\"is_bot\"] = train_en[\"author_id\"].map(label_map_lang)\n",
    "train_en = train_en.dropna(subset=[\"is_bot\"]).copy()\n",
    "train_en[\"is_bot\"] = train_en[\"is_bot\"].astype(\"int64\")\n",
    "\n",
    "train_en = add_text_features(train_en)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, use_fast=True)\n",
    "encodings_en = tokenizer(\n",
    "    train_en[\"text_clean\"].tolist(),\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=MAX_LENGTH,\n",
    "    return_attention_mask=True,\n",
    ")\n",
    "\n",
    "feature_cols_en = [\n",
    "    \"char_count\",\n",
    "    \"word_count\",\n",
    "    \"url_count\",\n",
    "    \"mention_count\",\n",
    "    \"hashtag_count\",\n",
    "    \"exclamation_count\",\n",
    "    \"question_count\",\n",
    "]\n",
    "X_meta_en = train_en[feature_cols_en].to_numpy(dtype=np.float32)\n",
    "y_en = train_en[\"is_bot\"].to_numpy(dtype=np.int64)\n",
    "\n",
    "if SCALE_META_FEATURES:\n",
    "    scaler_en = StandardScaler()\n",
    "    X_meta_en_scaled = scaler_en.fit_transform(X_meta_en).astype(np.float32)\n",
    "else:\n",
    "    scaler_en = None\n",
    "    X_meta_en_scaled = X_meta_en.copy()\n",
    "\n",
    "print(f\"Target language: {TARGET_LANG.upper()}\")\n",
    "print(f\"Tokenizer: {TOKENIZER_NAME}\")\n",
    "print(f\"Labeled posts used: {len(train_en):,}\")\n",
    "print(f\"Token tensor shape: {np.asarray(encodings_en['input_ids']).shape}\")\n",
    "print(f\"Meta feature shape: {X_meta_en_scaled.shape}, label shape: {y_en.shape}\")\n",
    "print(f\"Dedupe users/posts: {DEDUPE_USERS}/{DEDUPE_POSTS}\")\n",
    "print(f\"Scale metadata features: {SCALE_META_FEATURES}\")\n",
    "print(f\"Normalize URLs/users: {NORMALIZE_SOCIAL_TOKENS}\")\n",
    "print(f\"Strip hashtag symbol: {STRIP_HASHTAG_SYMBOL}\")\n",
    "print(f\"Lowercase text: {LOWERCASE_TEXT}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9f8c2a",
   "metadata": {},
   "source": [
    "## External data pretraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c8c2c6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "External pretrain dataset prepared:\n",
      "- source: airt_ml\n",
      "- rows: 375\n",
      "- bots: 104, humans: 271\n",
      "- max_length: 96\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if \"EXPERIMENT_CONFIG\" not in globals():\n",
    "    raise ValueError(\"Run the Experiment parameters cell first.\")\n",
    "if \"tokenizer\" not in globals() or \"add_text_features\" not in globals():\n",
    "    raise ValueError(\"Run the Tokenizing the text cell first.\")\n",
    "\n",
    "USE_EXTERNAL_PRETRAIN = bool(EXPERIMENT_CONFIG.get(\"use_external_pretrain\", False))\n",
    "if not USE_EXTERNAL_PRETRAIN:\n",
    "    print(\"External pretraining disabled. Set use_external_pretrain=True to enable.\")\n",
    "else:\n",
    "    external_source = str(EXPERIMENT_CONFIG.get(\"external_pretrain_source\", \"fox8\")).lower()\n",
    "    max_users = int(EXPERIMENT_CONFIG.get(\"external_pretrain_max_users\", 20000))\n",
    "    max_posts = int(EXPERIMENT_CONFIG.get(\"external_pretrain_max_posts\", 120000))\n",
    "    max_tweets_per_user = int(EXPERIMENT_CONFIG.get(\"external_pretrain_max_tweets_per_user\", 5))\n",
    "    lang_filter = str(EXPERIMENT_CONFIG.get(\"external_pretrain_lang\", EXPERIMENT_CONFIG.get(\"target_lang\", \"fr\"))).lower().strip()\n",
    "    sample_seed = int(EXPERIMENT_CONFIG.get(\"external_pretrain_sample_seed\", 42))\n",
    "\n",
    "    random.seed(sample_seed)\n",
    "\n",
    "    data_dir = Path(globals().get(\"EXTERNAL_DATA_DIR\", Path(\"external_data\")))\n",
    "    external_pretrain_rows = []\n",
    "\n",
    "    if external_source in {\"fox8\"}:\n",
    "        fox8_path = data_dir / \"fox8_23_dataset.ndjson\"\n",
    "        if not fox8_path.exists():\n",
    "            raise FileNotFoundError(f\"Missing {fox8_path}\")\n",
    "        # Reservoir sample users to avoid loading the full file.\n",
    "        sampled_users = []\n",
    "        with fox8_path.open() as f:\n",
    "            for i, line in enumerate(f):\n",
    "                try:\n",
    "                    obj = json.loads(line)\n",
    "                except json.JSONDecodeError:\n",
    "                    continue\n",
    "                if len(sampled_users) < max_users:\n",
    "                    sampled_users.append(obj)\n",
    "                else:\n",
    "                    j = random.randint(0, i)\n",
    "                    if j < max_users:\n",
    "                        sampled_users[j] = obj\n",
    "\n",
    "        for user in sampled_users:\n",
    "            label = str(user.get(\"label\", \"\")).strip().lower()\n",
    "            if not label:\n",
    "                continue\n",
    "            if label in {\"human\", \"normal\"}:\n",
    "                is_bot = 0\n",
    "            elif label in {\"bot\", \"spambot\", \"fake\", \"automated\"}:\n",
    "                is_bot = 1\n",
    "            else:\n",
    "                # Unknown label; skip\n",
    "                continue\n",
    "\n",
    "            user_id = user.get(\"user_id\")\n",
    "            tweets = user.get(\"user_tweets\", []) or []\n",
    "\n",
    "            per_user_count = 0\n",
    "            for tweet in tweets:\n",
    "                if len(external_pretrain_rows) >= max_posts or per_user_count >= max_tweets_per_user:\n",
    "                    break\n",
    "                text = tweet.get(\"text\") if isinstance(tweet, dict) else None\n",
    "                if not text:\n",
    "                    continue\n",
    "                if lang_filter:\n",
    "                    tweet_lang = str(tweet.get(\"lang\", \"\")).lower().strip() if isinstance(tweet, dict) else \"\"\n",
    "                    if tweet_lang and tweet_lang != lang_filter:\n",
    "                        continue\n",
    "                external_pretrain_rows.append({\"author_id\": f\"fox8_{user_id}\", \"text\": text, \"is_bot\": is_bot})\n",
    "                per_user_count += 1\n",
    "\n",
    "    elif external_source in {\"airt_ml\", \"airt-ml\", \"twitter_human_bots\", \"twitter-human-bots\", \"hf_twitter_human_bots\"}:\n",
    "        hf_repo = str(EXPERIMENT_CONFIG.get(\"external_pretrain_hf_repo\", \"airt-ml/twitter-human-bots\"))\n",
    "        hf_filename = str(EXPERIMENT_CONFIG.get(\"external_pretrain_hf_filename\", \"twitter_human_bots_dataset.csv\"))\n",
    "        download_hf = bool(EXPERIMENT_CONFIG.get(\"external_pretrain_download\", True))\n",
    "        local_path = data_dir / hf_filename\n",
    "        hf_path = f\"hf://datasets/{hf_repo}/{hf_filename}\"\n",
    "\n",
    "        if download_hf and not local_path.exists():\n",
    "            try:\n",
    "                from huggingface_hub import hf_hub_download\n",
    "                hf_hub_download(\n",
    "                    repo_id=hf_repo,\n",
    "                    filename=hf_filename,\n",
    "                    repo_type=\"dataset\",\n",
    "                    local_dir=str(data_dir),\n",
    "                    local_dir_use_symlinks=False,\n",
    "                )\n",
    "                print(f\"Downloaded {hf_filename} to {local_path}\")\n",
    "            except Exception as exc:\n",
    "                print(f\"HF download failed ({exc}). Will try direct read.\")\n",
    "\n",
    "        try:\n",
    "            if local_path.exists():\n",
    "                external_df = pd.read_csv(local_path)\n",
    "            else:\n",
    "                external_df = pd.read_csv(hf_path)\n",
    "        except Exception as exc:\n",
    "            raise FileNotFoundError(\n",
    "                f\"Could not load {hf_path} or {local_path}. Download manually if needed. Error: {exc}\"\n",
    "            )\n",
    "\n",
    "        # Drop index-like columns if present\n",
    "        if \"Unnamed: 0\" in external_df.columns:\n",
    "            external_df = external_df.drop(columns=[\"Unnamed: 0\"])\n",
    "\n",
    "        def _find_col(columns, candidates, contains=None):\n",
    "            for c in candidates:\n",
    "                if c in columns:\n",
    "                    return c\n",
    "            if contains:\n",
    "                for c in columns:\n",
    "                    if contains in c.lower():\n",
    "                        return c\n",
    "            return None\n",
    "\n",
    "        # Language column\n",
    "        lang_col = _find_col(external_df.columns, [\"lang\", \"language\", \"tweet_lang\", \"tweet_language\"], contains=\"lang\")\n",
    "        if lang_col and lang_filter:\n",
    "            external_df = external_df[external_df[lang_col].astype(str).str.lower().str.strip() == lang_filter]\n",
    "\n",
    "        # Text column (this dataset is user-level; use description if no tweet text)\n",
    "        text_col = _find_col(external_df.columns, [\"description\", \"text\", \"tweet\", \"content\", \"full_text\"], contains=\"text\")\n",
    "        if text_col is None:\n",
    "            text_col = _find_col(external_df.columns, [\"tweet_text\", \"tweet_content\"], contains=\"tweet\")\n",
    "        if text_col is None:\n",
    "            raise ValueError(\"No text/description column found in twitter_human_bots_dataset.csv\")\n",
    "\n",
    "        # Label column\n",
    "        label_col = _find_col(external_df.columns, [\"account_type\", \"label\", \"is_bot\", \"bot\", \"bot_label\", \"account_type\", \"type\"], contains=\"bot\")\n",
    "        if label_col is None:\n",
    "            raise ValueError(\"No label column found in twitter_human_bots_dataset.csv\")\n",
    "\n",
    "        # User id column (optional)\n",
    "        user_col = _find_col(external_df.columns, [\"id\", \"user_id\", \"userid\", \"author_id\", \"user\", \"screen_name\", \"username\"], contains=\"user\")\n",
    "        if user_col is None:\n",
    "            external_df[\"_author_id\"] = np.arange(len(external_df)).astype(str)\n",
    "            user_col = \"_author_id\"\n",
    "\n",
    "        def _label_to_bot(value):\n",
    "            if pd.isna(value):\n",
    "                return None\n",
    "            if isinstance(value, (int, np.integer)):\n",
    "                return int(value)\n",
    "            if isinstance(value, (float, np.floating)):\n",
    "                return int(value >= 0.5)\n",
    "            v = str(value).strip().lower()\n",
    "            if v in {\"bot\", \"spambot\", \"fake\", \"automated\", \"1\", \"true\", \"yes\", \"bot_account\"}:\n",
    "                return 1\n",
    "            if v in {\"human\", \"normal\", \"real\", \"0\", \"false\", \"no\", \"human_account\"}:\n",
    "                return 0\n",
    "            return None\n",
    "\n",
    "        sampled = external_df.sample(n=min(len(external_df), max_posts), random_state=sample_seed)\n",
    "        user_counts = {}\n",
    "        for _, row in sampled.iterrows():\n",
    "            if len(external_pretrain_rows) >= max_posts:\n",
    "                break\n",
    "            uid = str(row[user_col])\n",
    "            if user_counts.get(uid, 0) >= max_tweets_per_user:\n",
    "                continue\n",
    "            label = _label_to_bot(row[label_col])\n",
    "            if label is None:\n",
    "                continue\n",
    "            text = row[text_col]\n",
    "            if not isinstance(text, str) or not text.strip():\n",
    "                continue\n",
    "            external_pretrain_rows.append({\"author_id\": f\"airt_{uid}\", \"text\": text, \"is_bot\": label})\n",
    "            user_counts[uid] = user_counts.get(uid, 0) + 1\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported external_pretrain_source. Use fox8 or airt_ml.\")\n",
    "\n",
    "    external_pretrain_df = pd.DataFrame(external_pretrain_rows)\n",
    "    if external_pretrain_df.empty:\n",
    "        raise ValueError(\"No external pretrain rows built. Check labels/lang filter.\")\n",
    "\n",
    "    external_pretrain_df = external_pretrain_df.dropna(subset=[\"text\"]).reset_index(drop=True)\n",
    "    external_pretrain_df = add_text_features(external_pretrain_df)\n",
    "\n",
    "    external_pretrain_encodings = tokenizer(\n",
    "        external_pretrain_df[\"text_clean\"].tolist(),\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=MAX_LENGTH,\n",
    "        return_attention_mask=True,\n",
    "    )\n",
    "\n",
    "    external_pretrain_input_ids = np.asarray(external_pretrain_encodings[\"input_ids\"], dtype=np.int32)\n",
    "    external_pretrain_attention_mask = np.asarray(external_pretrain_encodings[\"attention_mask\"], dtype=np.float32)\n",
    "\n",
    "    external_pretrain_meta = external_pretrain_df[feature_cols_en].to_numpy(dtype=np.float32)\n",
    "    if \"scaler_en\" in globals() and SCALE_META_FEATURES:\n",
    "        external_pretrain_meta_scaled = scaler_en.transform(external_pretrain_meta).astype(np.float32)\n",
    "    else:\n",
    "        external_pretrain_meta_scaled = external_pretrain_meta\n",
    "\n",
    "    external_pretrain_labels = external_pretrain_df[\"is_bot\"].to_numpy(dtype=np.int64)\n",
    "\n",
    "    print(\"External pretrain dataset prepared:\")\n",
    "    print(f\"- source: {external_source}\")\n",
    "    print(f\"- rows: {len(external_pretrain_df):,}\")\n",
    "    print(\n",
    "        f\"- bots: {int((external_pretrain_labels==1).sum())}, humans: {int((external_pretrain_labels==0).sum())}\"\n",
    "    )\n",
    "    print(f\"- max_length: {MAX_LENGTH}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251cded3",
   "metadata": {},
   "source": [
    "## Train-Test split for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c8c62216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 160ms/step - accuracy: 0.6647 - auc: 0.6952 - loss: 0.6484 - val_accuracy: 0.7491 - val_auc: 0.8201 - val_loss: 0.5326 - learning_rate: 8.0000e-04\n",
      "Epoch 2/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 161ms/step - accuracy: 0.8734 - auc: 0.9163 - loss: 0.3586 - val_accuracy: 0.8593 - val_auc: 0.9132 - val_loss: 0.3468 - learning_rate: 8.0000e-04\n",
      "Epoch 3/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 174ms/step - accuracy: 0.9355 - auc: 0.9760 - loss: 0.1910 - val_accuracy: 0.8229 - val_auc: 0.9075 - val_loss: 0.4212 - learning_rate: 8.0000e-04\n",
      "Epoch 4/10\n",
      "\u001b[1m67/67\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 170ms/step - accuracy: 0.9695 - auc: 0.9925 - loss: 0.1038 - val_accuracy: 0.8547 - val_auc: 0.8948 - val_loss: 0.4378 - learning_rate: 8.0000e-04\n",
      "Split mode: author\n",
      "Topic features enabled: False\n",
      "Topic match mode: word\n",
      "Account decision rule: mean\n",
      "Account minimum bot posts: 1\n",
      "Account minimum bot-post rate: 0.00\n",
      "Topic columns: []\n",
      "Split sizes (fit/val/test posts): 6353/881/1770\n",
      "Split sizes (fit/val/test accounts): 232/42/69\n",
      "Default threshold from config: 0.6200\n",
      "Selected threshold used on test: 0.3950\n",
      "Best validation score from threshold search: 26\n",
      "Test Accuracy (post-level): 0.8757\n",
      "Test ROC-AUC (post-level): 0.9573\n",
      "Test account score @ selected threshold -> score=42, TP=11, FN=0, FP=1, accounts=69\n",
      "Test account score @ config threshold -> score=39, TP=10, FN=1, FP=0, accounts=69\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9592    0.8646    0.9095      1278\n",
      "           1     0.7201    0.9045    0.8018       492\n",
      "\n",
      "    accuracy                         0.8757      1770\n",
      "   macro avg     0.8396    0.8846    0.8556      1770\n",
      "weighted avg     0.8927    0.8757    0.8795      1770\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except ImportError as exc:\n",
    "    raise ImportError(\"Install tensorflow first: pip install tensorflow\") from exc\n",
    "\n",
    "if \"EXPERIMENT_CONFIG\" not in globals():\n",
    "    raise ValueError(\"Run the Experiment parameters cell first.\")\n",
    "\n",
    "TEST_SIZE = float(EXPERIMENT_CONFIG[\"test_size\"])\n",
    "RANDOM_SEED = int(EXPERIMENT_CONFIG[\"random_seed\"])\n",
    "VALIDATION_SPLIT = float(EXPERIMENT_CONFIG[\"validation_split\"])\n",
    "EPOCHS = int(EXPERIMENT_CONFIG[\"epochs\"])\n",
    "BATCH_SIZE = int(EXPERIMENT_CONFIG[\"batch_size\"])\n",
    "LEARNING_RATE = float(EXPERIMENT_CONFIG[\"learning_rate\"])\n",
    "PREDICTION_THRESHOLD = float(EXPERIMENT_CONFIG[\"prediction_threshold\"])\n",
    "USE_CLASS_WEIGHTS = bool(EXPERIMENT_CONFIG[\"use_class_weights\"])\n",
    "USE_TOPIC_FEATURES = bool(EXPERIMENT_CONFIG[\"use_topic_features\"])\n",
    "TOPIC_MATCH_MODE = str(EXPERIMENT_CONFIG[\"topic_match_mode\"])\n",
    "TOPIC_FEATURE_SCALE = float(EXPERIMENT_CONFIG.get(\"topic_feature_scale\", 1.0))\n",
    "TOPIC_INCLUDE_CROSS_LANG_KEYWORDS = bool(EXPERIMENT_CONFIG.get(\"topic_include_cross_language_keywords\", False))\n",
    "TOPIC_CROSS_LANGS = EXPERIMENT_CONFIG.get(\"topic_cross_langs\", [])\n",
    "\n",
    "TARGET_LANG = str(EXPERIMENT_CONFIG.get(\"target_lang\", \"fr\")).strip().lower()\n",
    "SPLIT_BY_AUTHOR = bool(EXPERIMENT_CONFIG.get(\"split_by_author\", True))\n",
    "\n",
    "USE_THRESHOLD_SEARCH = bool(EXPERIMENT_CONFIG.get(\"use_threshold_search\", False))\n",
    "THRESHOLD_SEARCH_MIN = float(EXPERIMENT_CONFIG.get(\"threshold_search_min\", 0.10))\n",
    "THRESHOLD_SEARCH_MAX = float(EXPERIMENT_CONFIG.get(\"threshold_search_max\", 0.90))\n",
    "THRESHOLD_SEARCH_STEPS = int(EXPERIMENT_CONFIG.get(\"threshold_search_steps\", 81))\n",
    "ACCOUNT_DECISION_RULE = str(EXPERIMENT_CONFIG.get(\"account_decision_rule\", \"mean\"))\n",
    "ACCOUNT_MIN_BOT_POSTS = int(EXPERIMENT_CONFIG.get(\"account_min_bot_posts\", 1))\n",
    "ACCOUNT_MIN_BOT_POST_RATE = float(EXPERIMENT_CONFIG.get(\"account_min_bot_post_rate\", 0.0))\n",
    "\n",
    "EMBEDDING_DIM = int(EXPERIMENT_CONFIG[\"embedding_dim\"])\n",
    "GRU_UNITS = int(EXPERIMENT_CONFIG[\"gru_units\"])\n",
    "AUX_DENSE_UNITS = int(EXPERIMENT_CONFIG[\"aux_dense_units\"])\n",
    "HEAD_DENSE_UNITS = int(EXPERIMENT_CONFIG[\"head_dense_units\"])\n",
    "DROPOUT_TEXT = float(EXPERIMENT_CONFIG[\"dropout_text\"])\n",
    "DROPOUT_AUX = float(EXPERIMENT_CONFIG[\"dropout_aux\"])\n",
    "DROPOUT_HEAD = float(EXPERIMENT_CONFIG[\"dropout_head\"])\n",
    "\n",
    "EARLY_STOPPING_PATIENCE = int(EXPERIMENT_CONFIG[\"early_stopping_patience\"])\n",
    "REDUCE_LR_PATIENCE = int(EXPERIMENT_CONFIG[\"reduce_lr_patience\"])\n",
    "REDUCE_LR_FACTOR = float(EXPERIMENT_CONFIG[\"reduce_lr_factor\"])\n",
    "REDUCE_LR_MIN_LR = float(EXPERIMENT_CONFIG[\"reduce_lr_min_lr\"])\n",
    "\n",
    "if not (0.0 < TEST_SIZE < 1.0):\n",
    "    raise ValueError(\"test_size must be between 0 and 1.\")\n",
    "if not (0.0 <= VALIDATION_SPLIT < 1.0):\n",
    "    raise ValueError(\"validation_split must be in [0, 1).\")\n",
    "if TOPIC_MATCH_MODE not in {\"contains\", \"word\"}:\n",
    "    raise ValueError('topic_match_mode must be \"contains\" or \"word\".')\n",
    "if ACCOUNT_DECISION_RULE not in {\"mean\", \"any\"}:\n",
    "    raise ValueError('account_decision_rule must be \"mean\" or \"any\".')\n",
    "if ACCOUNT_MIN_BOT_POSTS < 1:\n",
    "    raise ValueError(\"account_min_bot_posts must be >= 1.\")\n",
    "if not (0.0 <= ACCOUNT_MIN_BOT_POST_RATE <= 1.0):\n",
    "    raise ValueError(\"account_min_bot_post_rate must be in [0, 1].\")\n",
    "if USE_THRESHOLD_SEARCH:\n",
    "    if not (0.0 < THRESHOLD_SEARCH_MIN < 1.0 and 0.0 < THRESHOLD_SEARCH_MAX < 1.0):\n",
    "        raise ValueError(\"threshold_search_min and threshold_search_max must be in (0, 1).\")\n",
    "    if THRESHOLD_SEARCH_MIN >= THRESHOLD_SEARCH_MAX:\n",
    "        raise ValueError(\"threshold_search_min must be smaller than threshold_search_max.\")\n",
    "    if THRESHOLD_SEARCH_STEPS < 2:\n",
    "        raise ValueError(\"threshold_search_steps must be >= 2.\")\n",
    "\n",
    "\n",
    "def load_target_topic_keywords():\n",
    "    topic_keywords = {}\n",
    "    target_langs = [TARGET_LANG]\n",
    "    if TARGET_LANG == \"fr\" and TOPIC_INCLUDE_CROSS_LANG_KEYWORDS:\n",
    "        for lang in TOPIC_CROSS_LANGS:\n",
    "            if isinstance(lang, str) and lang.strip().lower() not in target_langs:\n",
    "                target_langs.append(lang.strip().lower())\n",
    "    for lang in target_langs:\n",
    "        for source_name in combined.get(lang, {}).get(\"sources\", []):\n",
    "            source_path = DATA_DIR / source_name\n",
    "            with source_path.open() as f:\n",
    "                payload = json.load(f)\n",
    "            for topic_item in payload.get(\"metadata\", {}).get(\"topics\", []):\n",
    "                topic = str(topic_item.get(\"topic\", \"\")).strip().lower()\n",
    "                if not topic:\n",
    "                    continue\n",
    "                keywords = {\n",
    "                    str(keyword).strip().lower()\n",
    "                    for keyword in topic_item.get(\"keywords\", [])\n",
    "                    if str(keyword).strip()\n",
    "                }\n",
    "                keywords.add(topic)\n",
    "                topic_keywords.setdefault(topic, set()).update(keywords)\n",
    "    return {topic: sorted(values, key=len, reverse=True) for topic, values in topic_keywords.items()}\n",
    "def add_topic_features(df, topic_keywords, match_mode):\n",
    "    out = df.copy()\n",
    "    text_lower = out[\"text_clean\"].str.lower()\n",
    "    topic_cols = []\n",
    "    for topic, keywords in topic_keywords.items():\n",
    "        col = f\"topic_{topic}\"\n",
    "        topic_cols.append(col)\n",
    "        if not keywords:\n",
    "            out[col] = 0\n",
    "            continue\n",
    "        if match_mode == \"word\":\n",
    "            pattern = \"|\".join(rf\"\\\\b{re.escape(keyword)}\\\\b\" for keyword in keywords)\n",
    "        else:\n",
    "            pattern = \"|\".join(re.escape(keyword) for keyword in keywords)\n",
    "        out[col] = text_lower.str.contains(pattern, regex=True).astype(np.int8)\n",
    "    return out, topic_cols\n",
    "\n",
    "\n",
    "def compute_account_score(author_ids, true_labels, pred_probs, threshold, decision_rule):\n",
    "    cfg = globals().get(\"EXPERIMENT_CONFIG\", {})\n",
    "    min_bot_posts = int(cfg.get(\"account_min_bot_posts\", globals().get(\"ACCOUNT_MIN_BOT_POSTS\", 1)))\n",
    "    min_bot_post_rate = float(cfg.get(\"account_min_bot_post_rate\", globals().get(\"ACCOUNT_MIN_BOT_POST_RATE\", 0.0)))\n",
    "\n",
    "    tmp = pd.DataFrame(\n",
    "        {\n",
    "            \"author_id\": author_ids,\n",
    "            \"true_is_bot\": np.asarray(true_labels, dtype=np.int64),\n",
    "            \"pred_prob\": np.asarray(pred_probs, dtype=np.float32),\n",
    "        }\n",
    "    )\n",
    "    tmp[\"pred_post\"] = (tmp[\"pred_prob\"] >= threshold).astype(np.int64)\n",
    "\n",
    "    account = (\n",
    "        tmp.groupby(\"author_id\", as_index=False)\n",
    "        .agg(\n",
    "            true_is_bot=(\"true_is_bot\", \"max\"),\n",
    "            mean_prob=(\"pred_prob\", \"mean\"),\n",
    "            any_pred=(\"pred_post\", \"max\"),\n",
    "            bot_post_count=(\"pred_post\", \"sum\"),\n",
    "            n_posts=(\"pred_post\", \"size\"),\n",
    "        )\n",
    "    )\n",
    "\n",
    "    if decision_rule == \"any\":\n",
    "        pred = account[\"any_pred\"].to_numpy(dtype=np.int64)\n",
    "    else:\n",
    "        pred = (account[\"mean_prob\"].to_numpy(dtype=np.float32) >= float(threshold)).astype(np.int64)\n",
    "\n",
    "    if min_bot_posts > 1:\n",
    "        pred = pred & (account[\"bot_post_count\"].to_numpy(dtype=np.int64) >= int(min_bot_posts))\n",
    "    if min_bot_post_rate > 0.0:\n",
    "        n_posts = np.maximum(account[\"n_posts\"].to_numpy(dtype=np.float32), 1.0)\n",
    "        rates = account[\"bot_post_count\"].to_numpy(dtype=np.float32) / n_posts\n",
    "        pred = pred & (rates >= float(min_bot_post_rate))\n",
    "\n",
    "    account[\"pred_is_bot\"] = pred.astype(np.int64)\n",
    "\n",
    "    tp_accounts = int(((account[\"true_is_bot\"] == 1) & (account[\"pred_is_bot\"] == 1)).sum())\n",
    "    fn_accounts = int(((account[\"true_is_bot\"] == 1) & (account[\"pred_is_bot\"] == 0)).sum())\n",
    "    fp_accounts = int(((account[\"true_is_bot\"] == 0) & (account[\"pred_is_bot\"] == 1)).sum())\n",
    "\n",
    "    score = (4 * tp_accounts) - (1 * fn_accounts) - (2 * fp_accounts)\n",
    "    return score, tp_accounts, fn_accounts, fp_accounts, len(account)\n",
    "\n",
    "\n",
    "def predict_for_indices(model, post_indices):\n",
    "    if len(post_indices) == 0:\n",
    "        return np.array([], dtype=np.float32)\n",
    "    return model.predict(\n",
    "        {\n",
    "            \"input_ids\": input_ids_en[post_indices],\n",
    "            \"attention_mask\": attention_mask_en[post_indices],\n",
    "            \"aux_features\": X_aux_en[post_indices],\n",
    "        },\n",
    "        verbose=0,\n",
    "    ).ravel()\n",
    "\n",
    "\n",
    "if USE_TOPIC_FEATURES:\n",
    "    topic_keywords_en = load_target_topic_keywords()\n",
    "    train_en_model, topic_feature_cols_en = add_topic_features(train_en, topic_keywords_en, TOPIC_MATCH_MODE)\n",
    "else:\n",
    "    train_en_model = train_en.copy()\n",
    "    topic_feature_cols_en = []\n",
    "\n",
    "input_ids_en = np.asarray(encodings_en[\"input_ids\"], dtype=np.int32)\n",
    "attention_mask_en = np.asarray(encodings_en[\"attention_mask\"], dtype=np.float32)\n",
    "X_topic_en = (\n",
    "    train_en_model[topic_feature_cols_en].to_numpy(dtype=np.float32)\n",
    "    if topic_feature_cols_en\n",
    "    else np.zeros((len(train_en_model), 0), dtype=np.float32)\n",
    ")\n",
    "if TOPIC_FEATURE_SCALE != 1.0:\n",
    "    X_topic_en = X_topic_en * TOPIC_FEATURE_SCALE\n",
    "X_aux_en = np.concatenate([X_meta_en_scaled, X_topic_en], axis=1)\n",
    "\n",
    "all_post_idx = np.arange(len(y_en))\n",
    "\n",
    "if SPLIT_BY_AUTHOR:\n",
    "    author_labels_df = (\n",
    "        train_en_model.groupby(\"author_id\", as_index=False)[\"is_bot\"]\n",
    "        .max()\n",
    "        .rename(columns={\"is_bot\": \"account_is_bot\"})\n",
    "    )\n",
    "    all_authors = author_labels_df[\"author_id\"].to_numpy()\n",
    "    all_author_labels = author_labels_df[\"account_is_bot\"].to_numpy(dtype=np.int64)\n",
    "\n",
    "    train_authors, test_authors = train_test_split(\n",
    "        all_authors,\n",
    "        test_size=TEST_SIZE,\n",
    "        random_state=RANDOM_SEED,\n",
    "        stratify=all_author_labels,\n",
    "    )\n",
    "\n",
    "    if VALIDATION_SPLIT > 0:\n",
    "        train_author_labels = (\n",
    "            author_labels_df.set_index(\"author_id\").loc[train_authors, \"account_is_bot\"].to_numpy(dtype=np.int64)\n",
    "        )\n",
    "        fit_authors, val_authors = train_test_split(\n",
    "            train_authors,\n",
    "            test_size=VALIDATION_SPLIT,\n",
    "            random_state=RANDOM_SEED,\n",
    "            stratify=train_author_labels,\n",
    "        )\n",
    "    else:\n",
    "        fit_authors = train_authors\n",
    "        val_authors = np.array([], dtype=all_authors.dtype)\n",
    "\n",
    "    fit_author_set = set(fit_authors.tolist())\n",
    "    val_author_set = set(val_authors.tolist())\n",
    "    test_author_set = set(test_authors.tolist())\n",
    "\n",
    "    fit_idx = np.flatnonzero(train_en_model[\"author_id\"].isin(fit_author_set).to_numpy())\n",
    "    val_idx = np.flatnonzero(train_en_model[\"author_id\"].isin(val_author_set).to_numpy())\n",
    "    test_idx = np.flatnonzero(train_en_model[\"author_id\"].isin(test_author_set).to_numpy())\n",
    "    split_mode = \"author\"\n",
    "else:\n",
    "    train_idx, test_idx = train_test_split(\n",
    "        all_post_idx,\n",
    "        test_size=TEST_SIZE,\n",
    "        random_state=RANDOM_SEED,\n",
    "        stratify=y_en,\n",
    "    )\n",
    "\n",
    "    if VALIDATION_SPLIT > 0:\n",
    "        fit_idx, val_idx = train_test_split(\n",
    "            train_idx,\n",
    "            test_size=VALIDATION_SPLIT,\n",
    "            random_state=RANDOM_SEED,\n",
    "            stratify=y_en[train_idx],\n",
    "        )\n",
    "    else:\n",
    "        fit_idx = train_idx\n",
    "        val_idx = np.array([], dtype=np.int64)\n",
    "\n",
    "    split_mode = \"post\"\n",
    "\n",
    "X_fit_ids, X_test_ids = input_ids_en[fit_idx], input_ids_en[test_idx]\n",
    "X_fit_mask, X_test_mask = attention_mask_en[fit_idx], attention_mask_en[test_idx]\n",
    "X_fit_aux, X_test_aux = X_aux_en[fit_idx], X_aux_en[test_idx]\n",
    "y_fit, y_test = y_en[fit_idx], y_en[test_idx]\n",
    "\n",
    "X_val_ids = input_ids_en[val_idx] if len(val_idx) else None\n",
    "X_val_mask = attention_mask_en[val_idx] if len(val_idx) else None\n",
    "X_val_aux = X_aux_en[val_idx] if len(val_idx) else None\n",
    "y_val = y_en[val_idx] if len(val_idx) else None\n",
    "\n",
    "fit_author_ids = np.unique(train_en_model.iloc[fit_idx][\"author_id\"].to_numpy())\n",
    "val_author_ids = np.unique(train_en_model.iloc[val_idx][\"author_id\"].to_numpy()) if len(val_idx) else np.array([])\n",
    "test_author_ids = np.unique(train_en_model.iloc[test_idx][\"author_id\"].to_numpy())\n",
    "\n",
    "class_weight_dict = None\n",
    "if USE_CLASS_WEIGHTS:\n",
    "    classes = np.unique(y_fit)\n",
    "    class_weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_fit)\n",
    "    class_weight_dict = {int(c): float(w) for c, w in zip(classes, class_weights)}\n",
    "\n",
    "\n",
    "def build_multifeature_model(\n",
    "    vocab_size,\n",
    "    seq_len,\n",
    "    aux_dim,\n",
    "    embedding_dim,\n",
    "    gru_units,\n",
    "    aux_dense_units,\n",
    "    head_dense_units,\n",
    "    dropout_text,\n",
    "    dropout_aux,\n",
    "    dropout_head,\n",
    "    learning_rate,\n",
    "):\n",
    "    ids_input = tf.keras.layers.Input(shape=(seq_len,), dtype=\"int32\", name=\"input_ids\")\n",
    "    mask_input = tf.keras.layers.Input(shape=(seq_len,), dtype=\"float32\", name=\"attention_mask\")\n",
    "    aux_input = tf.keras.layers.Input(shape=(aux_dim,), dtype=\"float32\", name=\"aux_features\")\n",
    "\n",
    "    x = tf.keras.layers.Embedding(vocab_size, embedding_dim, name=\"token_embedding\")(ids_input)\n",
    "    mask = tf.keras.layers.Reshape((seq_len, 1))(mask_input)\n",
    "    x = tf.keras.layers.Multiply()([x, mask])\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(gru_units))(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_text)(x)\n",
    "\n",
    "    aux = tf.keras.layers.Dense(aux_dense_units, activation=\"relu\")(aux_input)\n",
    "    aux = tf.keras.layers.Dropout(dropout_aux)(aux)\n",
    "\n",
    "    merged = tf.keras.layers.Concatenate()([x, aux])\n",
    "    merged = tf.keras.layers.Dense(head_dense_units, activation=\"relu\")(merged)\n",
    "    merged = tf.keras.layers.Dropout(dropout_head)(merged)\n",
    "    output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(merged)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[ids_input, mask_input, aux_input], outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy(name=\"accuracy\"), tf.keras.metrics.AUC(name=\"auc\")],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "\n",
    "model_en = build_multifeature_model(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    seq_len=MAX_LENGTH,\n",
    "    aux_dim=X_fit_aux.shape[1],\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    gru_units=GRU_UNITS,\n",
    "    aux_dense_units=AUX_DENSE_UNITS,\n",
    "    head_dense_units=HEAD_DENSE_UNITS,\n",
    "    dropout_text=DROPOUT_TEXT,\n",
    "    dropout_aux=DROPOUT_AUX,\n",
    "    dropout_head=DROPOUT_HEAD,\n",
    "    learning_rate=LEARNING_RATE,\n",
    ")\n",
    "\n",
    "has_validation = len(val_idx) > 0\n",
    "callbacks = []\n",
    "if has_validation and EARLY_STOPPING_PATIENCE > 0:\n",
    "    callbacks.append(\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_auc\",\n",
    "            mode=\"max\",\n",
    "            patience=EARLY_STOPPING_PATIENCE,\n",
    "            restore_best_weights=True,\n",
    "        )\n",
    "    )\n",
    "if has_validation and REDUCE_LR_PATIENCE > 0:\n",
    "    callbacks.append(\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_auc\",\n",
    "            mode=\"max\",\n",
    "            factor=REDUCE_LR_FACTOR,\n",
    "            patience=REDUCE_LR_PATIENCE,\n",
    "            min_lr=REDUCE_LR_MIN_LR,\n",
    "        )\n",
    "    )\n",
    "\n",
    "train_inputs = {\n",
    "    \"input_ids\": X_fit_ids,\n",
    "    \"attention_mask\": X_fit_mask,\n",
    "    \"aux_features\": X_fit_aux,\n",
    "}\n",
    "\n",
    "fit_kwargs = {\n",
    "    \"epochs\": EPOCHS,\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"class_weight\": class_weight_dict,\n",
    "    \"callbacks\": callbacks,\n",
    "    \"verbose\": 1,\n",
    "}\n",
    "if has_validation:\n",
    "    fit_kwargs[\"validation_data\"] = (\n",
    "        {\n",
    "            \"input_ids\": X_val_ids,\n",
    "            \"attention_mask\": X_val_mask,\n",
    "            \"aux_features\": X_val_aux,\n",
    "        },\n",
    "        y_val,\n",
    "    )\n",
    "\n",
    "history_en = model_en.fit(train_inputs, y_fit, **fit_kwargs)\n",
    "\n",
    "post_prob_fit = predict_for_indices(model_en, fit_idx)\n",
    "post_prob_val = predict_for_indices(model_en, val_idx)\n",
    "post_prob_test = predict_for_indices(model_en, test_idx)\n",
    "\n",
    "y_prob = post_prob_test.copy()\n",
    "threshold_search_results_en = pd.DataFrame()\n",
    "SELECTED_THRESHOLD = PREDICTION_THRESHOLD\n",
    "best_threshold_val_score = None\n",
    "\n",
    "if has_validation and USE_THRESHOLD_SEARCH:\n",
    "    val_author_ids_for_score = train_en_model.iloc[val_idx][\"author_id\"].to_numpy()\n",
    "    search_rows = []\n",
    "    for threshold in np.linspace(THRESHOLD_SEARCH_MIN, THRESHOLD_SEARCH_MAX, THRESHOLD_SEARCH_STEPS):\n",
    "        score, tp_acc, fn_acc, fp_acc, n_accounts = compute_account_score(\n",
    "            author_ids=val_author_ids_for_score,\n",
    "            true_labels=y_val,\n",
    "            pred_probs=post_prob_val,\n",
    "            threshold=float(threshold),\n",
    "            decision_rule=ACCOUNT_DECISION_RULE,\n",
    "        )\n",
    "        search_rows.append(\n",
    "            {\n",
    "                \"threshold\": float(threshold),\n",
    "                \"score\": int(score),\n",
    "                \"tp_accounts\": int(tp_acc),\n",
    "                \"fn_accounts\": int(fn_acc),\n",
    "                \"fp_accounts\": int(fp_acc),\n",
    "                \"n_accounts\": int(n_accounts),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    threshold_search_results_en = pd.DataFrame(search_rows)\n",
    "    best_row = threshold_search_results_en.sort_values(\n",
    "        by=[\"score\", \"fp_accounts\", \"tp_accounts\", \"threshold\"],\n",
    "        ascending=[False, True, False, False],\n",
    "    ).iloc[0]\n",
    "\n",
    "    SELECTED_THRESHOLD = float(best_row[\"threshold\"])\n",
    "    best_threshold_val_score = int(best_row[\"score\"])\n",
    "\n",
    "y_pred = (y_prob >= SELECTED_THRESHOLD).astype(np.int64)\n",
    "\n",
    "test_author_ids_for_score = train_en_model.iloc[test_idx][\"author_id\"].to_numpy()\n",
    "(\n",
    "    test_score,\n",
    "    test_tp_accounts,\n",
    "    test_fn_accounts,\n",
    "    test_fp_accounts,\n",
    "    test_n_accounts,\n",
    ") = compute_account_score(\n",
    "    author_ids=test_author_ids_for_score,\n",
    "    true_labels=y_test,\n",
    "    pred_probs=y_prob,\n",
    "    threshold=SELECTED_THRESHOLD,\n",
    "    decision_rule=ACCOUNT_DECISION_RULE,\n",
    ")\n",
    "\n",
    "(\n",
    "    baseline_test_score,\n",
    "    baseline_tp_accounts,\n",
    "    baseline_fn_accounts,\n",
    "    baseline_fp_accounts,\n",
    "    baseline_n_accounts,\n",
    ") = compute_account_score(\n",
    "    author_ids=test_author_ids_for_score,\n",
    "    true_labels=y_test,\n",
    "    pred_probs=y_prob,\n",
    "    threshold=PREDICTION_THRESHOLD,\n",
    "    decision_rule=ACCOUNT_DECISION_RULE,\n",
    ")\n",
    "\n",
    "print(f\"Split mode: {split_mode}\")\n",
    "print(f\"Topic features enabled: {USE_TOPIC_FEATURES}\")\n",
    "print(f\"Topic match mode: {TOPIC_MATCH_MODE}\")\n",
    "print(f\"Account decision rule: {ACCOUNT_DECISION_RULE}\")\n",
    "print(f\"Account minimum bot posts: {ACCOUNT_MIN_BOT_POSTS}\")\n",
    "print(f\"Account minimum bot-post rate: {ACCOUNT_MIN_BOT_POST_RATE:.2f}\")\n",
    "print(\"Topic columns:\", topic_feature_cols_en)\n",
    "print(f\"Split sizes (fit/val/test posts): {len(fit_idx)}/{len(val_idx)}/{len(test_idx)}\")\n",
    "print(\n",
    "    f\"Split sizes (fit/val/test accounts): {len(fit_author_ids)}/{len(val_author_ids)}/{len(test_author_ids)}\"\n",
    ")\n",
    "print(f\"Default threshold from config: {PREDICTION_THRESHOLD:.4f}\")\n",
    "print(f\"Selected threshold used on test: {SELECTED_THRESHOLD:.4f}\")\n",
    "if best_threshold_val_score is not None:\n",
    "    print(f\"Best validation score from threshold search: {best_threshold_val_score}\")\n",
    "print(f\"Test Accuracy (post-level): {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Test ROC-AUC (post-level): {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "print(\n",
    "    f\"Test account score @ selected threshold -> score={test_score}, TP={test_tp_accounts}, FN={test_fn_accounts}, FP={test_fp_accounts}, accounts={test_n_accounts}\"\n",
    ")\n",
    "print(\n",
    "    f\"Test account score @ config threshold -> score={baseline_test_score}, TP={baseline_tp_accounts}, FN={baseline_fn_accounts}, FP={baseline_fp_accounts}, accounts={baseline_n_accounts}\"\n",
    ")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f66a209",
   "metadata": {},
   "source": [
    "## Account-Level Threshold Calibration (No Booster)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a48c726f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibrated on val: threshold=0.375, rule=mean\n",
      "Score=26 (TP=7, FN=0, FP=1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if \"post_prob_val\" in globals() and len(val_idx):\n",
    "    eval_idx = val_idx\n",
    "    eval_probs = np.asarray(post_prob_val, dtype=np.float32)\n",
    "    split_name = \"val\"\n",
    "else:\n",
    "    eval_idx = test_idx\n",
    "    eval_probs = np.asarray(post_prob_test, dtype=np.float32)\n",
    "    split_name = \"test\"\n",
    "\n",
    "thresholds = np.linspace(0.05, 0.95, 181)\n",
    "rules = [\"mean\", \"any\"]\n",
    "\n",
    "def _score_accounts(author_ids, true_labels, probs, threshold, rule):\n",
    "    df = pd.DataFrame({\n",
    "        \"author_id\": author_ids,\n",
    "        \"true_is_bot\": true_labels.astype(np.int64),\n",
    "        \"pred_prob\": probs.astype(np.float32),\n",
    "    })\n",
    "    df[\"pred_post\"] = (df[\"pred_prob\"] >= threshold).astype(np.int64)\n",
    "    account_df = df.groupby(\"author_id\", as_index=False).agg(\n",
    "        true_is_bot=(\"true_is_bot\", \"max\"),\n",
    "        mean_prob=(\"pred_prob\", \"mean\"),\n",
    "        any_pred=(\"pred_post\", \"max\"),\n",
    "    )\n",
    "    if rule == \"any\":\n",
    "        account_df[\"pred_is_bot\"] = account_df[\"any_pred\"].astype(np.int64)\n",
    "    else:\n",
    "        account_df[\"pred_is_bot\"] = (account_df[\"mean_prob\"] >= threshold).astype(np.int64)\n",
    "    tp = int(((account_df[\"true_is_bot\"] == 1) & (account_df[\"pred_is_bot\"] == 1)).sum())\n",
    "    fn = int(((account_df[\"true_is_bot\"] == 1) & (account_df[\"pred_is_bot\"] == 0)).sum())\n",
    "    fp = int(((account_df[\"true_is_bot\"] == 0) & (account_df[\"pred_is_bot\"] == 1)).sum())\n",
    "    score = (4 * tp) - fn - (2 * fp)\n",
    "    return score, tp, fn, fp\n",
    "\n",
    "best = {\"score\": -1e9}\n",
    "author_ids = train_en_model.iloc[eval_idx][\"author_id\"].to_numpy()\n",
    "true_labels = train_en_model.iloc[eval_idx][\"is_bot\"].to_numpy()\n",
    "\n",
    "for rule in rules:\n",
    "    for thr in thresholds:\n",
    "        score, tp, fn, fp = _score_accounts(author_ids, true_labels, eval_probs, thr, rule)\n",
    "        if score > best[\"score\"]:\n",
    "            best = {\"score\": score, \"threshold\": float(thr), \"rule\": rule, \"tp\": tp, \"fn\": fn, \"fp\": fp}\n",
    "\n",
    "SELECTED_THRESHOLD = float(best[\"threshold\"])\n",
    "ACCOUNT_DECISION_RULE = str(best[\"rule\"])\n",
    "\n",
    "print(f\"Calibrated on {split_name}: threshold={SELECTED_THRESHOLD:.3f}, rule={ACCOUNT_DECISION_RULE}\")\n",
    "print(f\"Score={best[\"score\"]} (TP={best[\"tp\"]}, FN={best[\"fn\"]}, FP={best[\"fp\"]})\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fa6c0f",
   "metadata": {},
   "source": [
    "## Lightweight Account Model (Logistic Regression)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3bc2e0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogReg threshold (val): 0.170 score=24 TP=7 FN=0 FP=2\n",
      "LogReg test score: 40 TP=11 FN=0 FP=2 TN=54\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "USE_LOGREG = bool(EXPERIMENT_CONFIG.get(\"use_logreg_account_model\", False))\n",
    "if not USE_LOGREG:\n",
    "    print(\"Logistic-regression account model disabled. Set use_logreg_account_model=True to enable.\")\n",
    "else:\n",
    "    if \"post_prob_fit\" not in globals() or \"post_prob_test\" not in globals():\n",
    "        raise ValueError(\"Run the first-stage training cell first (post_prob_fit/post_prob_test missing).\")\n",
    "\n",
    "    min_posts = int(EXPERIMENT_CONFIG.get(\"logreg_min_posts\", 2))\n",
    "    max_iter = int(EXPERIMENT_CONFIG.get(\"logreg_max_iter\", 200))\n",
    "\n",
    "    def _account_features(author_ids, probs):\n",
    "        df = pd.DataFrame({\"author_id\": author_ids, \"prob\": probs.astype(np.float32)})\n",
    "        df[\"pred_05\"] = (df[\"prob\"] >= 0.5).astype(np.int64)\n",
    "        agg = df.groupby(\"author_id\", as_index=True).agg(\n",
    "            n_posts=(\"prob\", \"size\"),\n",
    "            mean_prob=(\"prob\", \"mean\"),\n",
    "            std_prob=(\"prob\", \"std\"),\n",
    "            min_prob=(\"prob\", \"min\"),\n",
    "            max_prob=(\"prob\", \"max\"),\n",
    "            frac_above_05=(\"pred_05\", \"mean\"),\n",
    "        )\n",
    "        agg = agg.fillna(0.0)\n",
    "        agg = agg[agg[\"n_posts\"] >= min_posts]\n",
    "        return agg\n",
    "\n",
    "    def _account_labels(indices):\n",
    "        return (\n",
    "            train_en_model.iloc[indices][[\"author_id\", \"is_bot\"]]\n",
    "            .groupby(\"author_id\", as_index=True)[\"is_bot\"]\n",
    "            .max()\n",
    "        )\n",
    "\n",
    "    fit_auth = train_en_model.iloc[fit_idx][\"author_id\"].to_numpy()\n",
    "    fit_features = _account_features(fit_auth, np.asarray(post_prob_fit, dtype=np.float32))\n",
    "    fit_labels = _account_labels(fit_idx).reindex(fit_features.index).to_numpy(dtype=np.int64)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_fit = scaler.fit_transform(fit_features.to_numpy())\n",
    "    lr_model = LogisticRegression(max_iter=max_iter, class_weight=\"balanced\")\n",
    "    lr_model.fit(X_fit, fit_labels)\n",
    "\n",
    "    # Use validation split for threshold selection if available\n",
    "    if \"post_prob_val\" in globals() and len(val_idx):\n",
    "        val_auth = train_en_model.iloc[val_idx][\"author_id\"].to_numpy()\n",
    "        val_features = _account_features(val_auth, np.asarray(post_prob_val, dtype=np.float32))\n",
    "        val_labels = _account_labels(val_idx).reindex(val_features.index).to_numpy(dtype=np.int64)\n",
    "        X_val = scaler.transform(val_features.to_numpy())\n",
    "        val_probs = lr_model.predict_proba(X_val)[:, 1]\n",
    "        thresholds = np.linspace(0.05, 0.95, 181)\n",
    "        best = {\"score\": -1e9}\n",
    "        for thr in thresholds:\n",
    "            pred = (val_probs >= thr).astype(np.int64)\n",
    "            tp = int(((val_labels == 1) & (pred == 1)).sum())\n",
    "            fn = int(((val_labels == 1) & (pred == 0)).sum())\n",
    "            fp = int(((val_labels == 0) & (pred == 1)).sum())\n",
    "            score = (4 * tp) - fn - (2 * fp)\n",
    "            if score > best[\"score\"]:\n",
    "                best = {\"score\": score, \"threshold\": float(thr), \"tp\": tp, \"fn\": fn, \"fp\": fp}\n",
    "        lr_threshold = float(best[\"threshold\"])\n",
    "        print(f\"LogReg threshold (val): {lr_threshold:.3f} score={best[\"score\"]} TP={best[\"tp\"]} FN={best[\"fn\"]} FP={best[\"fp\"]}\")\n",
    "    else:\n",
    "        lr_threshold = 0.5\n",
    "        print(\"No validation split; using default logreg threshold 0.5\")\n",
    "\n",
    "    # Evaluate on test\n",
    "    test_auth = train_en_model.iloc[test_idx][\"author_id\"].to_numpy()\n",
    "    test_features = _account_features(test_auth, np.asarray(post_prob_test, dtype=np.float32))\n",
    "    test_labels = _account_labels(test_idx).reindex(test_features.index).to_numpy(dtype=np.int64)\n",
    "    X_test = scaler.transform(test_features.to_numpy())\n",
    "    test_probs = lr_model.predict_proba(X_test)[:, 1]\n",
    "    test_pred = (test_probs >= lr_threshold).astype(np.int64)\n",
    "    tp = int(((test_labels == 1) & (test_pred == 1)).sum())\n",
    "    fn = int(((test_labels == 1) & (test_pred == 0)).sum())\n",
    "    fp = int(((test_labels == 0) & (test_pred == 1)).sum())\n",
    "    tn = int(((test_labels == 0) & (test_pred == 0)).sum())\n",
    "    score = (4 * tp) - fn - (2 * fp)\n",
    "    print(f\"LogReg test score: {score} TP={tp} FN={fn} FP={fp} TN={tn}\")\n",
    "\n",
    "    logreg_account_predictions = pd.DataFrame({\n",
    "        \"author_id\": test_features.index.to_numpy(),\n",
    "        \"true_is_bot\": test_labels,\n",
    "        \"pred_prob\": test_probs,\n",
    "        \"pred_is_bot\": test_pred,\n",
    "    })\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0a6a2a8",
   "metadata": {},
   "source": [
    "## Bot-detector score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e84ff0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Threshold used: 0.3750\n",
      "Account decision rule: mean\n",
      "Account minimum bot posts: 1\n",
      "Account minimum bot-post rate: 0.00\n",
      "Accounts scored (test split): 69\n",
      "TP: 11  FN: 0  FP: 2  TN: 56\n",
      "Bot detector score: 40\n",
      "Score out of max possible: 40/44 (90.91%)\n",
      "Score range on this split: [-127, 44]\n",
      "Range-normalized score: 97.66%\n"
     ]
    }
   ],
   "source": [
    "if any(name not in globals() for name in [\"train_en_model\", \"test_idx\", \"y_prob\", \"PREDICTION_THRESHOLD\"]):\n",
    "    raise ValueError(\"Run the Train-Test split for model cell first.\")\n",
    "\n",
    "threshold_for_score = float(globals().get(\"SELECTED_THRESHOLD\", PREDICTION_THRESHOLD))\n",
    "if \"ACCOUNT_DECISION_RULE\" in globals():\n",
    "    account_decision_rule = str(ACCOUNT_DECISION_RULE)\n",
    "elif \"EXPERIMENT_CONFIG\" in globals():\n",
    "    account_decision_rule = str(EXPERIMENT_CONFIG.get(\"account_decision_rule\", \"mean\"))\n",
    "else:\n",
    "    account_decision_rule = \"mean\"\n",
    "\n",
    "if account_decision_rule not in {\"mean\", \"any\"}:\n",
    "    raise ValueError('account_decision_rule must be \"mean\" or \"any\".')\n",
    "\n",
    "account_min_bot_posts = int(globals().get(\"ACCOUNT_MIN_BOT_POSTS\", EXPERIMENT_CONFIG.get(\"account_min_bot_posts\", 1)))\n",
    "account_min_bot_post_rate = float(globals().get(\"ACCOUNT_MIN_BOT_POST_RATE\", EXPERIMENT_CONFIG.get(\"account_min_bot_post_rate\", 0.0)))\n",
    "\n",
    "# Build test-set account labels/predictions from post-level outputs.\n",
    "score_df = train_en_model.iloc[test_idx][[\"author_id\", \"is_bot\"]].copy()\n",
    "score_df[\"pred_prob\"] = y_prob\n",
    "score_df[\"pred_post\"] = (score_df[\"pred_prob\"] >= threshold_for_score).astype(np.int64)\n",
    "\n",
    "account_df = (\n",
    "    score_df.groupby(\"author_id\", as_index=False)\n",
    "    .agg(\n",
    "        true_is_bot=(\"is_bot\", \"max\"),\n",
    "        mean_prob=(\"pred_prob\", \"mean\"),\n",
    "        any_pred=(\"pred_post\", \"max\"),\n",
    "        bot_post_count=(\"pred_post\", \"sum\"),\n",
    "        n_posts=(\"pred_post\", \"size\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "if account_decision_rule == \"any\":\n",
    "    pred = account_df[\"any_pred\"].to_numpy(dtype=np.int64)\n",
    "else:\n",
    "    pred = (account_df[\"mean_prob\"].to_numpy(dtype=np.float32) >= threshold_for_score).astype(np.int64)\n",
    "\n",
    "if account_min_bot_posts > 1:\n",
    "    pred = pred & (account_df[\"bot_post_count\"].to_numpy(dtype=np.int64) >= account_min_bot_posts)\n",
    "if account_min_bot_post_rate > 0.0:\n",
    "    n_posts = np.maximum(account_df[\"n_posts\"].to_numpy(dtype=np.float32), 1.0)\n",
    "    bot_rates = account_df[\"bot_post_count\"].to_numpy(dtype=np.float32) / n_posts\n",
    "    pred = pred & (bot_rates >= account_min_bot_post_rate)\n",
    "\n",
    "account_df[\"pred_is_bot\"] = pred.astype(np.int64)\n",
    "\n",
    "tp_accounts = int(((account_df[\"true_is_bot\"] == 1) & (account_df[\"pred_is_bot\"] == 1)).sum())\n",
    "fn_accounts = int(((account_df[\"true_is_bot\"] == 1) & (account_df[\"pred_is_bot\"] == 0)).sum())\n",
    "fp_accounts = int(((account_df[\"true_is_bot\"] == 0) & (account_df[\"pred_is_bot\"] == 1)).sum())\n",
    "tn_accounts = int(((account_df[\"true_is_bot\"] == 0) & (account_df[\"pred_is_bot\"] == 0)).sum())\n",
    "\n",
    "score = (4 * tp_accounts) - (1 * fn_accounts) - (2 * fp_accounts)\n",
    "max_possible_score = 4 * int((account_df[\"true_is_bot\"] == 1).sum())\n",
    "min_possible_score = (\n",
    "    -1 * int((account_df[\"true_is_bot\"] == 1).sum())\n",
    "    -2 * int((account_df[\"true_is_bot\"] == 0).sum())\n",
    ")\n",
    "\n",
    "score_ratio = score / max_possible_score if max_possible_score > 0 else np.nan\n",
    "score_normalized = (\n",
    "    (score - min_possible_score) / (max_possible_score - min_possible_score)\n",
    "    if max_possible_score != min_possible_score\n",
    "    else np.nan\n",
    ")\n",
    "\n",
    "print(f\"Threshold used: {threshold_for_score:.4f}\")\n",
    "print(f\"Account decision rule: {account_decision_rule}\")\n",
    "print(f\"Account minimum bot posts: {account_min_bot_posts}\")\n",
    "print(f\"Account minimum bot-post rate: {account_min_bot_post_rate:.2f}\")\n",
    "print(f\"Accounts scored (test split): {len(account_df)}\")\n",
    "print(f\"TP: {tp_accounts}  FN: {fn_accounts}  FP: {fp_accounts}  TN: {tn_accounts}\")\n",
    "print(f\"Bot detector score: {score}\")\n",
    "print(f\"Score out of max possible: {score}/{max_possible_score} ({score_ratio:.2%})\")\n",
    "print(f\"Score range on this split: [{min_possible_score}, {max_possible_score}]\")\n",
    "print(f\"Range-normalized score: {score_normalized:.2%}\")\n",
    "\n",
    "if \"second_stage_test_score_en\" in globals():\n",
    "    print(\n",
    "        f\"Second-stage account-model score (recommended): {second_stage_test_score_en} at threshold {second_stage_selected_threshold_en:.4f}\"\n",
    "    )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a2db3f5",
   "metadata": {},
   "source": [
    "## Strongest booster (account-level second stage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1c7e2d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-11 09:08:28.534058: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-11 09:08:33.571543: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-11 09:08:43.316865: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-11 09:09:22.492253: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-11 09:09:29.044763: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-11 09:09:34.727474: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-11 09:09:46.942398: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-11 09:10:41.739139: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-11 09:10:48.146763: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-11 09:10:53.263209: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-11 09:11:03.803076: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-11 09:11:42.586585: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-11 09:11:48.378814: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-11 09:11:54.540379: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-11 09:12:05.788186: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-11 09:12:52.832484: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-11 09:12:59.784422: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-11 09:13:05.195723: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-11 09:13:17.154522: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-11 09:14:05.205464: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-11 09:14:10.768079: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-11 09:14:16.052852: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-11 09:14:29.430556: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-11 09:15:05.308380: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-11 09:15:10.899952: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-11 09:15:25.791082: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-11 09:16:04.193118: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building OOF first-stage features for second-stage training (seeds=[13, 42], folds=4, epochs=4)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-02-11 09:16:10.587471: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-11 09:16:15.646575: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-11 09:16:22.666047: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-11 09:16:47.183826: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-11 09:16:53.717051: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-11 09:16:59.919162: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-11 09:17:22.213896: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-11 09:17:28.613387: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-11 09:17:35.902103: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-11 09:17:59.418420: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-11 09:18:06.327562: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-11 09:18:12.640169: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-11 09:18:35.371294: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-11 09:18:41.822992: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-11 09:18:48.343826: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-11 09:19:12.354440: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-11 09:19:18.658584: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-11 09:19:24.742718: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-11 09:19:46.022808: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-11 09:19:52.388772: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-11 09:19:58.584361: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-11 09:50:30.335763: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n",
      "2026-02-11 09:50:37.155838: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_18}}\n",
      "2026-02-11 09:50:43.442481: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_17}}\n",
      "2026-02-11 10:09:10.981685: E tensorflow/core/framework/node_def_util.cc:680] NodeDef mentions attribute use_unbounded_threadpool which is not in the op definition: Op<name=MapDataset; signature=input_dataset:variant, other_arguments: -> handle:variant; attr=f:func; attr=Targuments:list(type),min=0; attr=output_types:list(type),min=1; attr=output_shapes:list(shape),min=1; attr=use_inter_op_parallelism:bool,default=true; attr=preserve_cardinality:bool,default=false; attr=force_synchronous:bool,default=false; attr=metadata:string,default=\"\"> This may be expected if your graph generating binary is newer  than this binary. Unknown attributes will be ignored. NodeDef: {{node ParallelMapDatasetV2/_16}}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed-level first-stage account scores (test):\n",
      " seed  threshold  test_score  tp_accounts  fn_accounts  fp_accounts\n",
      "   13      0.475          39           10            1            0\n",
      "   29      0.495          39           10            1            0\n",
      "   42      0.430          39           10            1            0\n",
      "   73      0.220          38           11            0            3\n",
      "  101      0.335          40           11            0            2\n",
      "  137      0.470          39           10            1            0\n",
      "  173      0.525          39           10            1            0\n",
      "Seed score mean/std: 39.00 / 0.58\n",
      "Ensemble aggregation: median\n",
      "Ensemble selected threshold: 0.4400\n",
      "Ensemble test score: 39 (TP=10, FN=1, FP=0, accounts=69)\n",
      "Second-stage fit feature source: oof_first_stage\n",
      "OOF seed report for second-stage fit features:\n",
      " seed  fit_account_score_at_ensemble_threshold  tp_accounts  fn_accounts  fp_accounts  n_accounts\n",
      "   13                                       79           28            9           12         232\n",
      "   42                                       83           26           11            5         232\n",
      "Second-stage candidate report:\n",
      "    profile  alpha  threshold  min_bot_posts  min_bot_post_rate  val_score  val_tp_accounts  val_fn_accounts  val_fp_accounts  test_score  test_tp_accounts  test_fn_accounts  test_fp_accounts\n",
      "regularized    1.0      0.475              3               0.25         26                7                0                1          42                11                 0                 1\n",
      "Baseline account-level validation score (from first-stage means): 21 (TP=6, FN=1, FP=1, threshold=0.4400, min_posts=3, min_rate=0.25)\n",
      "Second-stage profile mode: regularized\n",
      "Second-stage selected profile: regularized\n",
      "Second-stage blend alpha (CatBoost weight): 1.00\n",
      "Second-stage threshold: 0.4750\n",
      "Second-stage min bot posts: 3\n",
      "Second-stage min bot-post rate: 0.25\n",
      "Second-stage test score: 42/44 (95.45%)\n",
      "Second-stage confusion components -> TP=11, FN=0, FP=1\n",
      "Second-stage precision=0.9167, recall=1.0000\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    0.9828    0.9913        58\n",
      "           1     0.9167    1.0000    0.9565        11\n",
      "\n",
      "    accuracy                         0.9855        69\n",
      "   macro avg     0.9583    0.9914    0.9739        69\n",
      "weighted avg     0.9867    0.9855    0.9858        69\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report, precision_score, recall_score\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostClassifier\n",
    "except ImportError as exc:\n",
    "    raise ImportError(\"Install catboost first: pip install catboost\") from exc\n",
    "\n",
    "required_globals = [\n",
    "    \"tf\",\n",
    "    \"EXPERIMENT_CONFIG\",\n",
    "    \"build_multifeature_model\",\n",
    "    \"compute_account_score\",\n",
    "    \"train_en_model\",\n",
    "    \"topic_feature_cols_en\",\n",
    "    \"fit_idx\",\n",
    "    \"val_idx\",\n",
    "    \"test_idx\",\n",
    "    \"input_ids_en\",\n",
    "    \"attention_mask_en\",\n",
    "    \"X_aux_en\",\n",
    "    \"y_en\",\n",
    "    \"MAX_LENGTH\",\n",
    "    \"tokenizer\",\n",
    "    \"users_en\",\n",
    "    \"EMBEDDING_DIM\",\n",
    "    \"GRU_UNITS\",\n",
    "    \"AUX_DENSE_UNITS\",\n",
    "    \"HEAD_DENSE_UNITS\",\n",
    "    \"DROPOUT_TEXT\",\n",
    "    \"DROPOUT_AUX\",\n",
    "    \"DROPOUT_HEAD\",\n",
    "    \"LEARNING_RATE\",\n",
    "    \"EPOCHS\",\n",
    "    \"BATCH_SIZE\",\n",
    "    \"RANDOM_SEED\",\n",
    "    \"USE_CLASS_WEIGHTS\",\n",
    "    \"class_weight_dict\",\n",
    "    \"USE_THRESHOLD_SEARCH\",\n",
    "    \"THRESHOLD_SEARCH_MIN\",\n",
    "    \"THRESHOLD_SEARCH_MAX\",\n",
    "    \"THRESHOLD_SEARCH_STEPS\",\n",
    "    \"PREDICTION_THRESHOLD\",\n",
    "    \"ACCOUNT_DECISION_RULE\",\n",
    "    \"EARLY_STOPPING_PATIENCE\",\n",
    "    \"REDUCE_LR_PATIENCE\",\n",
    "    \"REDUCE_LR_FACTOR\",\n",
    "    \"REDUCE_LR_MIN_LR\",\n",
    "]\n",
    "missing = [name for name in required_globals if name not in globals()]\n",
    "if missing:\n",
    "    raise ValueError(f\"Run the Train-Test split for model cell first. Missing: {missing}\")\n",
    "\n",
    "if ACCOUNT_DECISION_RULE not in {\"mean\", \"any\"}:\n",
    "    raise ValueError('account_decision_rule must be \"mean\" or \"any\".')\n",
    "\n",
    "ensemble_seeds_cfg = EXPERIMENT_CONFIG.get(\"ensemble_seeds\", [RANDOM_SEED])\n",
    "if isinstance(ensemble_seeds_cfg, (int, np.integer)):\n",
    "    ensemble_seeds = [int(ensemble_seeds_cfg)]\n",
    "elif isinstance(ensemble_seeds_cfg, (list, tuple)):\n",
    "    ensemble_seeds = [int(seed) for seed in ensemble_seeds_cfg]\n",
    "else:\n",
    "    raise ValueError(\"ensemble_seeds must be an int or a list of ints.\")\n",
    "ensemble_seeds = list(dict.fromkeys(ensemble_seeds))\n",
    "if not ensemble_seeds:\n",
    "    raise ValueError(\"ensemble_seeds cannot be empty.\")\n",
    "\n",
    "ENSEMBLE_AGGREGATION = str(EXPERIMENT_CONFIG.get(\"ensemble_aggregation\", \"mean\")).lower()\n",
    "if ENSEMBLE_AGGREGATION not in {\"mean\", \"median\"}:\n",
    "    raise ValueError('ensemble_aggregation must be \"mean\" or \"median\".')\n",
    "\n",
    "USE_EXTERNAL_PRETRAIN = bool(EXPERIMENT_CONFIG.get(\"use_external_pretrain\", False))\n",
    "EXTERNAL_PRETRAIN_EPOCHS = int(EXPERIMENT_CONFIG.get(\"external_pretrain_epochs\", 1))\n",
    "EXTERNAL_PRETRAIN_BATCH_SIZE = int(EXPERIMENT_CONFIG.get(\"external_pretrain_batch_size\", 128))\n",
    "EXTERNAL_PRETRAIN_USE_BALANCED_WEIGHTS = bool(\n",
    "    EXPERIMENT_CONFIG.get(\"external_pretrain_use_balanced_weights\", True)\n",
    ")\n",
    "\n",
    "USE_SECOND_STAGE_ACCOUNT_MODEL = bool(EXPERIMENT_CONFIG.get(\"use_second_stage_account_model\", True))\n",
    "SECOND_STAGE_PROFILE = str(EXPERIMENT_CONFIG.get(\"second_stage_profile\", \"auto\")).lower()\n",
    "if SECOND_STAGE_PROFILE not in {\"auto\", \"legacy\", \"regularized\", \"custom\"}:\n",
    "    raise ValueError('second_stage_profile must be one of: \"auto\", \"legacy\", \"regularized\", \"custom\".')\n",
    "\n",
    "SECOND_STAGE_USE_BLEND = bool(EXPERIMENT_CONFIG.get(\"second_stage_use_blend\", True))\n",
    "blend_alphas_cfg = EXPERIMENT_CONFIG.get(\"second_stage_blend_alphas\", [1.0, 0.85, 0.70, 0.55])\n",
    "if isinstance(blend_alphas_cfg, (int, float, np.floating, np.integer)):\n",
    "    SECOND_STAGE_BLEND_ALPHAS = [float(blend_alphas_cfg)]\n",
    "elif isinstance(blend_alphas_cfg, (list, tuple)):\n",
    "    SECOND_STAGE_BLEND_ALPHAS = [float(alpha) for alpha in blend_alphas_cfg]\n",
    "else:\n",
    "    raise ValueError(\"second_stage_blend_alphas must be a number or a list of numbers.\")\n",
    "SECOND_STAGE_BLEND_ALPHAS = [a for a in SECOND_STAGE_BLEND_ALPHAS if 0.0 <= a <= 1.0]\n",
    "if not SECOND_STAGE_BLEND_ALPHAS:\n",
    "    SECOND_STAGE_BLEND_ALPHAS = [1.0]\n",
    "if 1.0 not in SECOND_STAGE_BLEND_ALPHAS:\n",
    "    SECOND_STAGE_BLEND_ALPHAS.append(1.0)\n",
    "SECOND_STAGE_BLEND_ALPHAS = sorted(set(SECOND_STAGE_BLEND_ALPHAS), reverse=True)\n",
    "SECOND_STAGE_MIN_GAIN_VS_ENSEMBLE = float(EXPERIMENT_CONFIG.get(\"second_stage_min_gain_vs_ensemble\", 0.0))\n",
    "\n",
    "SECOND_STAGE_MAX_EXTRA_FP_VS_ENSEMBLE = int(\n",
    "    EXPERIMENT_CONFIG.get(\"second_stage_max_extra_fp_vs_ensemble\", 0)\n",
    ")\n",
    "\n",
    "SECOND_STAGE_POST_CALIBRATE = bool(EXPERIMENT_CONFIG.get(\"second_stage_post_calibrate\", True))\n",
    "SECOND_STAGE_ACCOUNT_MIN_BOT_POSTS_DEFAULT = int(\n",
    "    EXPERIMENT_CONFIG.get(\"account_min_bot_posts\", globals().get(\"ACCOUNT_MIN_BOT_POSTS\", 1))\n",
    ")\n",
    "SECOND_STAGE_ACCOUNT_MIN_BOT_POST_RATE_DEFAULT = float(\n",
    "    EXPERIMENT_CONFIG.get(\"account_min_bot_post_rate\", globals().get(\"ACCOUNT_MIN_BOT_POST_RATE\", 0.0))\n",
    ")\n",
    "\n",
    "min_posts_grid_cfg = EXPERIMENT_CONFIG.get(\n",
    "    \"second_stage_account_min_bot_posts_grid\",\n",
    "    [SECOND_STAGE_ACCOUNT_MIN_BOT_POSTS_DEFAULT],\n",
    ")\n",
    "if isinstance(min_posts_grid_cfg, (int, np.integer)):\n",
    "    SECOND_STAGE_ACCOUNT_MIN_BOT_POSTS_GRID = [int(min_posts_grid_cfg)]\n",
    "elif isinstance(min_posts_grid_cfg, (list, tuple)):\n",
    "    SECOND_STAGE_ACCOUNT_MIN_BOT_POSTS_GRID = [int(v) for v in min_posts_grid_cfg]\n",
    "else:\n",
    "    raise ValueError(\"second_stage_account_min_bot_posts_grid must be an int or list of ints.\")\n",
    "SECOND_STAGE_ACCOUNT_MIN_BOT_POSTS_GRID = sorted({max(1, int(v)) for v in SECOND_STAGE_ACCOUNT_MIN_BOT_POSTS_GRID})\n",
    "if not SECOND_STAGE_ACCOUNT_MIN_BOT_POSTS_GRID:\n",
    "    SECOND_STAGE_ACCOUNT_MIN_BOT_POSTS_GRID = [max(1, SECOND_STAGE_ACCOUNT_MIN_BOT_POSTS_DEFAULT)]\n",
    "\n",
    "min_rate_grid_cfg = EXPERIMENT_CONFIG.get(\n",
    "    \"second_stage_account_min_bot_post_rate_grid\",\n",
    "    [SECOND_STAGE_ACCOUNT_MIN_BOT_POST_RATE_DEFAULT],\n",
    ")\n",
    "if isinstance(min_rate_grid_cfg, (int, float, np.integer, np.floating)):\n",
    "    SECOND_STAGE_ACCOUNT_MIN_BOT_POST_RATE_GRID = [float(min_rate_grid_cfg)]\n",
    "elif isinstance(min_rate_grid_cfg, (list, tuple)):\n",
    "    SECOND_STAGE_ACCOUNT_MIN_BOT_POST_RATE_GRID = [float(v) for v in min_rate_grid_cfg]\n",
    "else:\n",
    "    raise ValueError(\"second_stage_account_min_bot_post_rate_grid must be a number or list of numbers.\")\n",
    "SECOND_STAGE_ACCOUNT_MIN_BOT_POST_RATE_GRID = sorted({min(1.0, max(0.0, float(v))) for v in SECOND_STAGE_ACCOUNT_MIN_BOT_POST_RATE_GRID})\n",
    "if not SECOND_STAGE_ACCOUNT_MIN_BOT_POST_RATE_GRID:\n",
    "    SECOND_STAGE_ACCOUNT_MIN_BOT_POST_RATE_GRID = [min(1.0, max(0.0, SECOND_STAGE_ACCOUNT_MIN_BOT_POST_RATE_DEFAULT))]\n",
    "\n",
    "SECOND_STAGE_MIN_VAL_ACCOUNTS_FOR_BOOSTER = int(\n",
    "    EXPERIMENT_CONFIG.get(\"second_stage_min_val_accounts_for_booster\", 25)\n",
    ")\n",
    "\n",
    "SECOND_STAGE_USE_OOF_FEATURES = bool(EXPERIMENT_CONFIG.get(\"second_stage_use_oof_features\", True))\n",
    "SECOND_STAGE_OOF_FOLDS = int(EXPERIMENT_CONFIG.get(\"second_stage_oof_folds\", 4))\n",
    "SECOND_STAGE_OOF_EPOCHS = int(EXPERIMENT_CONFIG.get(\"second_stage_oof_epochs\", max(2, min(EPOCHS, 4))))\n",
    "SECOND_STAGE_OOF_BATCH_SIZE = int(EXPERIMENT_CONFIG.get(\"second_stage_oof_batch_size\", BATCH_SIZE))\n",
    "SECOND_STAGE_OOF_USE_EXTERNAL_PRETRAIN = bool(\n",
    "    EXPERIMENT_CONFIG.get(\"second_stage_oof_use_external_pretrain\", False)\n",
    ")\n",
    "\n",
    "SECOND_STAGE_LEARNING_RATE = float(EXPERIMENT_CONFIG.get(\"second_stage_learning_rate\", 0.05))\n",
    "SECOND_STAGE_MAX_ITER = int(EXPERIMENT_CONFIG.get(\"second_stage_max_iter\", 300))\n",
    "SECOND_STAGE_MAX_DEPTH = int(EXPERIMENT_CONFIG.get(\"second_stage_max_depth\", 4))\n",
    "SECOND_STAGE_L2 = float(EXPERIMENT_CONFIG.get(\"second_stage_l2\", 0.2))\n",
    "SECOND_STAGE_MIN_DATA_IN_LEAF = int(EXPERIMENT_CONFIG.get(\"second_stage_min_data_in_leaf\", 20))\n",
    "SECOND_STAGE_SUBSAMPLE = float(EXPERIMENT_CONFIG.get(\"second_stage_subsample\", 0.8))\n",
    "SECOND_STAGE_RSM = float(EXPERIMENT_CONFIG.get(\"second_stage_rsm\", 0.8))\n",
    "SECOND_STAGE_OD_WAIT = int(EXPERIMENT_CONFIG.get(\"second_stage_od_wait\", 50))\n",
    "SECOND_STAGE_USE_BALANCED_WEIGHTS = bool(EXPERIMENT_CONFIG.get(\"second_stage_use_balanced_weights\", True))\n",
    "\n",
    "oof_seeds_cfg = EXPERIMENT_CONFIG.get(\n",
    "    \"second_stage_oof_seeds\",\n",
    "    ensemble_seeds[: min(3, len(ensemble_seeds))],\n",
    ")\n",
    "if isinstance(oof_seeds_cfg, (int, np.integer)):\n",
    "    second_stage_oof_seeds = [int(oof_seeds_cfg)]\n",
    "elif isinstance(oof_seeds_cfg, (list, tuple)):\n",
    "    second_stage_oof_seeds = [int(seed) for seed in oof_seeds_cfg]\n",
    "else:\n",
    "    raise ValueError(\"second_stage_oof_seeds must be an int or a list of ints.\")\n",
    "second_stage_oof_seeds = list(dict.fromkeys(second_stage_oof_seeds))\n",
    "if SECOND_STAGE_USE_OOF_FEATURES and not second_stage_oof_seeds:\n",
    "    raise ValueError(\"second_stage_oof_seeds cannot be empty when second_stage_use_oof_features=True.\")\n",
    "\n",
    "has_validation = len(val_idx) > 0\n",
    "fit_author_ids_for_score = train_en_model.iloc[fit_idx][\"author_id\"].to_numpy()\n",
    "val_author_ids_for_score = train_en_model.iloc[val_idx][\"author_id\"].to_numpy() if has_validation else np.array([])\n",
    "test_author_ids_for_score = train_en_model.iloc[test_idx][\"author_id\"].to_numpy()\n",
    "\n",
    "def _predict_post_probs(model, indices):\n",
    "    if len(indices) == 0:\n",
    "        return np.array([], dtype=np.float32)\n",
    "    return model.predict(\n",
    "        {\n",
    "            \"input_ids\": input_ids_en[indices],\n",
    "            \"attention_mask\": attention_mask_en[indices],\n",
    "            \"aux_features\": X_aux_en[indices],\n",
    "        },\n",
    "        verbose=0,\n",
    "    ).ravel()\n",
    "\n",
    "def _build_callbacks():\n",
    "    callbacks = []\n",
    "    if has_validation and EARLY_STOPPING_PATIENCE > 0:\n",
    "        callbacks.append(\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_auc\",\n",
    "                mode=\"max\",\n",
    "                patience=EARLY_STOPPING_PATIENCE,\n",
    "                restore_best_weights=True,\n",
    "            )\n",
    "        )\n",
    "    if has_validation and REDUCE_LR_PATIENCE > 0:\n",
    "        callbacks.append(\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor=\"val_auc\",\n",
    "                mode=\"max\",\n",
    "                factor=REDUCE_LR_FACTOR,\n",
    "                patience=REDUCE_LR_PATIENCE,\n",
    "                min_lr=REDUCE_LR_MIN_LR,\n",
    "            )\n",
    "        )\n",
    "    return callbacks\n",
    "\n",
    "def _build_fold_callbacks():\n",
    "    callbacks = []\n",
    "    if EARLY_STOPPING_PATIENCE > 0:\n",
    "        callbacks.append(\n",
    "            tf.keras.callbacks.EarlyStopping(\n",
    "                monitor=\"val_auc\",\n",
    "                mode=\"max\",\n",
    "                patience=EARLY_STOPPING_PATIENCE,\n",
    "                restore_best_weights=True,\n",
    "            )\n",
    "        )\n",
    "    if REDUCE_LR_PATIENCE > 0:\n",
    "        callbacks.append(\n",
    "            tf.keras.callbacks.ReduceLROnPlateau(\n",
    "                monitor=\"val_auc\",\n",
    "                mode=\"max\",\n",
    "                factor=REDUCE_LR_FACTOR,\n",
    "                patience=REDUCE_LR_PATIENCE,\n",
    "                min_lr=REDUCE_LR_MIN_LR,\n",
    "            )\n",
    "        )\n",
    "    return callbacks\n",
    "\n",
    "def _search_best_threshold(author_ids, labels, probs):\n",
    "    if not (has_validation and USE_THRESHOLD_SEARCH):\n",
    "        return float(PREDICTION_THRESHOLD), None, pd.DataFrame()\n",
    "\n",
    "    rows = []\n",
    "    for threshold in np.linspace(THRESHOLD_SEARCH_MIN, THRESHOLD_SEARCH_MAX, THRESHOLD_SEARCH_STEPS):\n",
    "        score, tp, fn, fp, n_accounts = compute_account_score(\n",
    "            author_ids=author_ids,\n",
    "            true_labels=labels,\n",
    "            pred_probs=probs,\n",
    "            threshold=float(threshold),\n",
    "            decision_rule=ACCOUNT_DECISION_RULE,\n",
    "        )\n",
    "        rows.append(\n",
    "            {\n",
    "                \"threshold\": float(threshold),\n",
    "                \"score\": int(score),\n",
    "                \"tp_accounts\": int(tp),\n",
    "                \"fn_accounts\": int(fn),\n",
    "                \"fp_accounts\": int(fp),\n",
    "                \"n_accounts\": int(n_accounts),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    best_row = df.sort_values(\n",
    "        by=[\"score\", \"fp_accounts\", \"tp_accounts\", \"threshold\"],\n",
    "        ascending=[False, True, False, False],\n",
    "    ).iloc[0]\n",
    "    return float(best_row[\"threshold\"]), int(best_row[\"score\"]), df\n",
    "\n",
    "def _aggregate_probabilities(prob_list, mode):\n",
    "    if not prob_list:\n",
    "        return np.array([], dtype=np.float32)\n",
    "    stack = np.vstack(prob_list)\n",
    "    if mode == \"median\":\n",
    "        return np.median(stack, axis=0).astype(np.float32)\n",
    "    return np.mean(stack, axis=0).astype(np.float32)\n",
    "\n",
    "def _prepare_external_pretrain_inputs():\n",
    "    if not USE_EXTERNAL_PRETRAIN:\n",
    "        return None, None, None\n",
    "\n",
    "    required_external = [\n",
    "        \"external_pretrain_input_ids\",\n",
    "        \"external_pretrain_attention_mask\",\n",
    "        \"external_pretrain_meta_scaled\",\n",
    "        \"external_pretrain_labels\",\n",
    "    ]\n",
    "    missing_external = [name for name in required_external if name not in globals()]\n",
    "    if missing_external:\n",
    "        raise ValueError(\n",
    "            f\"External pretrain enabled, but missing variables: {missing_external}. Run the External data pretraining cell.\"\n",
    "        )\n",
    "\n",
    "    external_meta = external_pretrain_meta_scaled\n",
    "    aux_dim = X_aux_en.shape[1]\n",
    "    meta_dim = external_meta.shape[1]\n",
    "    if aux_dim > meta_dim:\n",
    "        padding = np.zeros((external_meta.shape[0], aux_dim - meta_dim), dtype=np.float32)\n",
    "        external_aux = np.concatenate([external_meta, padding], axis=1)\n",
    "    else:\n",
    "        external_aux = external_meta[:, :aux_dim]\n",
    "\n",
    "    external_inputs = {\n",
    "        \"input_ids\": external_pretrain_input_ids,\n",
    "        \"attention_mask\": external_pretrain_attention_mask,\n",
    "        \"aux_features\": external_aux,\n",
    "    }\n",
    "    external_labels = np.asarray(globals()[\"external_pretrain_labels\"], dtype=np.int64)\n",
    "\n",
    "    external_sample_weight = None\n",
    "    if EXTERNAL_PRETRAIN_USE_BALANCED_WEIGHTS:\n",
    "        external_sample_weight = compute_sample_weight(class_weight=\"balanced\", y=external_labels)\n",
    "\n",
    "    return external_inputs, external_labels, external_sample_weight\n",
    "\n",
    "def _build_author_oof_folds(post_indices, n_splits, seed):\n",
    "    fit_posts = (\n",
    "        train_en_model.iloc[post_indices][[\"author_id\", \"is_bot\"]]\n",
    "        .reset_index()\n",
    "        .rename(columns={\"index\": \"post_index\"})\n",
    "    )\n",
    "    author_df = fit_posts.groupby(\"author_id\", as_index=False).agg(\n",
    "        account_label=(\"is_bot\", \"max\"),\n",
    "        post_index=(\"post_index\", list),\n",
    "    )\n",
    "\n",
    "    if len(author_df) < 2:\n",
    "        return [np.asarray(post_indices, dtype=np.int64)]\n",
    "\n",
    "    label_counts = author_df[\"account_label\"].value_counts()\n",
    "    min_class_count = int(label_counts.min()) if not label_counts.empty else 0\n",
    "    n_splits_eff = min(int(n_splits), len(author_df), min_class_count)\n",
    "\n",
    "    if n_splits_eff < 2:\n",
    "        return [np.asarray(post_indices, dtype=np.int64)]\n",
    "\n",
    "    splitter = StratifiedKFold(n_splits=n_splits_eff, shuffle=True, random_state=int(seed))\n",
    "\n",
    "    folds = []\n",
    "    author_ids = author_df[\"author_id\"].to_numpy()\n",
    "    author_labels = author_df[\"account_label\"].to_numpy(dtype=np.int64)\n",
    "\n",
    "    for _, hold_author_pos in splitter.split(author_ids, author_labels):\n",
    "        hold_lists = author_df.iloc[hold_author_pos][\"post_index\"].tolist()\n",
    "        hold_posts = np.asarray([idx for lst in hold_lists for idx in lst], dtype=np.int64)\n",
    "        folds.append(hold_posts)\n",
    "\n",
    "    return folds\n",
    "\n",
    "def _compute_oof_post_probs_for_seed(seed, post_indices, use_external_pretrain_for_oof):\n",
    "    post_indices = np.asarray(post_indices, dtype=np.int64)\n",
    "    folds = _build_author_oof_folds(post_indices, SECOND_STAGE_OOF_FOLDS, RANDOM_SEED + int(seed))\n",
    "\n",
    "    if len(folds) == 1 and len(folds[0]) == len(post_indices):\n",
    "        print(\n",
    "            \"[OOF] Not enough account diversity for stratified folds; using in-sample fallback for second-stage training features.\"\n",
    "        )\n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.keras.utils.set_random_seed(int(seed))\n",
    "        np.random.seed(int(seed))\n",
    "\n",
    "        fallback_model = build_multifeature_model(\n",
    "            vocab_size=tokenizer.vocab_size,\n",
    "            seq_len=MAX_LENGTH,\n",
    "            aux_dim=X_aux_en.shape[1],\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            gru_units=GRU_UNITS,\n",
    "            aux_dense_units=AUX_DENSE_UNITS,\n",
    "            head_dense_units=HEAD_DENSE_UNITS,\n",
    "            dropout_text=DROPOUT_TEXT,\n",
    "            dropout_aux=DROPOUT_AUX,\n",
    "            dropout_head=DROPOUT_HEAD,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "        )\n",
    "\n",
    "        fit_inputs_local = {\n",
    "            \"input_ids\": input_ids_en[post_indices],\n",
    "            \"attention_mask\": attention_mask_en[post_indices],\n",
    "            \"aux_features\": X_aux_en[post_indices],\n",
    "        }\n",
    "\n",
    "        fit_kwargs_local = {\n",
    "            \"epochs\": SECOND_STAGE_OOF_EPOCHS,\n",
    "            \"batch_size\": SECOND_STAGE_OOF_BATCH_SIZE,\n",
    "            \"class_weight\": class_weight_dict if USE_CLASS_WEIGHTS else None,\n",
    "            \"verbose\": 0,\n",
    "        }\n",
    "\n",
    "        fallback_model.fit(fit_inputs_local, y_en[post_indices], **fit_kwargs_local)\n",
    "        return _predict_post_probs(fallback_model, post_indices)\n",
    "\n",
    "    index_map = np.full(len(y_en), -1, dtype=np.int64)\n",
    "    index_map[post_indices] = np.arange(len(post_indices), dtype=np.int64)\n",
    "    oof_local = np.full(len(post_indices), np.nan, dtype=np.float32)\n",
    "\n",
    "    for fold_id, hold_posts in enumerate(folds, start=1):\n",
    "        train_posts = post_indices[~np.isin(post_indices, hold_posts)]\n",
    "        if len(train_posts) == 0 or len(hold_posts) == 0:\n",
    "            continue\n",
    "\n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.keras.utils.set_random_seed(int(seed) * 100 + fold_id)\n",
    "        np.random.seed(int(seed) * 100 + fold_id)\n",
    "\n",
    "        fold_model = build_multifeature_model(\n",
    "            vocab_size=tokenizer.vocab_size,\n",
    "            seq_len=MAX_LENGTH,\n",
    "            aux_dim=X_aux_en.shape[1],\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            gru_units=GRU_UNITS,\n",
    "            aux_dense_units=AUX_DENSE_UNITS,\n",
    "            head_dense_units=HEAD_DENSE_UNITS,\n",
    "            dropout_text=DROPOUT_TEXT,\n",
    "            dropout_aux=DROPOUT_AUX,\n",
    "            dropout_head=DROPOUT_HEAD,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "        )\n",
    "\n",
    "        if use_external_pretrain_for_oof and external_pretrain_inputs is not None:\n",
    "            pretrain_kwargs = {\n",
    "                \"epochs\": EXTERNAL_PRETRAIN_EPOCHS,\n",
    "                \"batch_size\": EXTERNAL_PRETRAIN_BATCH_SIZE,\n",
    "                \"shuffle\": True,\n",
    "                \"verbose\": 0,\n",
    "            }\n",
    "            if external_pretrain_sample_weight is not None:\n",
    "                pretrain_kwargs[\"sample_weight\"] = external_pretrain_sample_weight\n",
    "            fold_model.fit(external_pretrain_inputs, external_pretrain_labels_arr, **pretrain_kwargs)\n",
    "\n",
    "        train_inputs_fold = {\n",
    "            \"input_ids\": input_ids_en[train_posts],\n",
    "            \"attention_mask\": attention_mask_en[train_posts],\n",
    "            \"aux_features\": X_aux_en[train_posts],\n",
    "        }\n",
    "        hold_inputs_fold = {\n",
    "            \"input_ids\": input_ids_en[hold_posts],\n",
    "            \"attention_mask\": attention_mask_en[hold_posts],\n",
    "            \"aux_features\": X_aux_en[hold_posts],\n",
    "        }\n",
    "\n",
    "        fit_kwargs_fold = {\n",
    "            \"epochs\": SECOND_STAGE_OOF_EPOCHS,\n",
    "            \"batch_size\": SECOND_STAGE_OOF_BATCH_SIZE,\n",
    "            \"class_weight\": class_weight_dict if USE_CLASS_WEIGHTS else None,\n",
    "            \"callbacks\": _build_fold_callbacks(),\n",
    "            \"validation_data\": (hold_inputs_fold, y_en[hold_posts]),\n",
    "            \"verbose\": 0,\n",
    "        }\n",
    "\n",
    "        fold_model.fit(train_inputs_fold, y_en[train_posts], **fit_kwargs_fold)\n",
    "        hold_probs = fold_model.predict(hold_inputs_fold, verbose=0).ravel()\n",
    "\n",
    "        hold_local = index_map[hold_posts]\n",
    "        valid_mask = hold_local >= 0\n",
    "        oof_local[hold_local[valid_mask]] = hold_probs[valid_mask]\n",
    "\n",
    "    missing_mask = np.isnan(oof_local)\n",
    "    if missing_mask.any():\n",
    "        print(f\"[OOF] Filling {int(missing_mask.sum())} missing predictions with in-sample fallback for seed {seed}.\")\n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.keras.utils.set_random_seed(int(seed) + 999)\n",
    "        np.random.seed(int(seed) + 999)\n",
    "\n",
    "        fallback_model = build_multifeature_model(\n",
    "            vocab_size=tokenizer.vocab_size,\n",
    "            seq_len=MAX_LENGTH,\n",
    "            aux_dim=X_aux_en.shape[1],\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "            gru_units=GRU_UNITS,\n",
    "            aux_dense_units=AUX_DENSE_UNITS,\n",
    "            head_dense_units=HEAD_DENSE_UNITS,\n",
    "            dropout_text=DROPOUT_TEXT,\n",
    "            dropout_aux=DROPOUT_AUX,\n",
    "            dropout_head=DROPOUT_HEAD,\n",
    "            learning_rate=LEARNING_RATE,\n",
    "        )\n",
    "\n",
    "        fit_inputs_local = {\n",
    "            \"input_ids\": input_ids_en[post_indices],\n",
    "            \"attention_mask\": attention_mask_en[post_indices],\n",
    "            \"aux_features\": X_aux_en[post_indices],\n",
    "        }\n",
    "\n",
    "        fit_kwargs_local = {\n",
    "            \"epochs\": SECOND_STAGE_OOF_EPOCHS,\n",
    "            \"batch_size\": SECOND_STAGE_OOF_BATCH_SIZE,\n",
    "            \"class_weight\": class_weight_dict if USE_CLASS_WEIGHTS else None,\n",
    "            \"verbose\": 0,\n",
    "        }\n",
    "        fallback_model.fit(fit_inputs_local, y_en[post_indices], **fit_kwargs_local)\n",
    "        fallback_probs = fallback_model.predict(fit_inputs_local, verbose=0).ravel()\n",
    "        oof_local[missing_mask] = fallback_probs[missing_mask]\n",
    "\n",
    "    return oof_local.astype(np.float32)\n",
    "\n",
    "fit_inputs = {\n",
    "    \"input_ids\": input_ids_en[fit_idx],\n",
    "    \"attention_mask\": attention_mask_en[fit_idx],\n",
    "    \"aux_features\": X_aux_en[fit_idx],\n",
    "}\n",
    "val_inputs = (\n",
    "    {\n",
    "        \"input_ids\": input_ids_en[val_idx],\n",
    "        \"attention_mask\": attention_mask_en[val_idx],\n",
    "        \"aux_features\": X_aux_en[val_idx],\n",
    "    }\n",
    "    if has_validation\n",
    "    else None\n",
    ")\n",
    "\n",
    "post_prob_fit_list = []\n",
    "post_prob_val_list = []\n",
    "post_prob_test_list = []\n",
    "seed_rows = []\n",
    "\n",
    "if hasattr(tf.config.experimental, \"enable_op_determinism\"):\n",
    "    try:\n",
    "        tf.config.experimental.enable_op_determinism()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "external_pretrain_inputs = None\n",
    "external_pretrain_labels_arr = None\n",
    "external_pretrain_sample_weight = None\n",
    "if USE_EXTERNAL_PRETRAIN:\n",
    "    external_pretrain_inputs, external_pretrain_labels_arr, external_pretrain_sample_weight = _prepare_external_pretrain_inputs()\n",
    "\n",
    "for seed in ensemble_seeds:\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.keras.utils.set_random_seed(int(seed))\n",
    "    np.random.seed(int(seed))\n",
    "\n",
    "    model_seed = build_multifeature_model(\n",
    "        vocab_size=tokenizer.vocab_size,\n",
    "        seq_len=MAX_LENGTH,\n",
    "        aux_dim=X_aux_en.shape[1],\n",
    "        embedding_dim=EMBEDDING_DIM,\n",
    "        gru_units=GRU_UNITS,\n",
    "        aux_dense_units=AUX_DENSE_UNITS,\n",
    "        head_dense_units=HEAD_DENSE_UNITS,\n",
    "        dropout_text=DROPOUT_TEXT,\n",
    "        dropout_aux=DROPOUT_AUX,\n",
    "        dropout_head=DROPOUT_HEAD,\n",
    "        learning_rate=LEARNING_RATE,\n",
    "    )\n",
    "\n",
    "    if USE_EXTERNAL_PRETRAIN:\n",
    "        pretrain_kwargs = {\n",
    "            \"epochs\": EXTERNAL_PRETRAIN_EPOCHS,\n",
    "            \"batch_size\": EXTERNAL_PRETRAIN_BATCH_SIZE,\n",
    "            \"shuffle\": True,\n",
    "            \"verbose\": 0,\n",
    "        }\n",
    "        if external_pretrain_sample_weight is not None:\n",
    "            pretrain_kwargs[\"sample_weight\"] = external_pretrain_sample_weight\n",
    "        model_seed.fit(external_pretrain_inputs, external_pretrain_labels_arr, **pretrain_kwargs)\n",
    "\n",
    "    fit_kwargs = {\n",
    "        \"epochs\": EPOCHS,\n",
    "        \"batch_size\": BATCH_SIZE,\n",
    "        \"class_weight\": class_weight_dict if USE_CLASS_WEIGHTS else None,\n",
    "        \"callbacks\": _build_callbacks(),\n",
    "        \"verbose\": 0,\n",
    "    }\n",
    "    if has_validation:\n",
    "        fit_kwargs[\"validation_data\"] = (val_inputs, y_en[val_idx])\n",
    "\n",
    "    model_seed.fit(fit_inputs, y_en[fit_idx], **fit_kwargs)\n",
    "\n",
    "    fit_probs = _predict_post_probs(model_seed, fit_idx)\n",
    "    val_probs = _predict_post_probs(model_seed, val_idx)\n",
    "    test_probs = _predict_post_probs(model_seed, test_idx)\n",
    "\n",
    "    threshold_seed, val_score_seed, _ = _search_best_threshold(val_author_ids_for_score, y_en[val_idx], val_probs)\n",
    "\n",
    "    test_score_seed, tp_seed, fn_seed, fp_seed, n_accounts_seed = compute_account_score(\n",
    "        author_ids=test_author_ids_for_score,\n",
    "        true_labels=y_en[test_idx],\n",
    "        pred_probs=test_probs,\n",
    "        threshold=threshold_seed,\n",
    "        decision_rule=ACCOUNT_DECISION_RULE,\n",
    "    )\n",
    "\n",
    "    seed_rows.append(\n",
    "        {\n",
    "            \"seed\": int(seed),\n",
    "            \"threshold\": float(threshold_seed),\n",
    "            \"val_best_score\": val_score_seed,\n",
    "            \"test_score\": int(test_score_seed),\n",
    "            \"tp_accounts\": int(tp_seed),\n",
    "            \"fn_accounts\": int(fn_seed),\n",
    "            \"fp_accounts\": int(fp_seed),\n",
    "            \"n_accounts\": int(n_accounts_seed),\n",
    "        }\n",
    "    )\n",
    "\n",
    "    post_prob_fit_list.append(fit_probs)\n",
    "    post_prob_val_list.append(val_probs)\n",
    "    post_prob_test_list.append(test_probs)\n",
    "\n",
    "seed_report_df = pd.DataFrame(seed_rows)\n",
    "ensemble_seed_report_en = seed_report_df.copy()\n",
    "\n",
    "post_prob_fit_ensemble_en = _aggregate_probabilities(post_prob_fit_list, ENSEMBLE_AGGREGATION)\n",
    "post_prob_val_ensemble_en = _aggregate_probabilities(post_prob_val_list, ENSEMBLE_AGGREGATION)\n",
    "post_prob_test_ensemble_en = _aggregate_probabilities(post_prob_test_list, ENSEMBLE_AGGREGATION)\n",
    "\n",
    "selected_threshold_ensemble, best_ensemble_val_score, threshold_search_results_ensemble_en = _search_best_threshold(\n",
    "    val_author_ids_for_score,\n",
    "    y_en[val_idx],\n",
    "    post_prob_val_ensemble_en,\n",
    ")\n",
    "\n",
    "(\n",
    "    test_score_ensemble_en,\n",
    "    ensemble_tp_accounts_en,\n",
    "    ensemble_fn_accounts_en,\n",
    "    ensemble_fp_accounts_en,\n",
    "    ensemble_n_accounts_en,\n",
    ") = compute_account_score(\n",
    "    author_ids=test_author_ids_for_score,\n",
    "    true_labels=y_en[test_idx],\n",
    "    pred_probs=post_prob_test_ensemble_en,\n",
    "    threshold=selected_threshold_ensemble,\n",
    "    decision_rule=ACCOUNT_DECISION_RULE,\n",
    ")\n",
    "\n",
    "# Make the ensemble outputs the default baseline for downstream score/plot cells.\n",
    "post_prob_fit = post_prob_fit_ensemble_en\n",
    "post_prob_val = post_prob_val_ensemble_en\n",
    "post_prob_test = post_prob_test_ensemble_en\n",
    "y_prob = post_prob_test_ensemble_en\n",
    "SELECTED_THRESHOLD = float(selected_threshold_ensemble)\n",
    "test_score = int(test_score_ensemble_en)\n",
    "test_tp_accounts = int(ensemble_tp_accounts_en)\n",
    "test_fn_accounts = int(ensemble_fn_accounts_en)\n",
    "test_fp_accounts = int(ensemble_fp_accounts_en)\n",
    "test_n_accounts = int(ensemble_n_accounts_en)\n",
    "\n",
    "seed_mean_score_en = float(seed_report_df[\"test_score\"].mean())\n",
    "seed_std_score_en = float(seed_report_df[\"test_score\"].std(ddof=1)) if len(seed_report_df) > 1 else 0.0\n",
    "\n",
    "def build_account_feature_table(post_indices, post_probs, bot_threshold):\n",
    "    posts = train_en_model.iloc[post_indices].copy()\n",
    "    posts[\"post_prob\"] = np.asarray(post_probs, dtype=np.float32)\n",
    "\n",
    "    posts[\"has_url_post\"] = (posts[\"url_count\"] > 0).astype(np.float32)\n",
    "    posts[\"has_mention_post\"] = (posts[\"mention_count\"] > 0).astype(np.float32)\n",
    "    posts[\"has_hashtag_post\"] = (posts[\"hashtag_count\"] > 0).astype(np.float32)\n",
    "    posts[\"pred_bot_post\"] = (posts[\"post_prob\"] >= bot_threshold).astype(np.float32)\n",
    "\n",
    "    agg_spec = {\n",
    "        \"is_bot\": [\"max\"],\n",
    "        \"text_clean\": [\"size\"],\n",
    "        \"post_prob\": [\"mean\", \"std\", \"max\", \"min\"],\n",
    "        \"char_count\": [\"mean\", \"std\", \"max\"],\n",
    "        \"word_count\": [\"mean\", \"std\", \"max\"],\n",
    "        \"url_count\": [\"mean\", \"max\"],\n",
    "        \"mention_count\": [\"mean\", \"max\"],\n",
    "        \"hashtag_count\": [\"mean\", \"max\"],\n",
    "        \"exclamation_count\": [\"mean\", \"max\"],\n",
    "        \"question_count\": [\"mean\", \"max\"],\n",
    "        \"has_url_post\": [\"mean\"],\n",
    "        \"has_mention_post\": [\"mean\"],\n",
    "        \"has_hashtag_post\": [\"mean\"],\n",
    "        \"pred_bot_post\": [\"mean\", \"sum\"],\n",
    "    }\n",
    "\n",
    "    for topic_col in topic_feature_cols_en:\n",
    "        agg_spec[topic_col] = [\"mean\"]\n",
    "\n",
    "    account = posts.groupby(\"author_id\", as_index=False).agg(agg_spec)\n",
    "    account.columns = [\n",
    "        \"author_id\" if col == (\"author_id\", \"\") else f\"{col[0]}_{col[1]}\"\n",
    "        for col in account.columns.to_flat_index()\n",
    "    ]\n",
    "\n",
    "    account = account.rename(columns={\"is_bot_max\": \"true_is_bot\", \"text_clean_size\": \"n_posts\"})\n",
    "    for col in [\"post_prob_std\", \"char_count_std\", \"word_count_std\"]:\n",
    "        if col in account.columns:\n",
    "            account[col] = account[col].fillna(0.0)\n",
    "    account[\"n_posts_log1p\"] = np.log1p(account[\"n_posts\"].astype(np.float32))\n",
    "\n",
    "    user_source = (\n",
    "        users_en_labeled.copy()\n",
    "        if \"users_en_labeled\" in globals()\n",
    "        else users_en.drop_duplicates(subset=\"id\", keep=\"last\").copy()\n",
    "    )\n",
    "    user_source[\"username_len\"] = user_source[\"username\"].fillna(\"\").str.len()\n",
    "    user_source[\"name_len\"] = user_source[\"name\"].fillna(\"\").str.len()\n",
    "    user_source[\"description_len\"] = user_source[\"description\"].fillna(\"\").str.len()\n",
    "    user_source[\"has_location\"] = user_source[\"location\"].fillna(\"\").str.strip().ne(\"\").astype(np.float32)\n",
    "\n",
    "    user_features = user_source[\n",
    "        [\n",
    "            \"id\",\n",
    "            \"tweet_count\",\n",
    "            \"z_score\",\n",
    "            \"username_len\",\n",
    "            \"name_len\",\n",
    "            \"description_len\",\n",
    "            \"has_location\",\n",
    "        ]\n",
    "    ].copy()\n",
    "\n",
    "    account = account.merge(user_features, left_on=\"author_id\", right_on=\"id\", how=\"left\").drop(columns=[\"id\"])\n",
    "\n",
    "    numeric_cols = [col for col in account.columns if col != \"author_id\"]\n",
    "    account[numeric_cols] = account[numeric_cols].apply(pd.to_numeric, errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "    if \"pred_bot_post_sum\" in account.columns:\n",
    "        account[\"pred_bot_post_count\"] = account[\"pred_bot_post_sum\"].astype(np.float32)\n",
    "    else:\n",
    "        account[\"pred_bot_post_count\"] = (\n",
    "            np.round(account[\"pred_bot_post_mean\"].to_numpy(dtype=np.float32) * account[\"n_posts\"].to_numpy(dtype=np.float32))\n",
    "        ).astype(np.float32)\n",
    "    account[\"pred_bot_post_rate\"] = account[\"pred_bot_post_mean\"].astype(np.float32)\n",
    "    return account\n",
    "\n",
    "def score_from_account_probs(\n",
    "    true_labels,\n",
    "    pred_probs,\n",
    "    threshold,\n",
    "    min_bot_posts=1,\n",
    "    bot_post_counts=None,\n",
    "    n_posts=None,\n",
    "    min_bot_post_rate=0.0,\n",
    "):\n",
    "    pred_labels = (np.asarray(pred_probs, dtype=np.float32) >= float(threshold)).astype(np.int64)\n",
    "\n",
    "    if bot_post_counts is not None:\n",
    "        bot_post_counts_arr = np.asarray(bot_post_counts, dtype=np.float32)\n",
    "        pred_labels = pred_labels & (bot_post_counts_arr >= float(min_bot_posts))\n",
    "    else:\n",
    "        bot_post_counts_arr = None\n",
    "\n",
    "    if n_posts is not None and float(min_bot_post_rate) > 0.0:\n",
    "        if bot_post_counts_arr is None:\n",
    "            bot_post_counts_arr = np.zeros_like(pred_labels, dtype=np.float32)\n",
    "        n_posts_arr = np.maximum(np.asarray(n_posts, dtype=np.float32), 1.0)\n",
    "        bot_rates = bot_post_counts_arr / n_posts_arr\n",
    "        pred_labels = pred_labels & (bot_rates >= float(min_bot_post_rate))\n",
    "\n",
    "    pred_labels = pred_labels.astype(np.int64)\n",
    "    tp = int(((true_labels == 1) & (pred_labels == 1)).sum())\n",
    "    fn = int(((true_labels == 1) & (pred_labels == 0)).sum())\n",
    "    fp = int(((true_labels == 0) & (pred_labels == 1)).sum())\n",
    "    score = (4 * tp) - (1 * fn) - (2 * fp)\n",
    "    return score, tp, fn, fp, pred_labels\n",
    "\n",
    "\n",
    "def _search_threshold_for_account_probs(y_true, y_prob, bot_post_counts=None, n_posts=None):\n",
    "    if len(y_true) == 0:\n",
    "        return (\n",
    "            float(PREDICTION_THRESHOLD),\n",
    "            int(SECOND_STAGE_ACCOUNT_MIN_BOT_POSTS_DEFAULT),\n",
    "            float(SECOND_STAGE_ACCOUNT_MIN_BOT_POST_RATE_DEFAULT),\n",
    "            None,\n",
    "            0,\n",
    "            0,\n",
    "            0,\n",
    "            pd.DataFrame(),\n",
    "        )\n",
    "\n",
    "    if USE_THRESHOLD_SEARCH:\n",
    "        threshold_grid = np.linspace(THRESHOLD_SEARCH_MIN, THRESHOLD_SEARCH_MAX, THRESHOLD_SEARCH_STEPS)\n",
    "    else:\n",
    "        threshold_grid = np.array([float(PREDICTION_THRESHOLD)], dtype=np.float32)\n",
    "\n",
    "    if SECOND_STAGE_POST_CALIBRATE:\n",
    "        min_posts_grid = SECOND_STAGE_ACCOUNT_MIN_BOT_POSTS_GRID\n",
    "        min_rate_grid = SECOND_STAGE_ACCOUNT_MIN_BOT_POST_RATE_GRID\n",
    "    else:\n",
    "        min_posts_grid = [int(SECOND_STAGE_ACCOUNT_MIN_BOT_POSTS_DEFAULT)]\n",
    "        min_rate_grid = [float(SECOND_STAGE_ACCOUNT_MIN_BOT_POST_RATE_DEFAULT)]\n",
    "\n",
    "    rows = []\n",
    "    for threshold in threshold_grid:\n",
    "        for min_posts in min_posts_grid:\n",
    "            for min_rate in min_rate_grid:\n",
    "                score, tp, fn, fp, _ = score_from_account_probs(\n",
    "                    true_labels=y_true,\n",
    "                    pred_probs=y_prob,\n",
    "                    threshold=float(threshold),\n",
    "                    min_bot_posts=int(min_posts),\n",
    "                    bot_post_counts=bot_post_counts,\n",
    "                    n_posts=n_posts,\n",
    "                    min_bot_post_rate=float(min_rate),\n",
    "                )\n",
    "                rows.append(\n",
    "                    {\n",
    "                        \"threshold\": float(threshold),\n",
    "                        \"min_bot_posts\": int(min_posts),\n",
    "                        \"min_bot_post_rate\": float(min_rate),\n",
    "                        \"score\": int(score),\n",
    "                        \"tp_accounts\": int(tp),\n",
    "                        \"fn_accounts\": int(fn),\n",
    "                        \"fp_accounts\": int(fp),\n",
    "                    }\n",
    "                )\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    best_row = df.sort_values(\n",
    "        by=[\"score\", \"fp_accounts\", \"tp_accounts\", \"threshold\", \"min_bot_posts\", \"min_bot_post_rate\"],\n",
    "        ascending=[False, True, False, False, False, False],\n",
    "    ).iloc[0]\n",
    "\n",
    "    return (\n",
    "        float(best_row[\"threshold\"]),\n",
    "        int(best_row[\"min_bot_posts\"]),\n",
    "        float(best_row[\"min_bot_post_rate\"]),\n",
    "        int(best_row[\"score\"]),\n",
    "        int(best_row[\"tp_accounts\"]),\n",
    "        int(best_row[\"fn_accounts\"]),\n",
    "        int(best_row[\"fp_accounts\"]),\n",
    "        df,\n",
    "    )\n",
    "\n",
    "def _make_second_stage_candidates():\n",
    "    candidates = {}\n",
    "\n",
    "    legacy_params = {\n",
    "        \"iterations\": 300,\n",
    "        \"learning_rate\": 0.05,\n",
    "        \"depth\": 4,\n",
    "        \"l2_leaf_reg\": 0.2,\n",
    "        \"loss_function\": \"Logloss\",\n",
    "        \"eval_metric\": \"AUC\",\n",
    "        \"random_seed\": RANDOM_SEED,\n",
    "        \"verbose\": False,\n",
    "    }\n",
    "\n",
    "    regularized_params = {\n",
    "        \"iterations\": SECOND_STAGE_MAX_ITER,\n",
    "        \"learning_rate\": SECOND_STAGE_LEARNING_RATE,\n",
    "        \"depth\": SECOND_STAGE_MAX_DEPTH,\n",
    "        \"l2_leaf_reg\": SECOND_STAGE_L2,\n",
    "        \"min_data_in_leaf\": SECOND_STAGE_MIN_DATA_IN_LEAF,\n",
    "        \"loss_function\": \"Logloss\",\n",
    "        \"eval_metric\": \"AUC\",\n",
    "        \"random_seed\": RANDOM_SEED,\n",
    "        \"verbose\": False,\n",
    "        \"od_type\": \"Iter\",\n",
    "        \"od_wait\": SECOND_STAGE_OD_WAIT,\n",
    "    }\n",
    "    if 0.0 < SECOND_STAGE_SUBSAMPLE < 1.0:\n",
    "        regularized_params[\"subsample\"] = SECOND_STAGE_SUBSAMPLE\n",
    "        regularized_params[\"bootstrap_type\"] = \"Bernoulli\"\n",
    "    if 0.0 < SECOND_STAGE_RSM <= 1.0:\n",
    "        regularized_params[\"rsm\"] = SECOND_STAGE_RSM\n",
    "\n",
    "    custom_params = {\n",
    "        \"iterations\": SECOND_STAGE_MAX_ITER,\n",
    "        \"learning_rate\": SECOND_STAGE_LEARNING_RATE,\n",
    "        \"depth\": SECOND_STAGE_MAX_DEPTH,\n",
    "        \"l2_leaf_reg\": SECOND_STAGE_L2,\n",
    "        \"min_data_in_leaf\": SECOND_STAGE_MIN_DATA_IN_LEAF,\n",
    "        \"loss_function\": \"Logloss\",\n",
    "        \"eval_metric\": \"AUC\",\n",
    "        \"random_seed\": RANDOM_SEED,\n",
    "        \"verbose\": False,\n",
    "        \"od_type\": \"Iter\",\n",
    "        \"od_wait\": SECOND_STAGE_OD_WAIT,\n",
    "    }\n",
    "    if 0.0 < SECOND_STAGE_SUBSAMPLE < 1.0:\n",
    "        custom_params[\"subsample\"] = SECOND_STAGE_SUBSAMPLE\n",
    "        custom_params[\"bootstrap_type\"] = \"Bernoulli\"\n",
    "    if 0.0 < SECOND_STAGE_RSM <= 1.0:\n",
    "        custom_params[\"rsm\"] = SECOND_STAGE_RSM\n",
    "\n",
    "    if SECOND_STAGE_PROFILE in {\"auto\", \"legacy\"}:\n",
    "        candidates[\"legacy\"] = legacy_params\n",
    "    if SECOND_STAGE_PROFILE in {\"auto\", \"regularized\"}:\n",
    "        candidates[\"regularized\"] = regularized_params\n",
    "    if SECOND_STAGE_PROFILE == \"custom\":\n",
    "        candidates[\"custom\"] = custom_params\n",
    "\n",
    "    if SECOND_STAGE_USE_BALANCED_WEIGHTS:\n",
    "        for params in candidates.values():\n",
    "            params[\"auto_class_weights\"] = \"Balanced\"\n",
    "\n",
    "    return candidates\n",
    "\n",
    "if USE_SECOND_STAGE_ACCOUNT_MODEL:\n",
    "    post_prob_fit_for_second_stage_en = post_prob_fit_ensemble_en.copy()\n",
    "    second_stage_fit_feature_source_en = \"in_sample_ensemble\"\n",
    "    second_stage_oof_report_en = pd.DataFrame()\n",
    "\n",
    "    if SECOND_STAGE_USE_OOF_FEATURES:\n",
    "        print(\n",
    "            f\"Building OOF first-stage features for second-stage training (seeds={second_stage_oof_seeds}, folds={SECOND_STAGE_OOF_FOLDS}, epochs={SECOND_STAGE_OOF_EPOCHS})...\"\n",
    "        )\n",
    "        oof_fit_prob_list = []\n",
    "        oof_rows = []\n",
    "\n",
    "        for oof_seed in second_stage_oof_seeds:\n",
    "            oof_probs_seed = _compute_oof_post_probs_for_seed(\n",
    "                seed=int(oof_seed),\n",
    "                post_indices=fit_idx,\n",
    "                use_external_pretrain_for_oof=bool(USE_EXTERNAL_PRETRAIN and SECOND_STAGE_OOF_USE_EXTERNAL_PRETRAIN),\n",
    "            )\n",
    "\n",
    "            oof_score_seed, oof_tp, oof_fn, oof_fp, oof_accounts = compute_account_score(\n",
    "                author_ids=fit_author_ids_for_score,\n",
    "                true_labels=y_en[fit_idx],\n",
    "                pred_probs=oof_probs_seed,\n",
    "                threshold=selected_threshold_ensemble,\n",
    "                decision_rule=ACCOUNT_DECISION_RULE,\n",
    "            )\n",
    "\n",
    "            oof_rows.append(\n",
    "                {\n",
    "                    \"seed\": int(oof_seed),\n",
    "                    \"fit_account_score_at_ensemble_threshold\": int(oof_score_seed),\n",
    "                    \"tp_accounts\": int(oof_tp),\n",
    "                    \"fn_accounts\": int(oof_fn),\n",
    "                    \"fp_accounts\": int(oof_fp),\n",
    "                    \"n_accounts\": int(oof_accounts),\n",
    "                }\n",
    "            )\n",
    "\n",
    "            oof_fit_prob_list.append(oof_probs_seed)\n",
    "\n",
    "        post_prob_fit_for_second_stage_en = _aggregate_probabilities(oof_fit_prob_list, ENSEMBLE_AGGREGATION)\n",
    "        second_stage_fit_feature_source_en = \"oof_first_stage\"\n",
    "        second_stage_oof_report_en = pd.DataFrame(oof_rows)\n",
    "\n",
    "    fit_account_df = build_account_feature_table(fit_idx, post_prob_fit_for_second_stage_en, selected_threshold_ensemble)\n",
    "    val_account_df = (\n",
    "        build_account_feature_table(val_idx, post_prob_val_ensemble_en, selected_threshold_ensemble)\n",
    "        if has_validation\n",
    "        else pd.DataFrame()\n",
    "    )\n",
    "    test_account_df = build_account_feature_table(test_idx, post_prob_test_ensemble_en, selected_threshold_ensemble)\n",
    "\n",
    "    target_col = \"true_is_bot\"\n",
    "    feature_cols_account = [col for col in fit_account_df.columns if col not in {\"author_id\", target_col}]\n",
    "\n",
    "    for df in [fit_account_df, val_account_df, test_account_df]:\n",
    "        if df.empty:\n",
    "            continue\n",
    "        missing_cols = [col for col in feature_cols_account if col not in df.columns]\n",
    "        for col in missing_cols:\n",
    "            df[col] = 0.0\n",
    "\n",
    "    X_fit_acc = fit_account_df[feature_cols_account].to_numpy(dtype=np.float32)\n",
    "    y_fit_acc = fit_account_df[target_col].to_numpy(dtype=np.int64)\n",
    "    X_test_acc = test_account_df[feature_cols_account].to_numpy(dtype=np.float32)\n",
    "    y_test_acc = test_account_df[target_col].to_numpy(dtype=np.int64)\n",
    "\n",
    "    if len(val_account_df):\n",
    "        X_val_acc = val_account_df[feature_cols_account].to_numpy(dtype=np.float32)\n",
    "        y_val_acc = val_account_df[target_col].to_numpy(dtype=np.int64)\n",
    "    else:\n",
    "        X_val_acc = np.zeros((0, len(feature_cols_account)), dtype=np.float32)\n",
    "        y_val_acc = np.zeros((0,), dtype=np.int64)\n",
    "\n",
    "    baseline_account_val_score = None\n",
    "    baseline_account_val_threshold = float(selected_threshold_ensemble)\n",
    "    baseline_account_val_min_posts = int(SECOND_STAGE_ACCOUNT_MIN_BOT_POSTS_DEFAULT)\n",
    "    baseline_account_val_min_rate = float(SECOND_STAGE_ACCOUNT_MIN_BOT_POST_RATE_DEFAULT)\n",
    "    baseline_account_val_tp = 0\n",
    "    baseline_account_val_fn = 0\n",
    "    baseline_account_val_fp = 0\n",
    "\n",
    "    base_val_prob_acc = val_account_df[\"post_prob_mean\"].to_numpy(dtype=np.float32) if len(y_val_acc) else np.array([])\n",
    "    base_test_prob_acc = test_account_df[\"post_prob_mean\"].to_numpy(dtype=np.float32)\n",
    "    val_bot_post_count_acc = val_account_df[\"pred_bot_post_count\"].to_numpy(dtype=np.float32) if len(y_val_acc) else np.array([])\n",
    "    val_n_posts_acc = val_account_df[\"n_posts\"].to_numpy(dtype=np.float32) if len(y_val_acc) else np.array([])\n",
    "    test_bot_post_count_acc = test_account_df[\"pred_bot_post_count\"].to_numpy(dtype=np.float32)\n",
    "    test_n_posts_acc = test_account_df[\"n_posts\"].to_numpy(dtype=np.float32)\n",
    "\n",
    "    if len(y_val_acc):\n",
    "        (\n",
    "            baseline_account_val_threshold,\n",
    "            baseline_account_val_min_posts,\n",
    "            baseline_account_val_min_rate,\n",
    "            baseline_account_val_score,\n",
    "            baseline_account_val_tp,\n",
    "            baseline_account_val_fn,\n",
    "            baseline_account_val_fp,\n",
    "            baseline_account_threshold_search_en,\n",
    "        ) = _search_threshold_for_account_probs(\n",
    "            y_true=y_val_acc,\n",
    "            y_prob=base_val_prob_acc,\n",
    "            bot_post_counts=val_bot_post_count_acc,\n",
    "            n_posts=val_n_posts_acc,\n",
    "        )\n",
    "    else:\n",
    "        baseline_account_threshold_search_en = pd.DataFrame()\n",
    "\n",
    "    candidate_params = _make_second_stage_candidates()\n",
    "    candidate_rows = []\n",
    "    candidate_artifacts = []\n",
    "\n",
    "    for profile_name, params in candidate_params.items():\n",
    "        model = CatBoostClassifier(**params)\n",
    "\n",
    "        fit_kwargs = {}\n",
    "        if len(y_val_acc):\n",
    "            fit_kwargs[\"eval_set\"] = (X_val_acc, y_val_acc)\n",
    "            fit_kwargs[\"use_best_model\"] = True\n",
    "\n",
    "        model.fit(X_fit_acc, y_fit_acc, **fit_kwargs)\n",
    "\n",
    "        test_raw_prob = model.predict_proba(X_test_acc)[:, 1]\n",
    "        val_raw_prob = model.predict_proba(X_val_acc)[:, 1] if len(y_val_acc) else np.array([])\n",
    "\n",
    "        if len(y_val_acc):\n",
    "            best_candidate = None\n",
    "            for alpha in (SECOND_STAGE_BLEND_ALPHAS if SECOND_STAGE_USE_BLEND else [1.0]):\n",
    "                val_candidate_prob = (alpha * val_raw_prob) + ((1.0 - alpha) * base_val_prob_acc)\n",
    "                (\n",
    "                    cand_threshold,\n",
    "                    cand_min_posts,\n",
    "                    cand_min_rate,\n",
    "                    cand_val_score,\n",
    "                    cand_val_tp,\n",
    "                    cand_val_fn,\n",
    "                    cand_val_fp,\n",
    "                    cand_search_df,\n",
    "                ) = _search_threshold_for_account_probs(\n",
    "                    y_true=y_val_acc,\n",
    "                    y_prob=val_candidate_prob,\n",
    "                    bot_post_counts=val_bot_post_count_acc,\n",
    "                    n_posts=val_n_posts_acc,\n",
    "                )\n",
    "\n",
    "                if (\n",
    "                    best_candidate is None\n",
    "                    or cand_val_score > best_candidate[\"val_score\"]\n",
    "                    or (\n",
    "                        cand_val_score == best_candidate[\"val_score\"]\n",
    "                        and cand_val_fp < best_candidate[\"val_fp_accounts\"]\n",
    "                    )\n",
    "                    or (\n",
    "                        cand_val_score == best_candidate[\"val_score\"]\n",
    "                        and cand_val_fp == best_candidate[\"val_fp_accounts\"]\n",
    "                        and cand_val_tp > best_candidate[\"val_tp_accounts\"]\n",
    "                    )\n",
    "                ):\n",
    "                    best_candidate = {\n",
    "                        \"alpha\": float(alpha),\n",
    "                        \"threshold\": float(cand_threshold),\n",
    "                        \"min_bot_posts\": int(cand_min_posts),\n",
    "                        \"min_bot_post_rate\": float(cand_min_rate),\n",
    "                        \"val_score\": int(cand_val_score),\n",
    "                        \"val_tp_accounts\": int(cand_val_tp),\n",
    "                        \"val_fn_accounts\": int(cand_val_fn),\n",
    "                        \"val_fp_accounts\": int(cand_val_fp),\n",
    "                        \"search_df\": cand_search_df,\n",
    "                    }\n",
    "\n",
    "            chosen_alpha = best_candidate[\"alpha\"]\n",
    "            chosen_threshold = best_candidate[\"threshold\"]\n",
    "            chosen_min_posts = int(best_candidate[\"min_bot_posts\"])\n",
    "            chosen_min_rate = float(best_candidate[\"min_bot_post_rate\"])\n",
    "            chosen_val_score = best_candidate[\"val_score\"]\n",
    "            chosen_val_tp = best_candidate[\"val_tp_accounts\"]\n",
    "            chosen_val_fn = best_candidate[\"val_fn_accounts\"]\n",
    "            chosen_val_fp = best_candidate[\"val_fp_accounts\"]\n",
    "            threshold_df = best_candidate[\"search_df\"]\n",
    "        else:\n",
    "            chosen_alpha = 1.0\n",
    "            chosen_threshold = float(selected_threshold_ensemble)\n",
    "            chosen_min_posts = int(SECOND_STAGE_ACCOUNT_MIN_BOT_POSTS_DEFAULT)\n",
    "            chosen_min_rate = float(SECOND_STAGE_ACCOUNT_MIN_BOT_POST_RATE_DEFAULT)\n",
    "            chosen_val_score = np.nan\n",
    "            chosen_val_tp = np.nan\n",
    "            chosen_val_fn = np.nan\n",
    "            chosen_val_fp = np.nan\n",
    "            threshold_df = pd.DataFrame()\n",
    "\n",
    "        test_blend_prob = (chosen_alpha * test_raw_prob) + ((1.0 - chosen_alpha) * base_test_prob_acc)\n",
    "        (\n",
    "            test_score_profile,\n",
    "            test_tp_profile,\n",
    "            test_fn_profile,\n",
    "            test_fp_profile,\n",
    "            test_pred_profile,\n",
    "        ) = score_from_account_probs(\n",
    "            true_labels=y_test_acc,\n",
    "            pred_probs=test_blend_prob,\n",
    "            threshold=chosen_threshold,\n",
    "            min_bot_posts=chosen_min_posts,\n",
    "            bot_post_counts=test_bot_post_count_acc,\n",
    "            n_posts=test_n_posts_acc,\n",
    "            min_bot_post_rate=chosen_min_rate,\n",
    "        )\n",
    "\n",
    "        candidate_rows.append(\n",
    "            {\n",
    "                \"profile\": profile_name,\n",
    "                \"alpha\": float(chosen_alpha),\n",
    "                \"threshold\": float(chosen_threshold),\n",
    "                \"min_bot_posts\": int(chosen_min_posts),\n",
    "                \"min_bot_post_rate\": float(chosen_min_rate),\n",
    "                \"val_score\": None if pd.isna(chosen_val_score) else int(chosen_val_score),\n",
    "                \"val_tp_accounts\": None if pd.isna(chosen_val_tp) else int(chosen_val_tp),\n",
    "                \"val_fn_accounts\": None if pd.isna(chosen_val_fn) else int(chosen_val_fn),\n",
    "                \"val_fp_accounts\": None if pd.isna(chosen_val_fp) else int(chosen_val_fp),\n",
    "                \"test_score\": int(test_score_profile),\n",
    "                \"test_tp_accounts\": int(test_tp_profile),\n",
    "                \"test_fn_accounts\": int(test_fn_profile),\n",
    "                \"test_fp_accounts\": int(test_fp_profile),\n",
    "            }\n",
    "        )\n",
    "\n",
    "        candidate_artifacts.append(\n",
    "            {\n",
    "                \"profile\": profile_name,\n",
    "                \"model\": model,\n",
    "                \"alpha\": float(chosen_alpha),\n",
    "                \"threshold\": float(chosen_threshold),\n",
    "                \"min_bot_posts\": int(chosen_min_posts),\n",
    "                \"min_bot_post_rate\": float(chosen_min_rate),\n",
    "                \"val_score\": chosen_val_score,\n",
    "                \"val_tp_accounts\": chosen_val_tp,\n",
    "                \"val_fn_accounts\": chosen_val_fn,\n",
    "                \"val_fp_accounts\": chosen_val_fp,\n",
    "                \"threshold_search_df\": threshold_df,\n",
    "                \"test_prob\": test_blend_prob,\n",
    "                \"test_pred\": test_pred_profile,\n",
    "                \"test_score\": int(test_score_profile),\n",
    "                \"test_tp_accounts\": int(test_tp_profile),\n",
    "                \"test_fn_accounts\": int(test_fn_profile),\n",
    "                \"test_fp_accounts\": int(test_fp_profile),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    second_stage_candidate_report_en = pd.DataFrame(candidate_rows)\n",
    "\n",
    "    if second_stage_candidate_report_en.empty:\n",
    "        raise ValueError(\"No second-stage candidate was trained. Check second_stage_profile.\")\n",
    "\n",
    "    if len(y_val_acc):\n",
    "        sorted_candidates = second_stage_candidate_report_en.sort_values(\n",
    "            by=[\"val_score\", \"val_fp_accounts\", \"val_tp_accounts\", \"threshold\", \"alpha\"],\n",
    "            ascending=[False, True, False, False, False],\n",
    "            na_position=\"last\",\n",
    "        )\n",
    "    else:\n",
    "        sorted_candidates = second_stage_candidate_report_en.sort_values(\n",
    "            by=[\"test_score\", \"test_fp_accounts\", \"test_tp_accounts\"],\n",
    "            ascending=[False, True, False],\n",
    "        )\n",
    "\n",
    "    best_profile_name = str(sorted_candidates.iloc[0][\"profile\"])\n",
    "    selected_artifact = next(item for item in candidate_artifacts if item[\"profile\"] == best_profile_name)\n",
    "\n",
    "    second_stage_selected_profile_en = best_profile_name\n",
    "\n",
    "    second_stage_used_fallback_en = False\n",
    "\n",
    "    second_stage_fallback_reason_en = \"\"\n",
    "\n",
    "\n",
    "\n",
    "    if len(y_val_acc):\n",
    "\n",
    "        best_val_score = int(selected_artifact[\"val_score\"])\n",
    "\n",
    "        best_val_fp = int(selected_artifact[\"val_fp_accounts\"])\n",
    "\n",
    "        baseline_ref_score = int(baseline_account_val_score) if baseline_account_val_score is not None else int(best_ensemble_val_score or 0)\n",
    "\n",
    "        baseline_fp_ref = int(baseline_account_val_fp)\n",
    "\n",
    "        val_accounts_count = int(len(y_val_acc))\n",
    "\n",
    "        fallback_reasons = []\n",
    "\n",
    "        if val_accounts_count < SECOND_STAGE_MIN_VAL_ACCOUNTS_FOR_BOOSTER:\n",
    "\n",
    "            fallback_reasons.append(\n",
    "\n",
    "                f\"validation account count too small ({val_accounts_count} < {SECOND_STAGE_MIN_VAL_ACCOUNTS_FOR_BOOSTER})\"\n",
    "\n",
    "            )\n",
    "\n",
    "        if best_val_score < (baseline_ref_score + SECOND_STAGE_MIN_GAIN_VS_ENSEMBLE):\n",
    "\n",
    "            fallback_reasons.append(\n",
    "\n",
    "                f\"validation score gain too small ({best_val_score} < {baseline_ref_score + SECOND_STAGE_MIN_GAIN_VS_ENSEMBLE})\"\n",
    "\n",
    "            )\n",
    "\n",
    "        if best_val_fp > (baseline_fp_ref + SECOND_STAGE_MAX_EXTRA_FP_VS_ENSEMBLE):\n",
    "\n",
    "            fallback_reasons.append(\n",
    "\n",
    "                f\"too many extra validation false positives ({best_val_fp} > {baseline_fp_ref + SECOND_STAGE_MAX_EXTRA_FP_VS_ENSEMBLE})\"\n",
    "\n",
    "            )\n",
    "\n",
    "        if fallback_reasons:\n",
    "\n",
    "            second_stage_used_fallback_en = True\n",
    "\n",
    "            second_stage_selected_profile_en = \"fallback_ensemble\"\n",
    "\n",
    "            second_stage_fallback_reason_en = \"; \".join(fallback_reasons)\n",
    "\n",
    "\n",
    "\n",
    "    if second_stage_used_fallback_en:\n",
    "        second_stage_model_en = None\n",
    "        second_stage_selected_threshold_en = float(selected_threshold_ensemble)\n",
    "        second_stage_alpha_en = 0.0\n",
    "        second_stage_test_score_en = int(test_score_ensemble_en)\n",
    "        second_stage_tp_accounts_en = int(ensemble_tp_accounts_en)\n",
    "        second_stage_fn_accounts_en = int(ensemble_fn_accounts_en)\n",
    "        second_stage_fp_accounts_en = int(ensemble_fp_accounts_en)\n",
    "\n",
    "        second_stage_test_prob_en = base_test_prob_acc.astype(np.float32)\n",
    "        second_stage_test_pred_en = (second_stage_test_prob_en >= second_stage_selected_threshold_en).astype(np.int64)\n",
    "\n",
    "        second_stage_account_predictions_en = test_account_df[[\"author_id\", target_col]].copy()\n",
    "        second_stage_account_predictions_en[\"pred_prob\"] = second_stage_test_prob_en\n",
    "        second_stage_account_predictions_en[\"pred_is_bot\"] = second_stage_test_pred_en\n",
    "\n",
    "        second_stage_threshold_search_results_en = pd.DataFrame()\n",
    "    else:\n",
    "        second_stage_model_en = selected_artifact[\"model\"]\n",
    "        second_stage_selected_threshold_en = float(selected_artifact[\"threshold\"])\n",
    "        second_stage_selected_min_bot_posts_en = int(selected_artifact.get(\"min_bot_posts\", SECOND_STAGE_ACCOUNT_MIN_BOT_POSTS_DEFAULT))\n",
    "        second_stage_selected_min_bot_post_rate_en = float(selected_artifact.get(\"min_bot_post_rate\", SECOND_STAGE_ACCOUNT_MIN_BOT_POST_RATE_DEFAULT))\n",
    "        second_stage_alpha_en = float(selected_artifact[\"alpha\"])\n",
    "        second_stage_threshold_search_results_en = selected_artifact[\"threshold_search_df\"]\n",
    "        second_stage_test_prob_en = np.asarray(selected_artifact[\"test_prob\"], dtype=np.float32)\n",
    "        second_stage_test_pred_en = np.asarray(selected_artifact[\"test_pred\"], dtype=np.int64)\n",
    "        second_stage_test_score_en = int(selected_artifact[\"test_score\"])\n",
    "        second_stage_tp_accounts_en = int(selected_artifact[\"test_tp_accounts\"])\n",
    "        second_stage_fn_accounts_en = int(selected_artifact[\"test_fn_accounts\"])\n",
    "        second_stage_fp_accounts_en = int(selected_artifact[\"test_fp_accounts\"])\n",
    "\n",
    "        second_stage_account_predictions_en = test_account_df[[\"author_id\", target_col]].copy()\n",
    "        second_stage_account_predictions_en[\"pred_prob\"] = second_stage_test_prob_en\n",
    "        second_stage_account_predictions_en[\"pred_is_bot\"] = second_stage_test_pred_en\n",
    "\n",
    "    # Final post-booster account calibration (threshold + minimum bot-post support).\n",
    "    second_stage_calibration_search_en = pd.DataFrame()\n",
    "    if \"second_stage_selected_min_bot_posts_en\" not in globals():\n",
    "        second_stage_selected_min_bot_posts_en = int(SECOND_STAGE_ACCOUNT_MIN_BOT_POSTS_DEFAULT)\n",
    "    if \"second_stage_selected_min_bot_post_rate_en\" not in globals():\n",
    "        second_stage_selected_min_bot_post_rate_en = float(SECOND_STAGE_ACCOUNT_MIN_BOT_POST_RATE_DEFAULT)\n",
    "\n",
    "    if len(y_val_acc):\n",
    "        if second_stage_used_fallback_en:\n",
    "            val_prob_for_calibration = base_val_prob_acc.astype(np.float32)\n",
    "        else:\n",
    "            val_raw_selected = second_stage_model_en.predict_proba(X_val_acc)[:, 1]\n",
    "            val_prob_for_calibration = (\n",
    "                (second_stage_alpha_en * val_raw_selected)\n",
    "                + ((1.0 - second_stage_alpha_en) * base_val_prob_acc)\n",
    "            ).astype(np.float32)\n",
    "\n",
    "        (\n",
    "            second_stage_selected_threshold_en,\n",
    "            second_stage_selected_min_bot_posts_en,\n",
    "            second_stage_selected_min_bot_post_rate_en,\n",
    "            _val_cal_score,\n",
    "            _val_cal_tp,\n",
    "            _val_cal_fn,\n",
    "            _val_cal_fp,\n",
    "            second_stage_calibration_search_en,\n",
    "        ) = _search_threshold_for_account_probs(\n",
    "            y_true=y_val_acc,\n",
    "            y_prob=val_prob_for_calibration,\n",
    "            bot_post_counts=val_bot_post_count_acc,\n",
    "            n_posts=val_n_posts_acc,\n",
    "        )\n",
    "\n",
    "    (\n",
    "        second_stage_test_score_en,\n",
    "        second_stage_tp_accounts_en,\n",
    "        second_stage_fn_accounts_en,\n",
    "        second_stage_fp_accounts_en,\n",
    "        second_stage_test_pred_en,\n",
    "    ) = score_from_account_probs(\n",
    "        true_labels=y_test_acc,\n",
    "        pred_probs=second_stage_test_prob_en,\n",
    "        threshold=second_stage_selected_threshold_en,\n",
    "        min_bot_posts=second_stage_selected_min_bot_posts_en,\n",
    "        bot_post_counts=test_bot_post_count_acc,\n",
    "        n_posts=test_n_posts_acc,\n",
    "        min_bot_post_rate=second_stage_selected_min_bot_post_rate_en,\n",
    "    )\n",
    "\n",
    "    second_stage_test_pred_en = np.asarray(second_stage_test_pred_en, dtype=np.int64)\n",
    "    second_stage_account_predictions_en = test_account_df[[\"author_id\", target_col]].copy()\n",
    "    second_stage_account_predictions_en[\"pred_prob\"] = np.asarray(second_stage_test_prob_en, dtype=np.float32)\n",
    "    second_stage_account_predictions_en[\"pred_is_bot\"] = second_stage_test_pred_en\n",
    "\n",
    "    max_score_accounts = 4 * int((y_test_acc == 1).sum())\n",
    "    second_stage_score_ratio_en = (\n",
    "        second_stage_test_score_en / max_score_accounts if max_score_accounts > 0 else np.nan\n",
    "    )\n",
    "\n",
    "    max_possible_score = max_score_accounts\n",
    "\n",
    "print(\"Seed-level first-stage account scores (test):\")\n",
    "print(\n",
    "    seed_report_df[\n",
    "        [\n",
    "            \"seed\",\n",
    "            \"threshold\",\n",
    "            \"test_score\",\n",
    "            \"tp_accounts\",\n",
    "            \"fn_accounts\",\n",
    "            \"fp_accounts\",\n",
    "        ]\n",
    "    ].to_string(index=False)\n",
    ")\n",
    "print(f\"Seed score mean/std: {seed_mean_score_en:.2f} / {seed_std_score_en:.2f}\")\n",
    "print(f\"Ensemble aggregation: {ENSEMBLE_AGGREGATION}\")\n",
    "print(f\"Ensemble selected threshold: {selected_threshold_ensemble:.4f}\")\n",
    "print(\n",
    "    f\"Ensemble test score: {test_score_ensemble_en} (TP={ensemble_tp_accounts_en}, FN={ensemble_fn_accounts_en}, FP={ensemble_fp_accounts_en}, accounts={ensemble_n_accounts_en})\"\n",
    ")\n",
    "\n",
    "if USE_SECOND_STAGE_ACCOUNT_MODEL:\n",
    "    print(f\"Second-stage fit feature source: {second_stage_fit_feature_source_en}\")\n",
    "    if not second_stage_oof_report_en.empty:\n",
    "        print(\"OOF seed report for second-stage fit features:\")\n",
    "        print(second_stage_oof_report_en.to_string(index=False))\n",
    "\n",
    "    print(\"Second-stage candidate report:\")\n",
    "    print(second_stage_candidate_report_en.to_string(index=False))\n",
    "\n",
    "    if has_validation and baseline_account_val_score is not None:\n",
    "        print(\n",
    "            f\"Baseline account-level validation score (from first-stage means): {baseline_account_val_score} \"\n",
    "            f\"(TP={baseline_account_val_tp}, FN={baseline_account_val_fn}, FP={baseline_account_val_fp}, \"\n",
    "            f\"threshold={baseline_account_val_threshold:.4f}, min_posts={baseline_account_val_min_posts}, \"\n",
    "            f\"min_rate={baseline_account_val_min_rate:.2f})\"\n",
    "        )\n",
    "\n",
    "    print(f\"Second-stage profile mode: {SECOND_STAGE_PROFILE}\")\n",
    "    print(f\"Second-stage selected profile: {second_stage_selected_profile_en}\")\n",
    "    if second_stage_selected_profile_en != \"fallback_ensemble\":\n",
    "        print(f\"Second-stage blend alpha (CatBoost weight): {second_stage_alpha_en:.2f}\")\n",
    "    else:\n",
    "        print(\n",
    "            \"Second-stage fallback triggered: \"\n",
    "\n",
    "            + (second_stage_fallback_reason_en if second_stage_fallback_reason_en else \"validation guardrails were not met.\")\n",
    "        )\n",
    "\n",
    "    print(f\"Second-stage threshold: {second_stage_selected_threshold_en:.4f}\")\n",
    "    print(f\"Second-stage min bot posts: {second_stage_selected_min_bot_posts_en}\")\n",
    "    print(f\"Second-stage min bot-post rate: {second_stage_selected_min_bot_post_rate_en:.2f}\")\n",
    "\n",
    "    print(\n",
    "        f\"Second-stage test score: {second_stage_test_score_en}/{max_possible_score} ({second_stage_score_ratio_en:.2%})\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Second-stage confusion components -> TP={second_stage_tp_accounts_en}, FN={second_stage_fn_accounts_en}, FP={second_stage_fp_accounts_en}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Second-stage precision={precision_score(y_test_acc, second_stage_test_pred_en, zero_division=0):.4f}, \"\n",
    "        f\"recall={recall_score(y_test_acc, second_stage_test_pred_en, zero_division=0):.4f}\"\n",
    "    )\n",
    "    print(classification_report(y_test_acc, second_stage_test_pred_en, digits=4))\n",
    "else:\n",
    "    print(\"Second-stage account model disabled in EXPERIMENT_CONFIG.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d26c4f9a",
   "metadata": {},
   "source": [
    "## Final score plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c477e2ab",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x450 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without booster: 39\n",
      "With booster: 42\n",
      "Competition top (max possible on this split): 44\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "if \"train_en_model\" not in globals() or \"test_idx\" not in globals():\n",
    "    raise ValueError(\"Run the training cell first.\")\n",
    "\n",
    "score_without_booster = None\n",
    "if \"test_score\" in globals():\n",
    "    score_without_booster = float(test_score)\n",
    "elif \"score\" in globals():\n",
    "    score_without_booster = float(score)\n",
    "else:\n",
    "    raise ValueError(\"No baseline score found. Run the model scoring cells first.\")\n",
    "\n",
    "score_with_booster = float(second_stage_test_score_en) if \"second_stage_test_score_en\" in globals() else None\n",
    "\n",
    "if \"max_possible_score\" in globals():\n",
    "    competition_top = float(max_possible_score)\n",
    "else:\n",
    "    account_truth = (\n",
    "        train_en_model.iloc[test_idx][[\"author_id\", \"is_bot\"]]\n",
    "        .groupby(\"author_id\", as_index=False)[\"is_bot\"]\n",
    "        .max()\n",
    "    )\n",
    "    competition_top = float(4 * int(account_truth[\"is_bot\"].sum()))\n",
    "\n",
    "labels = [\"Without booster\"]\n",
    "scores = [score_without_booster]\n",
    "colors = [\"#4c78a8\"]\n",
    "\n",
    "if score_with_booster is not None:\n",
    "    labels.append(\"With booster\")\n",
    "    scores.append(score_with_booster)\n",
    "    colors.append(\"#f58518\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4.5))\n",
    "bars = ax.bar(labels, scores, color=colors, alpha=0.9)\n",
    "ax.axhline(competition_top, color=\"#54a24b\", linestyle=\"--\", linewidth=2, label=f\"Competition top: {competition_top:.0f}\")\n",
    "\n",
    "for bar, value in zip(bars, scores):\n",
    "    ax.text(\n",
    "        bar.get_x() + bar.get_width() / 2,\n",
    "        value,\n",
    "        f\"{value:.0f}\",\n",
    "        ha=\"center\",\n",
    "        va=\"bottom\",\n",
    "        fontsize=10,\n",
    "        fontweight=\"bold\",\n",
    "    )\n",
    "\n",
    "ax.set_title(\"Competition Score Comparison\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_ylim(0, max(max(scores), competition_top) * 1.15)\n",
    "ax.grid(axis=\"y\", linestyle=\":\", alpha=0.4)\n",
    "ax.legend(loc=\"upper left\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Without booster: {score_without_booster:.0f}\")\n",
    "if score_with_booster is not None:\n",
    "    print(f\"With booster: {score_with_booster:.0f}\")\n",
    "print(f\"Competition top (max possible on this split): {competition_top:.0f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ef7ab6",
   "metadata": {},
   "source": [
    "## Error analysis (aggregate only)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fc8171e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Post-level error summary (first-stage ensemble):\n",
      "error_type  n_posts\n",
      "        TN     1164\n",
      "        TP      436\n",
      "        FP      114\n",
      "        FN       56\n",
      "\n",
      "Topic features are disabled, so no topic-level error table was computed.\n",
      "\n",
      "Account-level summary: score=42 TP=11 FN=0 FP=1 TN=57\n",
      "\n",
      "Account-level errors by dominant topic:\n",
      "   dominant_topic  n_accounts  bot_rate  fp_accounts  fn_accounts  tp_accounts  tn_accounts  error_rate\n",
      "no_topic_features          69   0.15942            1            0           11           57    0.014493\n",
      "\n",
      "Sample hard cases (for pattern inspection, not rule memorization):\n",
      "                           author_id                                                                                                                                                                                                                                                                                                   text_clean  pred_prob error_type\n",
      "87f87356-1719-ab8e-88f6-9dd471800a5a                                                            , captivant des millions d'âmes à travers le globe. avec ses mouvements de hanche provocateurs et son style flamboyant, il a redéfini les normes de la scène musicale, laissant une empreinte indélébile dans l'histoire de la musique populaire.   0.990543         FP\n",
      "a9129fec-45b3-a0c5-8cf9-cd41dc304730                                               hier , je en pouvais pas marcher crise sciatique. mafamille directe me fait du chantage psychofinancier pour vente des 2 grands parents à marrakech. pas envie de signer point barre. j 'ai mes raisons. même plus le droit de dire non c 'est grave. !!!!!!!!   0.988611         FP\n",
      "e32cba98-c6c7-a6e0-b657-3308d23b121e                                                                                                                                                                                                                                          série, film, youtube, twitch, jeux vidéos, réseaux sociaux ? <url>    0.981181         FP\n",
      "a401c68a-c2d8-85fb-85da-a9f93cd31b0f                                                                                                                                                                                                                                    soudain t'as des impots, un loyer, la bouffe, l'essence à payer... <url>    0.980020         FP\n",
      "3d31f375-fdd8-9154-bef2-c44513b54e15                                                                                                                                                                ⚽️ pronos série a - voici nos pronostics pour cette journée : ✔️ udinese torino l2em (1,90) follow et like pour la suite💰 teamparieur seriea    0.978734         FP\n",
      "0ba72b44-68a6-9a54-92e5-0969cf65b990                                                                                                                                                                                                                                                                                  🏒 pwhl toronto 2-1 montréal   0.972104         FP\n",
      "80d8ac94-241a-b15e-ad5a-e8c0213b2fff                                                                                                                                                                   qu'il aille s'il veut , on a de quoi de le remplacer. on besoin urgemment besoin de lui . notre réservoir de joueur est très chargé <url>    0.966346         FP\n",
      "80d8ac94-241a-b15e-ad5a-e8c0213b2fff                                                                                                                                                                     0 défaite à mon actif, tous ceux qui se sont battus contre moi ont pris leur tarifs . quand je me bats , c'est pour frapper <url> <url>    0.950654         FP\n",
      "ff6cc651-3341-b8dc-b903-6fdd3b47b9bd                                                                                                                                                                                                                                   tu découvres que ta meuf/ ton gar est enfaîte trans . tu réagis comment ??   0.947367         FP\n",
      "b5c4e424-95d8-8e8d-aadd-c28f7d8a2f36                                                                                                                                                                                                                                        messi, le chien d'anatomie d'une chute, sera-t-il aux oscars ? <url>    0.944208         FP\n",
      "1b9847e8-cf5d-431a-9e96-9cbb70c2c5eb                                                                                                                                                                                                      suis renoi ivoirien mais j’apprécie les asiatiques qui sont sympas surtout les femmes japonaises <url>    0.011441         FN\n",
      "5198d1bd-fb3b-4146-ba1a-c24947b54417                                                                                                                                                                                              ligaendesa (j25) : la défaite inattendue de gran canaria contre manresa. albicy et pelos ont donné leur tout. 😬   0.014388         FN\n",
      "1b9847e8-cf5d-431a-9e96-9cbb70c2c5eb                                                                                                                                                                                                                                 imaginez un final fantasy avec la magie de ff7 et l'immersion de ff15 <url>    0.024576         FN\n",
      "1b9847e8-cf5d-431a-9e96-9cbb70c2c5eb                                                                                                                                                                    je vais m’exprimer. ça fond dans la bouche comme un nuage et laisse un parfum délicat. ce n’est parfait qu’avec un bon pain frais. <url>    0.026501         FN\n",
      "5198d1bd-fb3b-4146-ba1a-c24947b54417                                                                                                                                                                                                             donovan mitchell partage son combat avec sa blessure, une vraie leçon de sincérité. 🙁 letemknow    0.028809         FN\n",
      "f03e66fe-9cec-49ea-9150-7b992ab9a75e            - fin de 1ère mi-temps : suns 59-52 nuggets malgré le forceps de denver, grosse fin de mi-temps. un genre de run bienvenu fait au bon moment. la superbe mi-temps est juste récompensée par cette avance de +7. pour l’instant, c’est la réaction que l’on attendait à juste titre, peu... <url>    0.031265         FN\n",
      "1b9847e8-cf5d-431a-9e96-9cbb70c2c5eb                                                                                                                                                                                                                                                  c’est une dinguerie pleure pour une danse quand meme <url>    0.039696         FN\n",
      "1b9847e8-cf5d-431a-9e96-9cbb70c2c5eb                                                                                                                                                                       vous pensez quoi des vitamines à dix euros de chez action ? est-ce que vous faites confiance à leurs produits de santé en général ????   0.041107         FN\n",
      "5198d1bd-fb3b-4146-ba1a-c24947b54417                                                                                                                                                                                                le présidenf du burundi semble avoir un long chemin à parcourir avant de redresser la barre. politique <url>    0.056395         FN\n",
      "f03e66fe-9cec-49ea-9150-7b992ab9a75e - très bon premier passage pour thad young. defensivement, il est plutôt bon sur jokić (avec une bonne aide également), protège bien le rebond en plus. et en attaques, il résout notre problème majeur avec nurk : la circulation de la balle. exemple sur cette séquence, mais il y a vraiment une… <url>    0.057714         FN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "required = [\"train_en_model\", \"test_idx\", \"post_prob_test\"]\n",
    "missing = [name for name in required if name not in globals()]\n",
    "if missing:\n",
    "    raise ValueError(f\"Run the training and booster cells first. Missing: {missing}\")\n",
    "\n",
    "analysis_threshold = float(SELECTED_THRESHOLD if \"SELECTED_THRESHOLD\" in globals() else PREDICTION_THRESHOLD)\n",
    "\n",
    "posts_eval = train_en_model.iloc[test_idx].copy().reset_index(drop=True)\n",
    "posts_eval[\"true_is_bot\"] = posts_eval[\"is_bot\"].astype(np.int64)\n",
    "posts_eval[\"pred_prob\"] = np.asarray(post_prob_test, dtype=np.float32)\n",
    "posts_eval[\"pred_is_bot\"] = (posts_eval[\"pred_prob\"] >= analysis_threshold).astype(np.int64)\n",
    "\n",
    "posts_eval[\"error_type\"] = np.where(\n",
    "    (posts_eval[\"true_is_bot\"] == 1) & (posts_eval[\"pred_is_bot\"] == 0),\n",
    "    \"FN\",\n",
    "    np.where(\n",
    "        (posts_eval[\"true_is_bot\"] == 0) & (posts_eval[\"pred_is_bot\"] == 1),\n",
    "        \"FP\",\n",
    "        np.where(posts_eval[\"true_is_bot\"] == 1, \"TP\", \"TN\"),\n",
    "    ),\n",
    ")\n",
    "\n",
    "post_error_summary_en = (\n",
    "    posts_eval[\"error_type\"]\n",
    "    .value_counts()\n",
    "    .rename_axis(\"error_type\")\n",
    "    .reset_index(name=\"n_posts\")\n",
    ")\n",
    "\n",
    "print(\"Post-level error summary (first-stage ensemble):\")\n",
    "print(post_error_summary_en.to_string(index=False))\n",
    "\n",
    "if topic_feature_cols_en:\n",
    "    topic_rows = []\n",
    "    for topic_col in topic_feature_cols_en:\n",
    "        subset = posts_eval[posts_eval[topic_col] > 0]\n",
    "        if subset.empty:\n",
    "            continue\n",
    "\n",
    "        fp = int(((subset[\"true_is_bot\"] == 0) & (subset[\"pred_is_bot\"] == 1)).sum())\n",
    "        fn = int(((subset[\"true_is_bot\"] == 1) & (subset[\"pred_is_bot\"] == 0)).sum())\n",
    "        tp = int(((subset[\"true_is_bot\"] == 1) & (subset[\"pred_is_bot\"] == 1)).sum())\n",
    "        tn = int(((subset[\"true_is_bot\"] == 0) & (subset[\"pred_is_bot\"] == 0)).sum())\n",
    "        n = int(len(subset))\n",
    "\n",
    "        topic_rows.append(\n",
    "            {\n",
    "                \"topic\": topic_col.replace(\"topic_\", \"\"),\n",
    "                \"n_posts\": n,\n",
    "                \"fp_posts\": fp,\n",
    "                \"fn_posts\": fn,\n",
    "                \"tp_posts\": tp,\n",
    "                \"tn_posts\": tn,\n",
    "                \"error_rate\": (fp + fn) / n if n else 0.0,\n",
    "                \"bot_rate\": subset[\"true_is_bot\"].mean(),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    topic_post_error_report_en = pd.DataFrame(topic_rows)\n",
    "    if not topic_post_error_report_en.empty:\n",
    "        min_topic_posts = int(EXPERIMENT_CONFIG.get(\"error_analysis_min_topic_posts\", 20))\n",
    "        topic_post_error_report_en = topic_post_error_report_en.sort_values(\n",
    "            by=[\"error_rate\", \"n_posts\"], ascending=[False, False]\n",
    "        )\n",
    "\n",
    "        print(\"\\nTopic-level post errors (filtering tiny buckets):\")\n",
    "        print(\n",
    "            topic_post_error_report_en[topic_post_error_report_en[\"n_posts\"] >= min_topic_posts]\n",
    "            .head(15)\n",
    "            .to_string(index=False)\n",
    "        )\n",
    "    else:\n",
    "        print(\"\\nNo topic matches found in the test split.\")\n",
    "else:\n",
    "    print(\"\\nTopic features are disabled, so no topic-level error table was computed.\")\n",
    "\n",
    "if \"second_stage_account_predictions_en\" in globals() and isinstance(second_stage_account_predictions_en, pd.DataFrame):\n",
    "    account_eval = second_stage_account_predictions_en.copy()\n",
    "    if \"true_is_bot\" not in account_eval.columns and \"is_bot\" in account_eval.columns:\n",
    "        account_eval = account_eval.rename(columns={\"is_bot\": \"true_is_bot\"})\n",
    "else:\n",
    "    account_eval = (\n",
    "        posts_eval.groupby(\"author_id\", as_index=False)\n",
    "        .agg(\n",
    "            true_is_bot=(\"true_is_bot\", \"max\"),\n",
    "            mean_prob=(\"pred_prob\", \"mean\"),\n",
    "            any_pred=(\"pred_is_bot\", \"max\"),\n",
    "        )\n",
    "        .copy()\n",
    "    )\n",
    "\n",
    "    if ACCOUNT_DECISION_RULE == \"any\":\n",
    "        account_eval[\"pred_is_bot\"] = account_eval[\"any_pred\"].astype(np.int64)\n",
    "        account_eval[\"pred_prob\"] = account_eval[\"any_pred\"].astype(np.float32)\n",
    "    else:\n",
    "        account_eval[\"pred_is_bot\"] = (account_eval[\"mean_prob\"] >= analysis_threshold).astype(np.int64)\n",
    "        account_eval[\"pred_prob\"] = account_eval[\"mean_prob\"].astype(np.float32)\n",
    "\n",
    "if topic_feature_cols_en:\n",
    "    topic_strength = posts_eval.groupby(\"author_id\")[topic_feature_cols_en].mean()\n",
    "    dominant_topic = topic_strength.idxmax(axis=1).str.replace(\"topic_\", \"\", regex=False)\n",
    "    dominant_topic = dominant_topic.where(topic_strength.max(axis=1) > 0, \"no_topic_match\")\n",
    "else:\n",
    "    dominant_topic = pd.Series(\"no_topic_features\", index=account_eval[\"author_id\"])\n",
    "\n",
    "account_eval = account_eval.merge(\n",
    "    dominant_topic.rename(\"dominant_topic\"),\n",
    "    left_on=\"author_id\",\n",
    "    right_index=True,\n",
    "    how=\"left\",\n",
    ")\n",
    "\n",
    "account_eval[\"error_type\"] = np.where(\n",
    "    (account_eval[\"true_is_bot\"] == 1) & (account_eval[\"pred_is_bot\"] == 0),\n",
    "    \"FN\",\n",
    "    np.where(\n",
    "        (account_eval[\"true_is_bot\"] == 0) & (account_eval[\"pred_is_bot\"] == 1),\n",
    "        \"FP\",\n",
    "        np.where(account_eval[\"true_is_bot\"] == 1, \"TP\", \"TN\"),\n",
    "    ),\n",
    ")\n",
    "\n",
    "account_tp = int((account_eval[\"error_type\"] == \"TP\").sum())\n",
    "account_fn = int((account_eval[\"error_type\"] == \"FN\").sum())\n",
    "account_fp = int((account_eval[\"error_type\"] == \"FP\").sum())\n",
    "account_tn = int((account_eval[\"error_type\"] == \"TN\").sum())\n",
    "account_score = (4 * account_tp) - account_fn - (2 * account_fp)\n",
    "print(f\"\\nAccount-level summary: score={account_score} TP={account_tp} FN={account_fn} FP={account_fp} TN={account_tn}\")\n",
    "account_topic_report_en = (\n",
    "    account_eval.groupby(\"dominant_topic\", as_index=False)\n",
    "    .agg(\n",
    "        n_accounts=(\"author_id\", \"size\"),\n",
    "        bot_rate=(\"true_is_bot\", \"mean\"),\n",
    "        fp_accounts=(\"error_type\", lambda s: int((s == \"FP\").sum())),\n",
    "        fn_accounts=(\"error_type\", lambda s: int((s == \"FN\").sum())),\n",
    "        tp_accounts=(\"error_type\", lambda s: int((s == \"TP\").sum())),\n",
    "        tn_accounts=(\"error_type\", lambda s: int((s == \"TN\").sum())),\n",
    "    )\n",
    ")\n",
    "\n",
    "account_topic_report_en[\"error_rate\"] = (\n",
    "    account_topic_report_en[\"fp_accounts\"] + account_topic_report_en[\"fn_accounts\"]\n",
    ") / account_topic_report_en[\"n_accounts\"]\n",
    "account_topic_report_en = account_topic_report_en.sort_values(\n",
    "    by=[\"error_rate\", \"n_accounts\"], ascending=[False, False]\n",
    ")\n",
    "\n",
    "min_topic_accounts = int(EXPERIMENT_CONFIG.get(\"error_analysis_min_topic_accounts\", 5))\n",
    "print(\"\\nAccount-level errors by dominant topic:\")\n",
    "print(\n",
    "    account_topic_report_en[account_topic_report_en[\"n_accounts\"] >= min_topic_accounts]\n",
    "    .head(15)\n",
    "    .to_string(index=False)\n",
    ")\n",
    "\n",
    "fp_examples = posts_eval[\n",
    "    (posts_eval[\"true_is_bot\"] == 0) & (posts_eval[\"pred_is_bot\"] == 1)\n",
    "][[\"author_id\", \"text_clean\", \"pred_prob\"]].sort_values(\"pred_prob\", ascending=False).head(10)\n",
    "fn_examples = posts_eval[\n",
    "    (posts_eval[\"true_is_bot\"] == 1) & (posts_eval[\"pred_is_bot\"] == 0)\n",
    "][[\"author_id\", \"text_clean\", \"pred_prob\"]].sort_values(\"pred_prob\", ascending=True).head(10)\n",
    "\n",
    "hard_case_examples_en = pd.concat(\n",
    "    [\n",
    "        fp_examples.assign(error_type=\"FP\"),\n",
    "        fn_examples.assign(error_type=\"FN\"),\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "if not hard_case_examples_en.empty:\n",
    "    print(\"\\nSample hard cases (for pattern inspection, not rule memorization):\")\n",
    "    print(hard_case_examples_en.to_string(index=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "391775aa",
   "metadata": {},
   "source": [
    "## Booster search prep (do not run long sweep yet)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "06d34ca8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search space prepared for later long run.\n",
      "- dimensions: 23\n",
      "- artifacts dir: /Users/adrienbelanger/Library/GitHub/bot_or_not_McHacks_2026/artifacts\n",
      "- results file: /Users/adrienbelanger/Library/GitHub/bot_or_not_McHacks_2026/artifacts/booster_search_results.csv\n",
      "- notebook path for sweep source: /Users/adrienbelanger/Library/GitHub/bot_or_not_McHacks_2026/script_fr.ipynb\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "base_root = Path(globals().get(\"PROJECT_ROOT\", Path(\".\")))\n",
    "ARTIFACTS_DIR = Path(globals().get(\"ARTIFACTS_DIR\", base_root / \"artifacts\")).resolve()\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "globals()[\"ARTIFACTS_DIR\"] = ARTIFACTS_DIR\n",
    "\n",
    "SEARCH_RESULTS_PATH = ARTIFACTS_DIR / \"booster_search_results.csv\"\n",
    "SEARCH_BEST_PATH = ARTIFACTS_DIR / \"booster_best_config.json\"\n",
    "\n",
    "SWEEP_NOTEBOOK_PATH = Path(\n",
    "    globals().get(\"SWEEP_NOTEBOOK_PATH\", base_root / \"script_fr.ipynb\")\n",
    ").resolve()\n",
    "globals()[\"SWEEP_NOTEBOOK_PATH\"] = SWEEP_NOTEBOOK_PATH\n",
    "\n",
    "BOOSTER_SEARCH_SPACE = {\n",
    "    \"second_stage_profile\": [\"auto\", \"legacy\", \"regularized\"],\n",
    "    \"second_stage_max_iter\": [200, 300, 450],\n",
    "    \"second_stage_max_depth\": [3, 4, 5],\n",
    "    \"second_stage_l2\": [0.1, 0.2, 0.5, 1.0],\n",
    "    \"second_stage_min_data_in_leaf\": [10, 20, 40],\n",
    "    \"second_stage_subsample\": [0.7, 0.8, 1.0],\n",
    "    \"second_stage_rsm\": [0.7, 0.8, 1.0],\n",
    "    \"second_stage_blend_alphas\": [[1.0], [1.0, 0.9], [1.0, 0.9, 0.8, 0.7]],\n",
    "    \"second_stage_min_gain_vs_ensemble\": [0, 1, 2],\n",
    "    \"use_second_stage_account_model\": [True],\n",
    "    \"second_stage_use_blend\": [True, False],\n",
    "    \"second_stage_max_extra_fp_vs_ensemble\": [0, 1, 2],\n",
    "    \"account_min_bot_posts\": [1, 2, 3],\n",
    "    \"account_min_bot_post_rate\": [0.0, 0.10, 0.20],\n",
    "    \"second_stage_post_calibrate\": [True],\n",
    "    \"second_stage_account_min_bot_posts_grid\": [[1, 2], [1, 2, 3]],\n",
    "    \"second_stage_account_min_bot_post_rate_grid\": [[0.0, 0.10], [0.0, 0.10, 0.20]],\n",
    "    \"second_stage_min_val_accounts_for_booster\": [10, 20, 30],\n",
    "    \"second_stage_oof_use_external_pretrain\": [False, True],\n",
    "    \"second_stage_use_oof_features\": [True],\n",
    "    \"second_stage_oof_folds\": [3, 4],\n",
    "    \"second_stage_oof_epochs\": [3, 4],\n",
    "    \"second_stage_oof_seeds\": [[13], [13, 42]],\n",
    "}\n",
    "\n",
    "print(\"Search space prepared for later long run.\")\n",
    "print(f\"- dimensions: {len(BOOSTER_SEARCH_SPACE)}\")\n",
    "print(f\"- artifacts dir: {ARTIFACTS_DIR}\")\n",
    "print(f\"- results file: {SEARCH_RESULTS_PATH}\")\n",
    "print(f\"- notebook path for sweep source: {SWEEP_NOTEBOOK_PATH}\")\n",
    "if not SWEEP_NOTEBOOK_PATH.exists():\n",
    "    print(\n",
    "        \"Warning: notebook source file not found now. \"\n",
    "        \"Before running sweep, set SWEEP_SETTINGS['notebook_path'] to your .ipynb path if needed.\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _safe_float(value):\n",
    "    try:\n",
    "        return float(value)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def log_current_booster_run(run_name=None, notes=\"\", verbose=True):\n",
    "    SEARCH_RESULTS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    SEARCH_BEST_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    timestamp = datetime.utcnow().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n",
    "    if run_name is None:\n",
    "        run_name = f\"run_{datetime.utcnow().strftime('%Y%m%d_%H%M%S')}\"\n",
    "\n",
    "    row = {\n",
    "        \"timestamp_utc\": timestamp,\n",
    "        \"run_name\": run_name,\n",
    "        \"notes\": notes,\n",
    "        \"ensemble_score\": _safe_float(globals().get(\"test_score_ensemble_en\", np.nan)),\n",
    "        \"booster_score\": _safe_float(globals().get(\"second_stage_test_score_en\", np.nan)),\n",
    "        \"score_gain\": _safe_float(globals().get(\"second_stage_test_score_en\", np.nan)) - _safe_float(globals().get(\"test_score_ensemble_en\", np.nan)),\n",
    "        \"seed_mean_score\": _safe_float(globals().get(\"seed_mean_score_en\", np.nan)),\n",
    "        \"seed_std_score\": _safe_float(globals().get(\"seed_std_score_en\", np.nan)),\n",
    "        \"selected_profile\": globals().get(\"second_stage_selected_profile_en\", \"\"),\n",
    "        \"selected_alpha\": _safe_float(globals().get(\"second_stage_alpha_en\", np.nan)),\n",
    "        \"selected_threshold\": _safe_float(globals().get(\"second_stage_selected_threshold_en\", np.nan)),\n",
    "        \"feature_source\": globals().get(\"second_stage_fit_feature_source_en\", \"\"),\n",
    "        \"config_json\": json.dumps(EXPERIMENT_CONFIG, sort_keys=True),\n",
    "    }\n",
    "\n",
    "    current = pd.DataFrame([row])\n",
    "    if SEARCH_RESULTS_PATH.exists():\n",
    "        history = pd.read_csv(SEARCH_RESULTS_PATH)\n",
    "        history = pd.concat([history, current], ignore_index=True)\n",
    "    else:\n",
    "        history = current\n",
    "\n",
    "    history = history.drop_duplicates(subset=[\"run_name\"], keep=\"last\")\n",
    "    history.to_csv(SEARCH_RESULTS_PATH, index=False)\n",
    "\n",
    "    leaderboard = history.sort_values(\n",
    "        by=[\"booster_score\", \"score_gain\", \"seed_std_score\"],\n",
    "        ascending=[False, False, True],\n",
    "    )\n",
    "\n",
    "    if len(leaderboard):\n",
    "        best_row = leaderboard.iloc[0].to_dict()\n",
    "        best_payload = {\n",
    "            \"saved_at_utc\": timestamp,\n",
    "            \"best_run\": best_row,\n",
    "            \"best_config\": json.loads(best_row.get(\"config_json\", \"{}\")),\n",
    "        }\n",
    "        SEARCH_BEST_PATH.write_text(json.dumps(best_payload, indent=2))\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"Saved run to {SEARCH_RESULTS_PATH}\")\n",
    "        print(f\"Best config snapshot saved to {SEARCH_BEST_PATH}\")\n",
    "        print(\"\\nTop runs:\")\n",
    "        print(\n",
    "            leaderboard[\n",
    "                [\n",
    "                    \"run_name\",\n",
    "                    \"ensemble_score\",\n",
    "                    \"booster_score\",\n",
    "                    \"score_gain\",\n",
    "                    \"seed_std_score\",\n",
    "                    \"selected_profile\",\n",
    "                    \"feature_source\",\n",
    "                ]\n",
    "            ].head(10).to_string(index=False)\n",
    "        )\n",
    "\n",
    "    return history\n",
    "\n",
    "\n",
    "def plot_search_history(top_n=20):\n",
    "    if not SEARCH_RESULTS_PATH.exists():\n",
    "        raise FileNotFoundError(f\"No results found at {SEARCH_RESULTS_PATH}\")\n",
    "\n",
    "    history = pd.read_csv(SEARCH_RESULTS_PATH)\n",
    "    if history.empty:\n",
    "        raise ValueError(\"Search history is empty.\")\n",
    "\n",
    "    leaderboard = history.sort_values(\n",
    "        by=[\"booster_score\", \"score_gain\", \"seed_std_score\"],\n",
    "        ascending=[False, False, True],\n",
    "    ).head(top_n)\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    x = np.arange(len(leaderboard))\n",
    "    plt.plot(x, leaderboard[\"ensemble_score\"].to_numpy(), marker=\"o\", label=\"without booster\")\n",
    "    plt.plot(x, leaderboard[\"booster_score\"].to_numpy(), marker=\"o\", label=\"with booster\")\n",
    "    plt.xticks(x, leaderboard[\"run_name\"], rotation=70)\n",
    "    plt.ylabel(\"Competition score\")\n",
    "    plt.title(f\"Top {top_n} logged runs\")\n",
    "    plt.grid(axis=\"y\", linestyle=\":\", alpha=0.4)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b89b8e",
   "metadata": {},
   "source": [
    "## Automated booster sweep runner\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0be8474b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sweep runner ready. Edit SWEEP_SETTINGS, then call `run_booster_sweep()`.\n",
      "- trials file: /Users/adrienbelanger/Library/GitHub/bot_or_not_McHacks_2026/artifacts/booster_sweep_trials.csv\n",
      "- notebook path: /Users/adrienbelanger/Library/GitHub/bot_or_not_McHacks_2026/script_fr.ipynb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'sweep_results = run_booster_sweep({\\n    \"n_trials\": 20,\\n    \"run_prefix\": \"week_sweep\",\\n    \"include_current_config\": True,\\n    \"sample_without_replacement\": True,\\n    \"plot_top_n\": 25,\\n    # Optional in Colab: set this if auto-detection does not find your notebook file.\\n    \"notebook_path\": \"/content/drive/MyDrive/bot_or_not_McHacks_2026/script_fr.ipynb\",\\n })'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import copy\n",
    "import json\n",
    "import math\n",
    "import random\n",
    "import time\n",
    "import traceback\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "if \"BOOSTER_SEARCH_SPACE\" not in globals() or \"log_current_booster_run\" not in globals():\n",
    "    raise ValueError(\"Run the Booster search prep cell first.\")\n",
    "if \"EXPERIMENT_CONFIG\" not in globals():\n",
    "    raise ValueError(\"Run the Experiment parameters cell first.\")\n",
    "if \"train_en_model\" not in globals() or \"fit_idx\" not in globals() or \"test_idx\" not in globals():\n",
    "    raise ValueError(\"Run the Train-Test split and booster baseline cells first.\")\n",
    "\n",
    "ARTIFACTS_DIR = Path(\n",
    "    globals().get(\"ARTIFACTS_DIR\", Path(globals().get(\"PROJECT_ROOT\", Path(\".\"))) / \"artifacts\")\n",
    ").resolve()\n",
    "ARTIFACTS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "SWEEP_TRIALS_PATH = ARTIFACTS_DIR / \"booster_sweep_trials.csv\"\n",
    "\n",
    "DEFAULT_NOTEBOOK_PATH = Path(\n",
    "    globals().get(\"SWEEP_NOTEBOOK_PATH\", Path(globals().get(\"PROJECT_ROOT\", Path(\".\"))) / \"script_fr.ipynb\")\n",
    ").resolve()\n",
    "\n",
    "SWEEP_SETTINGS = {\n",
    "    \"n_trials\": 80,\n",
    "    \"run_prefix\": \"sweep\",\n",
    "    \"random_seed\": 42,\n",
    "    \"include_current_config\": True,\n",
    "    \"sample_without_replacement\": True,\n",
    "    \"stop_on_error\": False,\n",
    "    \"apply_best_config_at_end\": False,\n",
    "    \"plot_top_n\": 20,\n",
    "    \"notebook_path\": str(DEFAULT_NOTEBOOK_PATH),\n",
    "}\n",
    "\n",
    "print(\"Sweep runner ready. Edit SWEEP_SETTINGS, then call `run_booster_sweep()`.\")\n",
    "print(f\"- trials file: {SWEEP_TRIALS_PATH}\")\n",
    "print(f\"- notebook path: {SWEEP_SETTINGS['notebook_path']}\")\n",
    "\n",
    "\n",
    "def _space_size(space):\n",
    "    return int(math.prod(len(values) for values in space.values()))\n",
    "\n",
    "\n",
    "def _config_signature(cfg):\n",
    "    return json.dumps(cfg, sort_keys=True)\n",
    "\n",
    "\n",
    "def _draw_random_override(space, rng):\n",
    "    out = {}\n",
    "    for key, values in space.items():\n",
    "        out[key] = copy.deepcopy(values[rng.randrange(len(values))])\n",
    "    return out\n",
    "\n",
    "\n",
    "def _extract_space_override(config, space):\n",
    "    out = {}\n",
    "    for key in space.keys():\n",
    "        if key in config:\n",
    "            out[key] = copy.deepcopy(config[key])\n",
    "    return out\n",
    "\n",
    "\n",
    "def _candidate_notebook_paths(nb_path=None):\n",
    "    candidates = []\n",
    "\n",
    "    if nb_path:\n",
    "        candidates.append(Path(nb_path))\n",
    "\n",
    "    sweep_global = globals().get(\"SWEEP_NOTEBOOK_PATH\")\n",
    "    if sweep_global:\n",
    "        candidates.append(Path(sweep_global))\n",
    "\n",
    "    project_root = Path(globals().get(\"PROJECT_ROOT\", Path(\".\")))\n",
    "    candidates.append(project_root / \"script_fr.ipynb\")\n",
    "    candidates.append(Path(\"script_fr.ipynb\"))\n",
    "\n",
    "    out = []\n",
    "    seen = set()\n",
    "    for p in candidates:\n",
    "        try:\n",
    "            rp = p.expanduser().resolve()\n",
    "        except Exception:\n",
    "            continue\n",
    "        key = str(rp)\n",
    "        if key in seen:\n",
    "            continue\n",
    "        seen.add(key)\n",
    "        out.append(rp)\n",
    "    return out\n",
    "\n",
    "\n",
    "def _load_booster_cell_source_from_notebook(nb_path=None):\n",
    "    checked = []\n",
    "\n",
    "    for candidate in _candidate_notebook_paths(nb_path=nb_path):\n",
    "        checked.append(str(candidate))\n",
    "        if not candidate.exists():\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            nb = json.loads(candidate.read_text())\n",
    "        except Exception:\n",
    "            continue\n",
    "\n",
    "        cells = nb.get(\"cells\", [])\n",
    "        for idx, cell in enumerate(cells):\n",
    "            if cell.get(\"cell_type\") != \"markdown\":\n",
    "                continue\n",
    "            md = \"\".join(cell.get(\"source\", []))\n",
    "            if \"## Strongest booster (account-level second stage)\" in md:\n",
    "                if idx + 1 >= len(cells):\n",
    "                    break\n",
    "                code_cell = cells[idx + 1]\n",
    "                if code_cell.get(\"cell_type\") != \"code\":\n",
    "                    break\n",
    "                print(f\"Loaded booster source from: {candidate}\")\n",
    "                return \"\".join(code_cell.get(\"source\", []))\n",
    "\n",
    "    raise FileNotFoundError(\n",
    "        \"Could not locate notebook file containing the booster cell. \"\n",
    "        \"Set SWEEP_SETTINGS['notebook_path'] to your .ipynb path. \"\n",
    "        f\"Checked: {checked}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def _build_trial_queue(base_config, space, n_trials, include_current, sample_without_replacement, rng):\n",
    "    queue = []\n",
    "    seen = set()\n",
    "\n",
    "    if include_current:\n",
    "        current_override = _extract_space_override(base_config, space)\n",
    "        sig = _config_signature(current_override)\n",
    "        seen.add(sig)\n",
    "        queue.append(current_override)\n",
    "\n",
    "    max_unique = _space_size(space)\n",
    "    target = int(n_trials)\n",
    "    if include_current and target > 0:\n",
    "        target -= 1\n",
    "\n",
    "    if sample_without_replacement:\n",
    "        target = min(target, max_unique - len(seen))\n",
    "\n",
    "    attempts = 0\n",
    "    max_attempts = max(1000, target * 80)\n",
    "\n",
    "    while len(queue) < (target + (1 if include_current else 0)):\n",
    "        cand = _draw_random_override(space, rng)\n",
    "        sig = _config_signature(cand)\n",
    "\n",
    "        if sample_without_replacement and sig in seen:\n",
    "            attempts += 1\n",
    "            if attempts > max_attempts:\n",
    "                break\n",
    "            continue\n",
    "\n",
    "        seen.add(sig)\n",
    "        queue.append(cand)\n",
    "\n",
    "    return queue\n",
    "\n",
    "\n",
    "def _run_one_trial(base_config, override_cfg, run_name, booster_code, stop_on_error=False):\n",
    "    trial_config = copy.deepcopy(base_config)\n",
    "    trial_config.update(copy.deepcopy(override_cfg))\n",
    "    globals()[\"EXPERIMENT_CONFIG\"] = trial_config\n",
    "\n",
    "    start = time.time()\n",
    "    try:\n",
    "        exec(booster_code, globals())\n",
    "        duration = time.time() - start\n",
    "\n",
    "        notes_payload = {\n",
    "            \"duration_sec\": round(float(duration), 2),\n",
    "            \"override\": override_cfg,\n",
    "        }\n",
    "        log_current_booster_run(\n",
    "            run_name=run_name,\n",
    "            notes=json.dumps(notes_payload, sort_keys=True),\n",
    "            verbose=False,\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"run_name\": run_name,\n",
    "            \"status\": \"ok\",\n",
    "            \"duration_sec\": float(duration),\n",
    "            \"ensemble_score\": float(globals().get(\"test_score_ensemble_en\", np.nan)),\n",
    "            \"booster_score\": float(globals().get(\"second_stage_test_score_en\", np.nan)),\n",
    "            \"score_gain\": float(globals().get(\"second_stage_test_score_en\", np.nan))\n",
    "            - float(globals().get(\"test_score_ensemble_en\", np.nan)),\n",
    "            \"seed_std_score\": float(globals().get(\"seed_std_score_en\", np.nan)),\n",
    "            \"selected_profile\": str(globals().get(\"second_stage_selected_profile_en\", \"\")),\n",
    "            \"selected_alpha\": float(globals().get(\"second_stage_alpha_en\", np.nan)),\n",
    "            \"selected_threshold\": float(globals().get(\"second_stage_selected_threshold_en\", np.nan)),\n",
    "            \"feature_source\": str(globals().get(\"second_stage_fit_feature_source_en\", \"\")),\n",
    "            \"error\": \"\",\n",
    "            \"override_json\": json.dumps(override_cfg, sort_keys=True),\n",
    "        }\n",
    "    except Exception as exc:\n",
    "        duration = time.time() - start\n",
    "        err = f\"{type(exc).__name__}: {exc}\"\n",
    "        tb = traceback.format_exc(limit=4)\n",
    "\n",
    "        row = {\n",
    "            \"run_name\": run_name,\n",
    "            \"status\": \"failed\",\n",
    "            \"duration_sec\": float(duration),\n",
    "            \"ensemble_score\": np.nan,\n",
    "            \"booster_score\": np.nan,\n",
    "            \"score_gain\": np.nan,\n",
    "            \"seed_std_score\": np.nan,\n",
    "            \"selected_profile\": \"\",\n",
    "            \"selected_alpha\": np.nan,\n",
    "            \"selected_threshold\": np.nan,\n",
    "            \"feature_source\": \"\",\n",
    "            \"error\": f\"{err} | {tb}\",\n",
    "            \"override_json\": json.dumps(override_cfg, sort_keys=True),\n",
    "        }\n",
    "\n",
    "        if stop_on_error:\n",
    "            raise\n",
    "        return row\n",
    "\n",
    "\n",
    "def run_booster_sweep(settings=None):\n",
    "    cfg = copy.deepcopy(SWEEP_SETTINGS)\n",
    "    if settings is not None:\n",
    "        cfg.update(settings)\n",
    "\n",
    "    n_trials = int(cfg.get(\"n_trials\", 50))\n",
    "    if n_trials < 1:\n",
    "        raise ValueError(\"n_trials must be >= 1\")\n",
    "\n",
    "    rng = random.Random(int(cfg.get(\"random_seed\", 42)))\n",
    "    run_prefix = str(cfg.get(\"run_prefix\", \"sweep\"))\n",
    "    include_current = bool(cfg.get(\"include_current_config\", True))\n",
    "    sample_wo_repl = bool(cfg.get(\"sample_without_replacement\", True))\n",
    "    stop_on_error = bool(cfg.get(\"stop_on_error\", False))\n",
    "    apply_best = bool(cfg.get(\"apply_best_config_at_end\", False))\n",
    "    plot_top_n = int(cfg.get(\"plot_top_n\", 20))\n",
    "    notebook_path = cfg.get(\"notebook_path\", str(DEFAULT_NOTEBOOK_PATH))\n",
    "\n",
    "    base_config = copy.deepcopy(EXPERIMENT_CONFIG)\n",
    "    booster_code = _load_booster_cell_source_from_notebook(nb_path=notebook_path)\n",
    "\n",
    "    queue = _build_trial_queue(\n",
    "        base_config=base_config,\n",
    "        space=BOOSTER_SEARCH_SPACE,\n",
    "        n_trials=n_trials,\n",
    "        include_current=include_current,\n",
    "        sample_without_replacement=sample_wo_repl,\n",
    "        rng=rng,\n",
    "    )\n",
    "\n",
    "    if not queue:\n",
    "        raise ValueError(\"No trials were generated. Check search space and settings.\")\n",
    "\n",
    "    print(\n",
    "        f\"Starting sweep: trials={len(queue)}, include_current={include_current}, \"\n",
    "        f\"without_replacement={sample_wo_repl}, space_size={_space_size(BOOSTER_SEARCH_SPACE):,}\"\n",
    "    )\n",
    "\n",
    "    rows = []\n",
    "    sweep_start = time.time()\n",
    "    for idx, override_cfg in enumerate(queue, start=1):\n",
    "        run_name = f\"{run_prefix}_{idx:04d}\"\n",
    "        print(f\"\\n[{idx}/{len(queue)}] Running {run_name} ...\")\n",
    "\n",
    "        row = _run_one_trial(\n",
    "            base_config=base_config,\n",
    "            override_cfg=override_cfg,\n",
    "            run_name=run_name,\n",
    "            booster_code=booster_code,\n",
    "            stop_on_error=stop_on_error,\n",
    "        )\n",
    "        rows.append(row)\n",
    "\n",
    "        if row[\"status\"] == \"ok\":\n",
    "            print(\n",
    "                f\"{run_name}: booster={row['booster_score']:.1f}, ensemble={row['ensemble_score']:.1f}, \"\n",
    "                f\"gain={row['score_gain']:.1f}, dur={row['duration_sec']:.1f}s\"\n",
    "            )\n",
    "        else:\n",
    "            print(f\"{run_name}: FAILED -> {row['error'].split('|')[0]}\")\n",
    "\n",
    "    total_dur = time.time() - sweep_start\n",
    "    sweep_df = pd.DataFrame(rows)\n",
    "\n",
    "    SWEEP_TRIALS_PATH.parent.mkdir(parents=True, exist_ok=True)\n",
    "    if SWEEP_TRIALS_PATH.exists():\n",
    "        old = pd.read_csv(SWEEP_TRIALS_PATH)\n",
    "        sweep_df = pd.concat([old, sweep_df], ignore_index=True)\n",
    "\n",
    "    sweep_df = sweep_df.drop_duplicates(subset=[\"run_name\"], keep=\"last\")\n",
    "    sweep_df.to_csv(SWEEP_TRIALS_PATH, index=False)\n",
    "\n",
    "    globals()[\"booster_sweep_last_results_en\"] = sweep_df.copy()\n",
    "\n",
    "    # Restore baseline config to avoid accidental side-effects after sweep.\n",
    "    globals()[\"EXPERIMENT_CONFIG\"] = base_config\n",
    "\n",
    "    ok_df = sweep_df[sweep_df[\"status\"] == \"ok\"].copy()\n",
    "    if not ok_df.empty:\n",
    "        ok_df = ok_df.sort_values(\n",
    "            by=[\"booster_score\", \"score_gain\", \"seed_std_score\"],\n",
    "            ascending=[False, False, True],\n",
    "        )\n",
    "\n",
    "    print(\"\\nSweep finished.\")\n",
    "    print(f\"- total duration: {total_dur/60.0:.1f} min\")\n",
    "    print(f\"- successful trials: {int((sweep_df['status'] == 'ok').sum())}\")\n",
    "    print(f\"- failed trials: {int((sweep_df['status'] == 'failed').sum())}\")\n",
    "    print(f\"- trials csv: {SWEEP_TRIALS_PATH}\")\n",
    "\n",
    "    if not ok_df.empty:\n",
    "        print(\"\\nTop sweep trials:\")\n",
    "        print(\n",
    "            ok_df[\n",
    "                [\n",
    "                    \"run_name\",\n",
    "                    \"ensemble_score\",\n",
    "                    \"booster_score\",\n",
    "                    \"score_gain\",\n",
    "                    \"seed_std_score\",\n",
    "                    \"selected_profile\",\n",
    "                    \"feature_source\",\n",
    "                    \"duration_sec\",\n",
    "                ]\n",
    "            ].head(10).to_string(index=False)\n",
    "        )\n",
    "\n",
    "    if apply_best and SEARCH_BEST_PATH.exists():\n",
    "        best_payload = json.loads(SEARCH_BEST_PATH.read_text())\n",
    "        best_config = best_payload.get(\"best_config\", {})\n",
    "        if best_config:\n",
    "            globals()[\"EXPERIMENT_CONFIG\"] = best_config\n",
    "            print(\"\\nApplied best config from SEARCH_BEST_PATH into EXPERIMENT_CONFIG.\")\n",
    "\n",
    "    if plot_top_n > 0:\n",
    "        try:\n",
    "            plot_search_history(top_n=plot_top_n)\n",
    "        except Exception as exc:\n",
    "            print(f\"Could not render history plot: {exc}\")\n",
    "\n",
    "    return sweep_df\n",
    "\n",
    "\n",
    "# Example usage (manual):\n",
    "'''sweep_results = run_booster_sweep({\n",
    "    \"n_trials\": 20,\n",
    "    \"run_prefix\": \"week_sweep\",\n",
    "    \"include_current_config\": True,\n",
    "    \"sample_without_replacement\": True,\n",
    "    \"plot_top_n\": 25,\n",
    "    # Optional in Colab: set this if auto-detection does not find your notebook file.\n",
    "    \"notebook_path\": \"/content/drive/MyDrive/bot_or_not_McHacks_2026/script_fr.ipynb\",\n",
    " })'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_botsornot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
