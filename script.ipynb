{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4fa69427",
   "metadata": {},
   "source": [
    "# McHacks 26 - Bot or Not\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "25e59269",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib as plt\n",
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c78ef1e",
   "metadata": {},
   "source": [
    "### Data processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "071ab039",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EN sources: ['dataset.posts&users.30.json', 'dataset.posts&users.32.json']\n",
      "EN posts: 15,765 users: 546 bot_ids: 129\n",
      "FR sources: ['dataset.posts&users.31.json', 'dataset.posts&users.33.json']\n",
      "FR posts: 9,004 users: 343 bot_ids: 55\n"
     ]
    }
   ],
   "source": [
    "DATA_DIR = Path(\"data\")\n",
    "\n",
    "def get_version(path):\n",
    "    try:\n",
    "        return int(path.stem.split(\".\")[-1])\n",
    "    except ValueError:\n",
    "        return None\n",
    "\n",
    "posts_users_files = sorted(DATA_DIR.glob(\"dataset.posts&users.*.json\"), key=get_version)\n",
    "if not posts_users_files:\n",
    "    raise FileNotFoundError(\"No dataset.posts&users.*.json files found in data/\")\n",
    "\n",
    "combined = {}\n",
    "bots_by_lang = {}\n",
    "\n",
    "for path in posts_users_files:\n",
    "    with path.open() as f:\n",
    "        data = json.load(f)\n",
    "    lang = data.get(\"lang\")\n",
    "\n",
    "    combined.setdefault(lang, {\"posts\": [], \"users\": [], \"sources\": []})\n",
    "    combined[lang][\"posts\"].extend(data.get(\"posts\", []))\n",
    "    combined[lang][\"users\"].extend(data.get(\"users\", []))\n",
    "    combined[lang][\"sources\"].append(path.name)\n",
    "\n",
    "    version = get_version(path)\n",
    "    if version is not None:\n",
    "        bots_path = DATA_DIR / f\"dataset.bots.{version}.txt\"\n",
    "        if bots_path.exists():\n",
    "            bots_by_lang.setdefault(lang, set()).update(bots_path.read_text().splitlines())\n",
    "\n",
    "posts_en = pd.DataFrame(combined.get(\"en\", {}).get(\"posts\", []))\n",
    "users_en = pd.DataFrame(combined.get(\"en\", {}).get(\"users\", []))\n",
    "bot_ids_en = bots_by_lang.get(\"en\", set())\n",
    "if not users_en.empty:\n",
    "    users_en[\"is_bot\"] = users_en[\"id\"].isin(bot_ids_en)\n",
    "\n",
    "posts_fr = pd.DataFrame(combined.get(\"fr\", {}).get(\"posts\", []))\n",
    "users_fr = pd.DataFrame(combined.get(\"fr\", {}).get(\"users\", []))\n",
    "bot_ids_fr = bots_by_lang.get(\"fr\", set())\n",
    "if not users_fr.empty:\n",
    "    users_fr[\"is_bot\"] = users_fr[\"id\"].isin(bot_ids_fr)\n",
    "\n",
    "print(\"EN sources:\", combined.get(\"en\", {}).get(\"sources\", []))\n",
    "print(f\"EN posts: {len(posts_en):,} users: {len(users_en):,} bot_ids: {len(bot_ids_en):,}\")\n",
    "print(\"FR sources:\", combined.get(\"fr\", {}).get(\"sources\", []))\n",
    "print(f\"FR posts: {len(posts_fr):,} users: {len(users_fr):,} bot_ids: {len(bot_ids_fr):,}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e34ec8c",
   "metadata": {},
   "source": [
    "### Experiment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "926640c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Experiment config loaded:\n",
      "- tokenizer_name: vinai/bertweet-base\n",
      "- max_length: 96\n",
      "- dedupe_users: True\n",
      "- dedupe_posts: True\n",
      "- scale_meta_features: True\n",
      "- use_topic_features: True\n",
      "- topic_match_mode: word\n",
      "- test_size: 0.2\n",
      "- random_seed: 42\n",
      "- validation_split: 0.15\n",
      "- epochs: 8\n",
      "- batch_size: 128\n",
      "- learning_rate: 0.001\n",
      "- prediction_threshold: 0.62\n",
      "- use_class_weights: False\n",
      "- embedding_dim: 96\n",
      "- gru_units: 48\n",
      "- aux_dense_units: 32\n",
      "- head_dense_units: 48\n",
      "- dropout_text: 0.4\n",
      "- dropout_aux: 0.3\n",
      "- dropout_head: 0.4\n",
      "- early_stopping_patience: 1\n",
      "- reduce_lr_patience: 1\n",
      "- reduce_lr_factor: 0.5\n",
      "- reduce_lr_min_lr: 1e-05\n"
     ]
    }
   ],
   "source": [
    "EXPERIMENT_CONFIG = {\n",
    "    \"tokenizer_name\": \"vinai/bertweet-base\",\n",
    "    \"max_length\": 96,\n",
    "    \"dedupe_users\": True,\n",
    "    \"dedupe_posts\": True,\n",
    "    \"scale_meta_features\": True,\n",
    "    \"use_topic_features\": True,\n",
    "    \"topic_match_mode\": \"word\",  # options: \"contains\", \"word\"\n",
    "    \"test_size\": 0.20,\n",
    "    \"random_seed\": 42,\n",
    "    \"validation_split\": 0.15,\n",
    "    \"epochs\": 8,\n",
    "    \"batch_size\": 128,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"prediction_threshold\": 0.62,\n",
    "    \"use_class_weights\": False,\n",
    "    \"embedding_dim\": 96,\n",
    "    \"gru_units\": 48,\n",
    "    \"aux_dense_units\": 32,\n",
    "    \"head_dense_units\": 48,\n",
    "    \"dropout_text\": 0.40,\n",
    "    \"dropout_aux\": 0.30,\n",
    "    \"dropout_head\": 0.40,\n",
    "    \"early_stopping_patience\": 1,\n",
    "    \"reduce_lr_patience\": 1,\n",
    "    \"reduce_lr_factor\": 0.50,\n",
    "    \"reduce_lr_min_lr\": 1e-5,\n",
    "}\n",
    "\n",
    "print(\"Experiment config loaded:\")\n",
    "for key, value in EXPERIMENT_CONFIG.items():\n",
    "    print(f\"- {key}: {value}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37411cfe",
   "metadata": {},
   "source": [
    "### Tokenizing the text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a1288615",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: You are sending unauthenticated requests to the HF Hub. Please set a HF_TOKEN to enable higher rate limits and faster downloads.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer: vinai/bertweet-base\n",
      "English labeled posts: 15,765\n",
      "Token tensor shape: (15765, 96)\n",
      "Meta feature shape: (15765, 7), label shape: (15765,)\n",
      "Dedupe users/posts: True/True\n",
      "Scale metadata features: True\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "try:\n",
    "    from transformers import AutoTokenizer\n",
    "except ImportError as exc:\n",
    "    raise ImportError(\"Install transformers first: pip install transformers\") from exc\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "if \"EXPERIMENT_CONFIG\" not in globals():\n",
    "    raise ValueError(\"Run the Experiment parameters cell first.\")\n",
    "\n",
    "TOKENIZER_NAME = str(EXPERIMENT_CONFIG[\"tokenizer_name\"])\n",
    "MAX_LENGTH = int(EXPERIMENT_CONFIG[\"max_length\"])\n",
    "DEDUPE_USERS = bool(EXPERIMENT_CONFIG[\"dedupe_users\"])\n",
    "DEDUPE_POSTS = bool(EXPERIMENT_CONFIG[\"dedupe_posts\"])\n",
    "SCALE_META_FEATURES = bool(EXPERIMENT_CONFIG[\"scale_meta_features\"])\n",
    "\n",
    "def clean_text(text):\n",
    "    if not isinstance(text, str):\n",
    "        return \"\"\n",
    "    text = text.replace(\"\\n\", \" \").strip()\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "def add_text_features(df):\n",
    "    out = df.copy()\n",
    "    out[\"text_clean\"] = out[\"text\"].fillna(\"\").map(clean_text)\n",
    "    out[\"char_count\"] = out[\"text_clean\"].str.len()\n",
    "    out[\"word_count\"] = out[\"text_clean\"].str.split().str.len()\n",
    "    out[\"url_count\"] = out[\"text_clean\"].str.count(r\"https?://\\S+|www\\.\\S+\")\n",
    "    out[\"mention_count\"] = out[\"text_clean\"].str.count(r\"@\\w+\")\n",
    "    out[\"hashtag_count\"] = out[\"text_clean\"].str.count(r\"#\\w+\")\n",
    "    out[\"exclamation_count\"] = out[\"text_clean\"].str.count(r\"!\")\n",
    "    out[\"question_count\"] = out[\"text_clean\"].str.count(r\"\\?\")\n",
    "    return out\n",
    "\n",
    "if posts_en.empty or users_en.empty:\n",
    "    raise ValueError(\"Run the data processing cell first to load English data.\")\n",
    "\n",
    "users_en_labeled = (\n",
    "    users_en.drop_duplicates(subset=\"id\", keep=\"last\").copy()\n",
    "    if DEDUPE_USERS\n",
    "    else users_en.copy()\n",
    ")\n",
    "posts_en_unique = (\n",
    "    posts_en.drop_duplicates(subset=\"id\", keep=\"last\").copy()\n",
    "    if DEDUPE_POSTS\n",
    "    else posts_en.copy()\n",
    ")\n",
    "\n",
    "label_map_en = users_en_labeled.set_index(\"id\")[\"is_bot\"]\n",
    "train_en = posts_en_unique.copy()\n",
    "train_en[\"is_bot\"] = train_en[\"author_id\"].map(label_map_en)\n",
    "train_en = train_en.dropna(subset=[\"is_bot\"]).copy()\n",
    "train_en[\"is_bot\"] = train_en[\"is_bot\"].astype(\"int64\")\n",
    "\n",
    "train_en = add_text_features(train_en)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(TOKENIZER_NAME, use_fast=True)\n",
    "encodings_en = tokenizer(\n",
    "    train_en[\"text_clean\"].tolist(),\n",
    "    truncation=True,\n",
    "    padding=\"max_length\",\n",
    "    max_length=MAX_LENGTH,\n",
    "    return_attention_mask=True,\n",
    ")\n",
    "\n",
    "feature_cols_en = [\n",
    "    \"char_count\",\n",
    "    \"word_count\",\n",
    "    \"url_count\",\n",
    "    \"mention_count\",\n",
    "    \"hashtag_count\",\n",
    "    \"exclamation_count\",\n",
    "    \"question_count\",\n",
    "]\n",
    "X_meta_en = train_en[feature_cols_en].to_numpy(dtype=np.float32)\n",
    "y_en = train_en[\"is_bot\"].to_numpy(dtype=np.int64)\n",
    "\n",
    "if SCALE_META_FEATURES:\n",
    "    scaler_en = StandardScaler()\n",
    "    X_meta_en_scaled = scaler_en.fit_transform(X_meta_en).astype(np.float32)\n",
    "else:\n",
    "    scaler_en = None\n",
    "    X_meta_en_scaled = X_meta_en.copy()\n",
    "\n",
    "print(f\"Tokenizer: {TOKENIZER_NAME}\")\n",
    "print(f\"English labeled posts: {len(train_en):,}\")\n",
    "print(f\"Token tensor shape: {np.asarray(encodings_en['input_ids']).shape}\")\n",
    "print(f\"Meta feature shape: {X_meta_en_scaled.shape}, label shape: {y_en.shape}\")\n",
    "print(f\"Dedupe users/posts: {DEDUPE_USERS}/{DEDUPE_POSTS}\")\n",
    "print(f\"Scale metadata features: {SCALE_META_FEATURES}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "251cded3",
   "metadata": {},
   "source": [
    "## Train-Test split for model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c8c62216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/8\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 162ms/step - accuracy: 0.7372 - auc: 0.7434 - loss: 0.5359 - val_accuracy: 0.8679 - val_auc: 0.8553 - val_loss: 0.3691 - learning_rate: 0.0010\n",
      "Epoch 2/8\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 162ms/step - accuracy: 0.8961 - auc: 0.9008 - loss: 0.3012 - val_accuracy: 0.8800 - val_auc: 0.8580 - val_loss: 0.3575 - learning_rate: 0.0010\n",
      "Epoch 3/8\n",
      "\u001b[1m84/84\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 159ms/step - accuracy: 0.9257 - auc: 0.9430 - loss: 0.2240 - val_accuracy: 0.8663 - val_auc: 0.8460 - val_loss: 0.3965 - learning_rate: 0.0010\n",
      "Topic features enabled: True\n",
      "Topic match mode: word\n",
      "Topic columns: ['topic_pop', 'topic_nba', 'topic_movies', 'topic_nhl']\n",
      "Train/Test sizes: 12612 3153\n",
      "Test Accuracy: 0.8890\n",
      "Test ROC-AUC: 0.8805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8862    0.9628    0.9229      2175\n",
      "           1     0.8975    0.7249    0.8020       978\n",
      "\n",
      "    accuracy                         0.8890      3153\n",
      "   macro avg     0.8918    0.8439    0.8625      3153\n",
      "weighted avg     0.8897    0.8890    0.8854      3153\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report, roc_auc_score\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "except ImportError as exc:\n",
    "    raise ImportError(\"Install tensorflow first: pip install tensorflow\") from exc\n",
    "\n",
    "if \"EXPERIMENT_CONFIG\" not in globals():\n",
    "    raise ValueError(\"Run the Experiment parameters cell first.\")\n",
    "\n",
    "TEST_SIZE = float(EXPERIMENT_CONFIG[\"test_size\"])\n",
    "RANDOM_SEED = int(EXPERIMENT_CONFIG[\"random_seed\"])\n",
    "VALIDATION_SPLIT = float(EXPERIMENT_CONFIG[\"validation_split\"])\n",
    "EPOCHS = int(EXPERIMENT_CONFIG[\"epochs\"])\n",
    "BATCH_SIZE = int(EXPERIMENT_CONFIG[\"batch_size\"])\n",
    "LEARNING_RATE = float(EXPERIMENT_CONFIG[\"learning_rate\"])\n",
    "PREDICTION_THRESHOLD = float(EXPERIMENT_CONFIG[\"prediction_threshold\"])\n",
    "USE_CLASS_WEIGHTS = bool(EXPERIMENT_CONFIG[\"use_class_weights\"])\n",
    "USE_TOPIC_FEATURES = bool(EXPERIMENT_CONFIG[\"use_topic_features\"])\n",
    "TOPIC_MATCH_MODE = str(EXPERIMENT_CONFIG[\"topic_match_mode\"])\n",
    "\n",
    "EMBEDDING_DIM = int(EXPERIMENT_CONFIG[\"embedding_dim\"])\n",
    "GRU_UNITS = int(EXPERIMENT_CONFIG[\"gru_units\"])\n",
    "AUX_DENSE_UNITS = int(EXPERIMENT_CONFIG[\"aux_dense_units\"])\n",
    "HEAD_DENSE_UNITS = int(EXPERIMENT_CONFIG[\"head_dense_units\"])\n",
    "DROPOUT_TEXT = float(EXPERIMENT_CONFIG[\"dropout_text\"])\n",
    "DROPOUT_AUX = float(EXPERIMENT_CONFIG[\"dropout_aux\"])\n",
    "DROPOUT_HEAD = float(EXPERIMENT_CONFIG[\"dropout_head\"])\n",
    "\n",
    "EARLY_STOPPING_PATIENCE = int(EXPERIMENT_CONFIG[\"early_stopping_patience\"])\n",
    "REDUCE_LR_PATIENCE = int(EXPERIMENT_CONFIG[\"reduce_lr_patience\"])\n",
    "REDUCE_LR_FACTOR = float(EXPERIMENT_CONFIG[\"reduce_lr_factor\"])\n",
    "REDUCE_LR_MIN_LR = float(EXPERIMENT_CONFIG[\"reduce_lr_min_lr\"])\n",
    "\n",
    "if not (0.0 < TEST_SIZE < 1.0):\n",
    "    raise ValueError(\"test_size must be between 0 and 1.\")\n",
    "if not (0.0 <= VALIDATION_SPLIT < 1.0):\n",
    "    raise ValueError(\"validation_split must be in [0, 1).\")\n",
    "if TOPIC_MATCH_MODE not in {\"contains\", \"word\"}:\n",
    "    raise ValueError('topic_match_mode must be \"contains\" or \"word\".')\n",
    "\n",
    "def load_english_topic_keywords():\n",
    "    topic_keywords = {}\n",
    "    for source_name in combined.get(\"en\", {}).get(\"sources\", []):\n",
    "        source_path = DATA_DIR / source_name\n",
    "        with source_path.open() as f:\n",
    "            payload = json.load(f)\n",
    "        for topic_item in payload.get(\"metadata\", {}).get(\"topics\", []):\n",
    "            topic = str(topic_item.get(\"topic\", \"\")).strip().lower()\n",
    "            if not topic:\n",
    "                continue\n",
    "            keywords = {\n",
    "                str(keyword).strip().lower()\n",
    "                for keyword in topic_item.get(\"keywords\", [])\n",
    "                if str(keyword).strip()\n",
    "            }\n",
    "            keywords.add(topic)\n",
    "            topic_keywords.setdefault(topic, set()).update(keywords)\n",
    "    return {topic: sorted(values, key=len, reverse=True) for topic, values in topic_keywords.items()}\n",
    "\n",
    "def add_topic_features(df, topic_keywords, match_mode):\n",
    "    out = df.copy()\n",
    "    text_lower = out[\"text_clean\"].str.lower()\n",
    "    topic_cols = []\n",
    "    for topic, keywords in topic_keywords.items():\n",
    "        col = f\"topic_{topic}\"\n",
    "        topic_cols.append(col)\n",
    "        if not keywords:\n",
    "            out[col] = 0\n",
    "            continue\n",
    "        if match_mode == \"word\":\n",
    "            pattern = \"|\".join(rf\"\\\\b{re.escape(keyword)}\\\\b\" for keyword in keywords)\n",
    "        else:\n",
    "            pattern = \"|\".join(re.escape(keyword) for keyword in keywords)\n",
    "        out[col] = text_lower.str.contains(pattern, regex=True).astype(np.int8)\n",
    "    return out, topic_cols\n",
    "\n",
    "if USE_TOPIC_FEATURES:\n",
    "    topic_keywords_en = load_english_topic_keywords()\n",
    "    train_en_model, topic_feature_cols_en = add_topic_features(train_en, topic_keywords_en, TOPIC_MATCH_MODE)\n",
    "else:\n",
    "    train_en_model = train_en.copy()\n",
    "    topic_feature_cols_en = []\n",
    "\n",
    "input_ids_en = np.asarray(encodings_en[\"input_ids\"], dtype=np.int32)\n",
    "attention_mask_en = np.asarray(encodings_en[\"attention_mask\"], dtype=np.float32)\n",
    "X_topic_en = (\n",
    "    train_en_model[topic_feature_cols_en].to_numpy(dtype=np.float32)\n",
    "    if topic_feature_cols_en\n",
    "    else np.zeros((len(train_en_model), 0), dtype=np.float32)\n",
    ")\n",
    "X_aux_en = np.concatenate([X_meta_en_scaled, X_topic_en], axis=1)\n",
    "\n",
    "idx = np.arange(len(y_en))\n",
    "train_idx, test_idx = train_test_split(\n",
    "    idx,\n",
    "    test_size=TEST_SIZE,\n",
    "    random_state=RANDOM_SEED,\n",
    "    stratify=y_en,\n",
    ")\n",
    "\n",
    "X_train_ids, X_test_ids = input_ids_en[train_idx], input_ids_en[test_idx]\n",
    "X_train_mask, X_test_mask = attention_mask_en[train_idx], attention_mask_en[test_idx]\n",
    "X_train_aux, X_test_aux = X_aux_en[train_idx], X_aux_en[test_idx]\n",
    "y_train, y_test = y_en[train_idx], y_en[test_idx]\n",
    "\n",
    "class_weight_dict = None\n",
    "if USE_CLASS_WEIGHTS:\n",
    "    classes = np.unique(y_train)\n",
    "    class_weights = compute_class_weight(class_weight=\"balanced\", classes=classes, y=y_train)\n",
    "    class_weight_dict = {int(c): float(w) for c, w in zip(classes, class_weights)}\n",
    "\n",
    "def build_multifeature_model(\n",
    "    vocab_size,\n",
    "    seq_len,\n",
    "    aux_dim,\n",
    "    embedding_dim,\n",
    "    gru_units,\n",
    "    aux_dense_units,\n",
    "    head_dense_units,\n",
    "    dropout_text,\n",
    "    dropout_aux,\n",
    "    dropout_head,\n",
    "    learning_rate,\n",
    "):\n",
    "    ids_input = tf.keras.layers.Input(shape=(seq_len,), dtype=\"int32\", name=\"input_ids\")\n",
    "    mask_input = tf.keras.layers.Input(shape=(seq_len,), dtype=\"float32\", name=\"attention_mask\")\n",
    "    aux_input = tf.keras.layers.Input(shape=(aux_dim,), dtype=\"float32\", name=\"aux_features\")\n",
    "\n",
    "    x = tf.keras.layers.Embedding(vocab_size, embedding_dim, name=\"token_embedding\")(ids_input)\n",
    "    mask = tf.keras.layers.Reshape((seq_len, 1))(mask_input)\n",
    "    x = tf.keras.layers.Multiply()([x, mask])\n",
    "    x = tf.keras.layers.Bidirectional(tf.keras.layers.GRU(gru_units))(x)\n",
    "    x = tf.keras.layers.Dropout(dropout_text)(x)\n",
    "\n",
    "    aux = tf.keras.layers.Dense(aux_dense_units, activation=\"relu\")(aux_input)\n",
    "    aux = tf.keras.layers.Dropout(dropout_aux)(aux)\n",
    "\n",
    "    merged = tf.keras.layers.Concatenate()([x, aux])\n",
    "    merged = tf.keras.layers.Dense(head_dense_units, activation=\"relu\")(merged)\n",
    "    merged = tf.keras.layers.Dropout(dropout_head)(merged)\n",
    "    output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(merged)\n",
    "\n",
    "    model = tf.keras.Model(inputs=[ids_input, mask_input, aux_input], outputs=output)\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n",
    "        loss=\"binary_crossentropy\",\n",
    "        metrics=[tf.keras.metrics.BinaryAccuracy(name=\"accuracy\"), tf.keras.metrics.AUC(name=\"auc\")],\n",
    "    )\n",
    "    return model\n",
    "\n",
    "model_en = build_multifeature_model(\n",
    "    vocab_size=tokenizer.vocab_size,\n",
    "    seq_len=MAX_LENGTH,\n",
    "    aux_dim=X_train_aux.shape[1],\n",
    "    embedding_dim=EMBEDDING_DIM,\n",
    "    gru_units=GRU_UNITS,\n",
    "    aux_dense_units=AUX_DENSE_UNITS,\n",
    "    head_dense_units=HEAD_DENSE_UNITS,\n",
    "    dropout_text=DROPOUT_TEXT,\n",
    "    dropout_aux=DROPOUT_AUX,\n",
    "    dropout_head=DROPOUT_HEAD,\n",
    "    learning_rate=LEARNING_RATE,\n",
    ")\n",
    "\n",
    "callbacks = []\n",
    "if EARLY_STOPPING_PATIENCE > 0:\n",
    "    callbacks.append(\n",
    "        tf.keras.callbacks.EarlyStopping(\n",
    "            monitor=\"val_auc\",\n",
    "            mode=\"max\",\n",
    "            patience=EARLY_STOPPING_PATIENCE,\n",
    "            restore_best_weights=True,\n",
    "        )\n",
    "    )\n",
    "if REDUCE_LR_PATIENCE > 0:\n",
    "    callbacks.append(\n",
    "        tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor=\"val_auc\",\n",
    "            mode=\"max\",\n",
    "            factor=REDUCE_LR_FACTOR,\n",
    "            patience=REDUCE_LR_PATIENCE,\n",
    "            min_lr=REDUCE_LR_MIN_LR,\n",
    "        )\n",
    "    )\n",
    "\n",
    "history_en = model_en.fit(\n",
    "    {\n",
    "        \"input_ids\": X_train_ids,\n",
    "        \"attention_mask\": X_train_mask,\n",
    "        \"aux_features\": X_train_aux,\n",
    "    },\n",
    "    y_train,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    epochs=EPOCHS,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_weight=class_weight_dict,\n",
    "    callbacks=callbacks,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "y_prob = model_en.predict(\n",
    "    {\n",
    "        \"input_ids\": X_test_ids,\n",
    "        \"attention_mask\": X_test_mask,\n",
    "        \"aux_features\": X_test_aux,\n",
    "    },\n",
    "    verbose=0,\n",
    ").ravel()\n",
    "y_pred = (y_prob >= PREDICTION_THRESHOLD).astype(np.int64)\n",
    "\n",
    "print(f\"Topic features enabled: {USE_TOPIC_FEATURES}\")\n",
    "print(f\"Topic match mode: {TOPIC_MATCH_MODE}\")\n",
    "print(\"Topic columns:\", topic_feature_cols_en)\n",
    "print(\"Train/Test sizes:\", len(train_idx), len(test_idx))\n",
    "print(f\"Test Accuracy: {accuracy_score(y_test, y_pred):.4f}\")\n",
    "print(f\"Test ROC-AUC: {roc_auc_score(y_test, y_prob):.4f}\")\n",
    "print(classification_report(y_test, y_pred, digits=4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_botsornot (3.13.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
